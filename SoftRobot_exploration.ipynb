{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CorpusReaders import Elsevier_Corpus_Reader, Corpus_Pre_Processor, Elsivier_Ingestor\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta review of soft robots using the scopus database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 1: download the raw corpus from elsivier\n",
    "* search the Scopus database for the term 'soft robot\n",
    "* search over the date range 1950 to 2021\n",
    "* Downloaded on date 26 August 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_corpus() -> None:\n",
    "    \"\"\"\n",
    "    download a all papers for a given search term for a given year.\n",
    "    Returns\n",
    "    -------\n",
    "        None\n",
    "    \"\"\"\n",
    "    builder = Elsivier_Ingestor.ScopusIngestionEngine(\n",
    "        file_path=\"Corpus/Raw_corpus/\",\n",
    "        home=False,\n",
    "        batch_size=25)\n",
    "\n",
    "    builder.build_corpus(search_terms=['soft robot'],\n",
    "                         dates=(1950, 2021))\n",
    "    \n",
    "if False:\n",
    "    download_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 2: refactor the corpus\n",
    "* The corpus is downloaded as collections containing all publications\n",
    "  within a given year. This step splits these collections into\n",
    "  individual documents that can be accessed independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refactor_corpus() -> None:\n",
    "    \"\"\"\n",
    "    Read the raw corpus and refactor the collections from a single file per\n",
    "    year to a single file per document.\n",
    "    Returns\n",
    "    -------\n",
    "        None\n",
    "    \"\"\"\n",
    "    root = \"Corpus/Raw_corpus/\"\n",
    "    target = \"Corpus/Split_corpus/\"\n",
    "\n",
    "    corpus = Elsevier_Corpus_Reader.RawCorpusReader(root=root)\n",
    "    Corpus_Pre_Processor.split_corpus(corpus=corpus, target=target)\n",
    "    \n",
    "if False:\n",
    "    refactor_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 3: Pre process the corpus to clean and format the data.\n",
    "* tokenize text into format where one document returns\n",
    "    list of paragraphs\n",
    "        list of sentences\n",
    "            list of tagged tokens\n",
    "                (token, tag)\n",
    "* tokenize additional text fields such as author, city, journal names\n",
    "* add the file path each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_corpus() -> None:\n",
    "    \"\"\"\n",
    "    processes and refomats both text and meta data\n",
    "    Returns\n",
    "    -------\n",
    "        None\n",
    "    \"\"\"\n",
    "    corp = Elsevier_Corpus_Reader.ScopusCorpusReader(\n",
    "            \"Corpus/Split_corpus/\")\n",
    "\n",
    "    formatter = Corpus_Pre_Processor.ScopusCorpusProcessor(corp)\n",
    "    formatter.transform()\n",
    "\n",
    "if False:\n",
    "    preprocess_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 4: Preliminary exploration of the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details of the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What text data do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'description_lexical_diversity': 334.92504354500636,\n",
      "    'description_num': 58711,\n",
      "    'description_vocab': 99243,\n",
      "    'description_word_count': 11344916,\n",
      "    'description_words_per_description': 193.2332271635639,\n",
      "    'descriptions_per_doc': 1.0,\n",
      "    'files': 58711,\n",
      "    'time_to_process': 26.47777271270752,\n",
      "    'title_lexical_diversity': 21.424763085643434,\n",
      "    'title_num': 58711,\n",
      "    'title_vocab': 33873,\n",
      "    'title_word_count': 725721,\n",
      "    'title_words_per_title': 12.360903408219924,\n",
      "    'titles_per_doc': 1.0,\n",
      "    'topics': 1}\n"
     ]
    }
   ],
   "source": [
    "root = \"Corpus/Split_corpus/\"\n",
    "corpus = Elsevier_Corpus_Reader.ScopusProcessedCorpusReader(root)\n",
    "pp.pprint(corpus.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fictional-barnical-no-accelerat]",
   "language": "python",
   "name": "conda-env-fictional-barnical-no-accelerat-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

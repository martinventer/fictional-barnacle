2019-06-18 10:34:08,373 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 10:34:29,536 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 10:35:09,484 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 10:35:57,785 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 10:36:13,194 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 10:36:25,021 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 10:37:14,973 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 10:37:47,412 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 10:38:00,937 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 10:38:36,121 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 10:38:54,393 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 10:39:07,292 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 10:40:18,292 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 10:40:42,146 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 10:40:57,868 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 10:41:17,539 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 10:46:20,580 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 10:47:01,543 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 10:52:34,276 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 10:59:41,172 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 10:59:41,182 E-1001 (BAD_COLUMN_NAME): Glyph refers to nonexistent column name. This could either be due to a misspelling or typo, or due to an expected column being missing. : key "radius" value "node_siza" [renderer: GlyphRenderer(id='3577', ...)]
2019-06-18 10:59:52,428 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 10:59:52,438 E-1001 (BAD_COLUMN_NAME): Glyph refers to nonexistent column name. This could either be due to a misspelling or typo, or due to an expected column being missing. : key "radius" value "node_size" [renderer: GlyphRenderer(id='3807', ...)]
2019-06-18 11:00:06,873 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 11:01:32,203 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 11:01:32,213 E-1001 (BAD_COLUMN_NAME): Glyph refers to nonexistent column name. This could either be due to a misspelling or typo, or due to an expected column being missing. : key "radius" value "node_options" [renderer: GlyphRenderer(id='4267', ...)]
2019-06-18 11:05:33,594 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 11:05:53,834 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 11:06:07,579 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 11:07:09,783 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 11:09:53,079 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 11:17:54,956 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 11:30:20,321 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 11:35:19,372 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 11:35:54,309 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 11:42:56,131 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 11:46:58,887 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 12:01:18,652 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 12:01:47,867 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 12:03:43,329 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 12:06:59,024 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 12:07:26,929 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 12:10:31,367 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 12:11:09,141 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 12:11:19,840 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 12:12:14,282 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 12:12:47,021 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 12:14:09,768 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 12:14:31,569 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 12:14:51,851 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 12:15:19,882 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 12:15:35,405 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 12:16:12,915 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 12:18:38,425 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 12:20:07,438 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-18 12:21:13,832 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-06-19 07:45:59,242 Session output file 'interactive_graphs.html' already exists, will be overwritten.
2019-07-03 09:50:59,474 Could not open font file /usr/share/fonts/truetype/noto/NotoColorEmoji.ttf: In FT2Font: Could not set the fontsize
2019-07-03 09:51:00,162 generated new fontManager
2019-07-08 16:23:20,833 'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.
2019-07-08 16:33:34,803 'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.
2019-07-08 17:42:32,561 'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.
2019-07-08 17:49:50,542 'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.
2019-07-08 17:49:50,553 'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.
2019-07-08 17:49:50,558 'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.
2019-07-09 17:51:33,002 Start
2019-07-09 17:51:33,087 End
2019-07-10 14:19:17,548 file Corpus/Processed_corpus//shoes already exists
2019-07-10 14:19:17,552 file Corpus/Processed_corpus//socks already exists
2019-07-17 10:17:09,417 'pattern' package not found; tag filters are not available for English
2019-07-17 10:42:04,386 collecting all words and their counts
2019-07-17 10:42:04,387 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 10:42:04,387 collected 17 word types and 2 unique tags from a corpus of 2 examples and 33 words
2019-07-17 10:42:04,387 Loading a fresh vocabulary
2019-07-17 10:42:04,387 min_count=5 retains 0 unique words (0% of original 17, drops 17)
2019-07-17 10:42:04,387 min_count=5 leaves 0 word corpus (0% of original 33, drops 33)
2019-07-17 10:42:04,387 deleting the raw counts dictionary of 17 items
2019-07-17 10:42:04,387 sample=0.001 downsamples 0 most-common words
2019-07-17 10:42:04,388 downsampling leaves estimated 0 word corpus (0.0% of prior 0)
2019-07-17 10:42:04,388 estimated required memory for 0 words and 100 dimensions: 1200 bytes
2019-07-17 10:42:04,388 resetting layer weights
2019-07-17 10:42:27,587 collecting all words and their counts
2019-07-17 10:42:27,587 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 10:42:27,587 collected 17 word types and 2 unique tags from a corpus of 2 examples and 33 words
2019-07-17 10:42:27,587 Loading a fresh vocabulary
2019-07-17 10:42:27,587 min_count=5 retains 0 unique words (0% of original 17, drops 17)
2019-07-17 10:42:27,587 min_count=5 leaves 0 word corpus (0% of original 33, drops 33)
2019-07-17 10:42:27,587 deleting the raw counts dictionary of 17 items
2019-07-17 10:42:27,587 sample=0.001 downsamples 0 most-common words
2019-07-17 10:42:27,587 downsampling leaves estimated 0 word corpus (0.0% of prior 0)
2019-07-17 10:42:27,587 estimated required memory for 0 words and 100 dimensions: 1200 bytes
2019-07-17 10:42:27,587 resetting layer weights
2019-07-17 10:54:05,787 collecting all words and their counts
2019-07-17 10:54:45,390 collecting all words and their counts
2019-07-17 10:54:45,390 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 10:54:45,391 collected 17 word types and 2 unique tags from a corpus of 2 examples and 33 words
2019-07-17 10:54:45,391 Loading a fresh vocabulary
2019-07-17 10:54:45,391 min_count=5 retains 0 unique words (0% of original 17, drops 17)
2019-07-17 10:54:45,391 min_count=5 leaves 0 word corpus (0% of original 33, drops 33)
2019-07-17 10:54:45,391 deleting the raw counts dictionary of 17 items
2019-07-17 10:54:45,391 sample=0.001 downsamples 0 most-common words
2019-07-17 10:54:45,391 downsampling leaves estimated 0 word corpus (0.0% of prior 0)
2019-07-17 10:54:45,391 estimated required memory for 0 words and 100 dimensions: 1200 bytes
2019-07-17 10:54:45,391 resetting layer weights
2019-07-17 10:56:03,892 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 10:56:03,892 collecting all words and their counts
2019-07-17 10:56:03,893 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 10:56:03,893 collected 17 word types and 2 unique tags from a corpus of 2 examples and 33 words
2019-07-17 10:56:03,893 Loading a fresh vocabulary
2019-07-17 10:56:03,893 min_count=0 retains 17 unique words (100% of original 17, drops 0)
2019-07-17 10:56:03,893 min_count=0 leaves 33 word corpus (100% of original 33, drops 0)
2019-07-17 10:56:03,893 deleting the raw counts dictionary of 17 items
2019-07-17 10:56:03,893 sample=0.001 downsamples 17 most-common words
2019-07-17 10:56:03,893 downsampling leaves estimated 4 word corpus (14.7% of prior 33)
2019-07-17 10:56:03,893 estimated required memory for 17 words and 5 dimensions: 9620 bytes
2019-07-17 10:56:03,893 resetting layer weights
2019-07-17 10:57:34,426 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 10:57:34,426 collecting all words and their counts
2019-07-17 10:57:34,426 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 10:57:34,426 collected 18 word types and 3 unique tags from a corpus of 3 examples and 26 words
2019-07-17 10:57:34,426 Loading a fresh vocabulary
2019-07-17 10:57:34,426 min_count=0 retains 18 unique words (100% of original 18, drops 0)
2019-07-17 10:57:34,426 min_count=0 leaves 26 word corpus (100% of original 26, drops 0)
2019-07-17 10:57:34,427 deleting the raw counts dictionary of 18 items
2019-07-17 10:57:34,427 sample=0.001 downsamples 18 most-common words
2019-07-17 10:57:34,427 downsampling leaves estimated 3 word corpus (14.8% of prior 26)
2019-07-17 10:57:34,427 estimated required memory for 18 words and 5 dimensions: 10380 bytes
2019-07-17 10:57:34,427 resetting layer weights
2019-07-17 10:57:34,427 training model with 3 workers on 18 vocabulary and 5 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 10:57:34,428 worker thread finished; awaiting finish of 2 more threads
2019-07-17 10:57:34,428 worker thread finished; awaiting finish of 1 more threads
2019-07-17 10:57:34,432 worker thread finished; awaiting finish of 0 more threads
2019-07-17 10:57:34,432 EPOCH - 1 : training on 26 raw words (9 effective words) took 0.0s, 2384 effective words/s
2019-07-17 10:57:34,433 worker thread finished; awaiting finish of 2 more threads
2019-07-17 10:57:34,433 worker thread finished; awaiting finish of 1 more threads
2019-07-17 10:57:34,433 worker thread finished; awaiting finish of 0 more threads
2019-07-17 10:57:34,433 EPOCH - 2 : training on 26 raw words (5 effective words) took 0.0s, 9112 effective words/s
2019-07-17 10:57:34,434 worker thread finished; awaiting finish of 2 more threads
2019-07-17 10:57:34,435 worker thread finished; awaiting finish of 1 more threads
2019-07-17 10:57:34,435 worker thread finished; awaiting finish of 0 more threads
2019-07-17 10:57:34,435 EPOCH - 3 : training on 26 raw words (8 effective words) took 0.0s, 8748 effective words/s
2019-07-17 10:57:34,436 worker thread finished; awaiting finish of 2 more threads
2019-07-17 10:57:34,436 worker thread finished; awaiting finish of 1 more threads
2019-07-17 10:57:34,436 worker thread finished; awaiting finish of 0 more threads
2019-07-17 10:57:34,436 EPOCH - 4 : training on 26 raw words (6 effective words) took 0.0s, 10357 effective words/s
2019-07-17 10:57:34,437 worker thread finished; awaiting finish of 2 more threads
2019-07-17 10:57:34,437 worker thread finished; awaiting finish of 1 more threads
2019-07-17 10:57:34,437 worker thread finished; awaiting finish of 0 more threads
2019-07-17 10:57:34,437 EPOCH - 5 : training on 26 raw words (8 effective words) took 0.0s, 18741 effective words/s
2019-07-17 10:57:34,437 training on a 130 raw words (36 effective words) took 0.0s, 3621 effective words/s
2019-07-17 10:57:34,437 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-17 11:01:44,729 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 11:02:13,354 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 11:02:13,355 collecting all words and their counts
2019-07-17 11:02:13,355 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 11:02:13,355 collected 18 word types and 3 unique tags from a corpus of 3 examples and 26 words
2019-07-17 11:02:13,355 Loading a fresh vocabulary
2019-07-17 11:02:13,355 min_count=0 retains 18 unique words (100% of original 18, drops 0)
2019-07-17 11:02:13,355 min_count=0 leaves 26 word corpus (100% of original 26, drops 0)
2019-07-17 11:02:13,355 deleting the raw counts dictionary of 18 items
2019-07-17 11:02:13,356 sample=0.001 downsamples 18 most-common words
2019-07-17 11:02:13,356 downsampling leaves estimated 3 word corpus (14.8% of prior 26)
2019-07-17 11:02:13,356 estimated required memory for 18 words and 5 dimensions: 10380 bytes
2019-07-17 11:02:13,356 resetting layer weights
2019-07-17 11:02:13,357 training model with 3 workers on 18 vocabulary and 5 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 11:02:13,359 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:02:13,359 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:02:13,359 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:02:13,359 EPOCH - 1 : training on 26 raw words (9 effective words) took 0.0s, 10283 effective words/s
2019-07-17 11:02:13,364 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:02:13,364 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:02:13,364 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:02:13,364 EPOCH - 2 : training on 26 raw words (5 effective words) took 0.0s, 8913 effective words/s
2019-07-17 11:02:13,364 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:02:13,365 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:02:13,365 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:02:13,365 EPOCH - 3 : training on 26 raw words (8 effective words) took 0.0s, 19472 effective words/s
2019-07-17 11:02:13,366 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:02:13,366 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:02:13,366 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:02:13,366 EPOCH - 4 : training on 26 raw words (6 effective words) took 0.0s, 9208 effective words/s
2019-07-17 11:02:13,367 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:02:13,367 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:02:13,367 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:02:13,367 EPOCH - 5 : training on 26 raw words (8 effective words) took 0.0s, 18547 effective words/s
2019-07-17 11:02:13,367 training on a 130 raw words (36 effective words) took 0.0s, 3404 effective words/s
2019-07-17 11:02:13,367 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-17 11:02:42,592 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 11:02:42,592 collecting all words and their counts
2019-07-17 11:02:42,592 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 11:02:42,592 collected 18 word types and 3 unique tags from a corpus of 3 examples and 26 words
2019-07-17 11:02:42,592 Loading a fresh vocabulary
2019-07-17 11:02:42,592 min_count=0 retains 18 unique words (100% of original 18, drops 0)
2019-07-17 11:02:42,592 min_count=0 leaves 26 word corpus (100% of original 26, drops 0)
2019-07-17 11:02:42,593 deleting the raw counts dictionary of 18 items
2019-07-17 11:02:42,593 sample=0.001 downsamples 18 most-common words
2019-07-17 11:02:42,593 downsampling leaves estimated 3 word corpus (14.8% of prior 26)
2019-07-17 11:02:42,593 estimated required memory for 18 words and 5 dimensions: 10380 bytes
2019-07-17 11:02:42,593 resetting layer weights
2019-07-17 11:02:42,593 training model with 3 workers on 18 vocabulary and 5 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 11:02:42,595 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:02:42,595 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:02:42,595 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:02:42,595 EPOCH - 1 : training on 26 raw words (9 effective words) took 0.0s, 10438 effective words/s
2019-07-17 11:02:42,596 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:02:42,596 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:02:42,597 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:02:42,597 EPOCH - 2 : training on 26 raw words (5 effective words) took 0.0s, 7050 effective words/s
2019-07-17 11:02:42,597 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:02:42,598 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:02:42,598 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:02:42,598 EPOCH - 3 : training on 26 raw words (8 effective words) took 0.0s, 16436 effective words/s
2019-07-17 11:02:42,598 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:02:42,599 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:02:42,599 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:02:42,599 EPOCH - 4 : training on 26 raw words (6 effective words) took 0.0s, 11881 effective words/s
2019-07-17 11:02:42,600 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:02:42,600 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:02:42,600 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:02:42,600 EPOCH - 5 : training on 26 raw words (8 effective words) took 0.0s, 15258 effective words/s
2019-07-17 11:02:42,600 training on a 130 raw words (36 effective words) took 0.0s, 5261 effective words/s
2019-07-17 11:02:42,600 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-17 11:03:20,001 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 11:03:20,001 collecting all words and their counts
2019-07-17 11:03:20,001 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 11:03:20,001 collected 18 word types and 3 unique tags from a corpus of 3 examples and 26 words
2019-07-17 11:03:20,001 Loading a fresh vocabulary
2019-07-17 11:03:20,001 min_count=0 retains 18 unique words (100% of original 18, drops 0)
2019-07-17 11:03:20,002 min_count=0 leaves 26 word corpus (100% of original 26, drops 0)
2019-07-17 11:03:20,002 deleting the raw counts dictionary of 18 items
2019-07-17 11:03:20,002 sample=0.001 downsamples 18 most-common words
2019-07-17 11:03:20,002 downsampling leaves estimated 3 word corpus (14.8% of prior 26)
2019-07-17 11:03:20,002 estimated required memory for 18 words and 5 dimensions: 10380 bytes
2019-07-17 11:03:20,002 resetting layer weights
2019-07-17 11:03:20,002 training model with 3 workers on 18 vocabulary and 5 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 11:03:20,003 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:03:20,003 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:03:20,003 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:03:20,003 EPOCH - 1 : training on 26 raw words (9 effective words) took 0.0s, 17623 effective words/s
2019-07-17 11:03:20,004 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:03:20,005 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:03:20,005 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:03:20,005 EPOCH - 2 : training on 26 raw words (5 effective words) took 0.0s, 8382 effective words/s
2019-07-17 11:03:20,005 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:03:20,006 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:03:20,006 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:03:20,006 EPOCH - 3 : training on 26 raw words (8 effective words) took 0.0s, 18287 effective words/s
2019-07-17 11:03:20,007 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:03:20,008 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:03:20,008 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:03:20,008 EPOCH - 4 : training on 26 raw words (6 effective words) took 0.0s, 14474 effective words/s
2019-07-17 11:03:20,008 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:03:20,008 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:03:20,008 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:03:20,009 EPOCH - 5 : training on 26 raw words (8 effective words) took 0.0s, 23002 effective words/s
2019-07-17 11:03:20,009 training on a 130 raw words (36 effective words) took 0.0s, 5639 effective words/s
2019-07-17 11:03:20,009 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-17 11:03:54,635 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 11:04:21,890 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 11:05:36,980 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 11:05:36,980 collecting all words and their counts
2019-07-17 11:05:36,980 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 11:05:36,980 collected 18 word types and 3 unique tags from a corpus of 3 examples and 26 words
2019-07-17 11:05:36,981 Loading a fresh vocabulary
2019-07-17 11:05:36,981 min_count=0 retains 18 unique words (100% of original 18, drops 0)
2019-07-17 11:05:36,981 min_count=0 leaves 26 word corpus (100% of original 26, drops 0)
2019-07-17 11:05:36,981 deleting the raw counts dictionary of 18 items
2019-07-17 11:05:36,981 sample=0.001 downsamples 18 most-common words
2019-07-17 11:05:36,981 downsampling leaves estimated 3 word corpus (14.8% of prior 26)
2019-07-17 11:05:36,981 estimated required memory for 18 words and 5 dimensions: 10380 bytes
2019-07-17 11:05:36,981 resetting layer weights
2019-07-17 11:05:36,981 training model with 3 workers on 18 vocabulary and 5 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 11:05:36,983 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:05:36,983 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:05:36,983 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:05:36,983 EPOCH - 1 : training on 26 raw words (9 effective words) took 0.0s, 11767 effective words/s
2019-07-17 11:05:36,984 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:05:36,984 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:05:36,984 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:05:36,984 EPOCH - 2 : training on 26 raw words (5 effective words) took 0.0s, 9787 effective words/s
2019-07-17 11:05:36,987 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:05:36,987 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:05:36,987 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:05:36,987 EPOCH - 3 : training on 26 raw words (8 effective words) took 0.0s, 15249 effective words/s
2019-07-17 11:05:36,988 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:05:36,988 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:05:36,988 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:05:36,988 EPOCH - 4 : training on 26 raw words (6 effective words) took 0.0s, 11050 effective words/s
2019-07-17 11:05:36,989 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:05:36,989 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:05:36,989 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:05:36,989 EPOCH - 5 : training on 26 raw words (8 effective words) took 0.0s, 20978 effective words/s
2019-07-17 11:05:36,989 training on a 130 raw words (36 effective words) took 0.0s, 4375 effective words/s
2019-07-17 11:05:36,989 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-17 11:06:06,867 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 11:06:25,340 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 11:10:00,993 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 11:10:00,993 collecting all words and their counts
2019-07-17 11:10:00,993 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 11:10:00,993 collected 17 word types and 3 unique tags from a corpus of 3 examples and 48 words
2019-07-17 11:10:00,993 Loading a fresh vocabulary
2019-07-17 11:10:00,993 min_count=0 retains 17 unique words (100% of original 17, drops 0)
2019-07-17 11:10:00,993 min_count=0 leaves 48 word corpus (100% of original 48, drops 0)
2019-07-17 11:10:00,993 deleting the raw counts dictionary of 17 items
2019-07-17 11:10:00,994 sample=0.001 downsamples 17 most-common words
2019-07-17 11:10:00,994 downsampling leaves estimated 7 word corpus (14.6% of prior 48)
2019-07-17 11:10:00,994 estimated required memory for 17 words and 5 dimensions: 9840 bytes
2019-07-17 11:10:00,994 resetting layer weights
2019-07-17 11:10:00,994 training model with 3 workers on 17 vocabulary and 5 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 11:10:00,995 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:10:00,995 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:10:00,995 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:10:00,995 EPOCH - 1 : training on 48 raw words (8 effective words) took 0.0s, 16008 effective words/s
2019-07-17 11:10:00,996 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:10:00,996 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:10:00,996 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:10:00,996 EPOCH - 2 : training on 48 raw words (10 effective words) took 0.0s, 17932 effective words/s
2019-07-17 11:10:00,997 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:10:00,997 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:10:00,998 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:10:00,998 EPOCH - 3 : training on 48 raw words (7 effective words) took 0.0s, 12938 effective words/s
2019-07-17 11:10:00,998 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:10:00,999 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:10:00,999 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:10:00,999 EPOCH - 4 : training on 48 raw words (8 effective words) took 0.0s, 14589 effective words/s
2019-07-17 11:10:00,999 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:10:00,999 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:10:00,999 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:10:00,999 EPOCH - 5 : training on 48 raw words (9 effective words) took 0.0s, 20825 effective words/s
2019-07-17 11:10:01,000 training on a 240 raw words (42 effective words) took 0.0s, 7653 effective words/s
2019-07-17 11:10:01,000 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-17 11:10:26,339 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 11:10:26,339 collecting all words and their counts
2019-07-17 11:10:26,339 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 11:10:26,339 collected 17 word types and 2 unique tags from a corpus of 2 examples and 33 words
2019-07-17 11:10:26,339 Loading a fresh vocabulary
2019-07-17 11:10:26,340 min_count=0 retains 17 unique words (100% of original 17, drops 0)
2019-07-17 11:10:26,340 min_count=0 leaves 33 word corpus (100% of original 33, drops 0)
2019-07-17 11:10:26,340 deleting the raw counts dictionary of 17 items
2019-07-17 11:10:26,340 sample=0.001 downsamples 17 most-common words
2019-07-17 11:10:26,340 downsampling leaves estimated 4 word corpus (14.7% of prior 33)
2019-07-17 11:10:26,340 estimated required memory for 17 words and 5 dimensions: 9620 bytes
2019-07-17 11:10:26,340 resetting layer weights
2019-07-17 11:10:26,340 training model with 3 workers on 17 vocabulary and 5 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 11:10:26,342 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:10:26,342 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:10:26,342 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:10:26,342 EPOCH - 1 : training on 33 raw words (7 effective words) took 0.0s, 9220 effective words/s
2019-07-17 11:10:26,343 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:10:26,344 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:10:26,344 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:10:26,344 EPOCH - 2 : training on 33 raw words (5 effective words) took 0.0s, 10316 effective words/s
2019-07-17 11:10:26,345 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:10:26,345 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:10:26,345 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:10:26,345 EPOCH - 3 : training on 33 raw words (7 effective words) took 0.0s, 16761 effective words/s
2019-07-17 11:10:26,345 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:10:26,346 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:10:26,346 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:10:26,346 EPOCH - 4 : training on 33 raw words (4 effective words) took 0.0s, 8972 effective words/s
2019-07-17 11:10:26,346 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:10:26,347 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:10:26,347 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:10:26,347 EPOCH - 5 : training on 33 raw words (7 effective words) took 0.0s, 18583 effective words/s
2019-07-17 11:10:26,347 training on a 165 raw words (30 effective words) took 0.0s, 4626 effective words/s
2019-07-17 11:10:26,347 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-17 11:11:15,491 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 11:12:49,806 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 11:12:49,806 collecting all words and their counts
2019-07-17 11:12:49,806 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 11:12:49,806 collected 17 word types and 2 unique tags from a corpus of 2 examples and 33 words
2019-07-17 11:12:49,806 Loading a fresh vocabulary
2019-07-17 11:12:49,806 min_count=0 retains 17 unique words (100% of original 17, drops 0)
2019-07-17 11:12:49,806 min_count=0 leaves 33 word corpus (100% of original 33, drops 0)
2019-07-17 11:12:49,806 deleting the raw counts dictionary of 17 items
2019-07-17 11:12:49,806 sample=0.001 downsamples 17 most-common words
2019-07-17 11:12:49,806 downsampling leaves estimated 4 word corpus (14.7% of prior 33)
2019-07-17 11:12:49,806 estimated required memory for 17 words and 5 dimensions: 9620 bytes
2019-07-17 11:12:49,806 resetting layer weights
2019-07-17 11:12:49,807 training model with 3 workers on 17 vocabulary and 5 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 11:12:49,808 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:12:49,808 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:12:49,808 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:12:49,808 EPOCH - 1 : training on 33 raw words (7 effective words) took 0.0s, 11941 effective words/s
2019-07-17 11:12:49,809 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:12:49,809 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:12:49,809 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:12:49,809 EPOCH - 2 : training on 33 raw words (5 effective words) took 0.0s, 10629 effective words/s
2019-07-17 11:12:49,810 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:12:49,810 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:12:49,810 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:12:49,810 EPOCH - 3 : training on 33 raw words (7 effective words) took 0.0s, 15363 effective words/s
2019-07-17 11:12:49,811 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:12:49,811 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:12:49,811 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:12:49,811 EPOCH - 4 : training on 33 raw words (4 effective words) took 0.0s, 8943 effective words/s
2019-07-17 11:12:49,812 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:12:49,812 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:12:49,812 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:12:49,812 EPOCH - 5 : training on 33 raw words (7 effective words) took 0.0s, 13671 effective words/s
2019-07-17 11:12:49,812 training on a 165 raw words (30 effective words) took 0.0s, 5393 effective words/s
2019-07-17 11:12:49,812 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-17 11:13:38,441 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 11:13:38,442 collecting all words and their counts
2019-07-17 11:13:38,442 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 11:13:38,442 collected 17 word types and 2 unique tags from a corpus of 2 examples and 33 words
2019-07-17 11:13:38,442 Loading a fresh vocabulary
2019-07-17 11:13:38,442 min_count=0 retains 17 unique words (100% of original 17, drops 0)
2019-07-17 11:13:38,442 min_count=0 leaves 33 word corpus (100% of original 33, drops 0)
2019-07-17 11:13:38,442 deleting the raw counts dictionary of 17 items
2019-07-17 11:13:38,442 sample=0.001 downsamples 17 most-common words
2019-07-17 11:13:38,442 downsampling leaves estimated 4 word corpus (14.7% of prior 33)
2019-07-17 11:13:38,442 estimated required memory for 17 words and 5 dimensions: 9620 bytes
2019-07-17 11:13:38,442 resetting layer weights
2019-07-17 11:13:38,442 training model with 3 workers on 17 vocabulary and 5 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 11:13:38,444 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:13:38,444 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:13:38,444 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:13:38,444 EPOCH - 1 : training on 33 raw words (7 effective words) took 0.0s, 9653 effective words/s
2019-07-17 11:13:38,445 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:13:38,445 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:13:38,445 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:13:38,445 EPOCH - 2 : training on 33 raw words (5 effective words) took 0.0s, 12994 effective words/s
2019-07-17 11:13:38,446 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:13:38,446 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:13:38,446 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:13:38,446 EPOCH - 3 : training on 33 raw words (7 effective words) took 0.0s, 12875 effective words/s
2019-07-17 11:13:38,447 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:13:38,447 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:13:38,447 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:13:38,447 EPOCH - 4 : training on 33 raw words (4 effective words) took 0.0s, 11656 effective words/s
2019-07-17 11:13:38,448 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:13:38,448 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:13:38,448 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:13:38,448 EPOCH - 5 : training on 33 raw words (7 effective words) took 0.0s, 15760 effective words/s
2019-07-17 11:13:38,448 training on a 165 raw words (30 effective words) took 0.0s, 5515 effective words/s
2019-07-17 11:13:38,448 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-17 11:13:38,448 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 11:13:38,448 collecting all words and their counts
2019-07-17 11:13:38,448 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 11:13:38,448 collected 17 word types and 2 unique tags from a corpus of 2 examples and 33 words
2019-07-17 11:13:38,448 Loading a fresh vocabulary
2019-07-17 11:13:38,448 min_count=0 retains 17 unique words (100% of original 17, drops 0)
2019-07-17 11:13:38,448 min_count=0 leaves 33 word corpus (100% of original 33, drops 0)
2019-07-17 11:13:38,448 deleting the raw counts dictionary of 17 items
2019-07-17 11:13:38,449 sample=0.001 downsamples 17 most-common words
2019-07-17 11:13:38,449 downsampling leaves estimated 4 word corpus (14.7% of prior 33)
2019-07-17 11:13:38,449 estimated required memory for 17 words and 5 dimensions: 9620 bytes
2019-07-17 11:13:38,449 resetting layer weights
2019-07-17 11:13:38,449 training model with 3 workers on 17 vocabulary and 5 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 11:13:38,450 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:13:38,450 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:13:38,450 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:13:38,450 EPOCH - 1 : training on 33 raw words (7 effective words) took 0.0s, 16157 effective words/s
2019-07-17 11:13:38,451 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:13:38,451 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:13:38,451 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:13:38,451 EPOCH - 2 : training on 33 raw words (5 effective words) took 0.0s, 10255 effective words/s
2019-07-17 11:13:38,452 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:13:38,452 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:13:38,453 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:13:38,453 EPOCH - 3 : training on 33 raw words (7 effective words) took 0.0s, 13754 effective words/s
2019-07-17 11:13:38,453 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:13:38,454 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:13:38,454 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:13:38,454 EPOCH - 4 : training on 33 raw words (4 effective words) took 0.0s, 7485 effective words/s
2019-07-17 11:13:38,455 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:13:38,455 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:13:38,455 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:13:38,455 EPOCH - 5 : training on 33 raw words (7 effective words) took 0.0s, 12889 effective words/s
2019-07-17 11:13:38,455 training on a 165 raw words (30 effective words) took 0.0s, 4682 effective words/s
2019-07-17 11:13:38,455 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-17 11:13:50,938 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 11:13:50,938 collecting all words and their counts
2019-07-17 11:13:50,938 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 11:13:50,939 collected 17 word types and 2 unique tags from a corpus of 2 examples and 33 words
2019-07-17 11:13:50,939 Loading a fresh vocabulary
2019-07-17 11:13:50,939 min_count=0 retains 17 unique words (100% of original 17, drops 0)
2019-07-17 11:13:50,939 min_count=0 leaves 33 word corpus (100% of original 33, drops 0)
2019-07-17 11:13:50,939 deleting the raw counts dictionary of 17 items
2019-07-17 11:13:50,939 sample=0.001 downsamples 17 most-common words
2019-07-17 11:13:50,939 downsampling leaves estimated 4 word corpus (14.7% of prior 33)
2019-07-17 11:13:50,940 estimated required memory for 17 words and 5 dimensions: 9620 bytes
2019-07-17 11:13:50,940 resetting layer weights
2019-07-17 11:13:50,940 training model with 3 workers on 17 vocabulary and 5 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 11:13:50,941 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:13:50,941 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:13:50,941 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:13:50,941 EPOCH - 1 : training on 33 raw words (7 effective words) took 0.0s, 10907 effective words/s
2019-07-17 11:13:50,943 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:13:50,943 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:13:50,943 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:13:50,943 EPOCH - 2 : training on 33 raw words (5 effective words) took 0.0s, 7057 effective words/s
2019-07-17 11:13:50,944 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:13:50,944 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:13:50,944 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:13:50,944 EPOCH - 3 : training on 33 raw words (7 effective words) took 0.0s, 10327 effective words/s
2019-07-17 11:13:50,946 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:13:50,946 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:13:50,946 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:13:50,946 EPOCH - 4 : training on 33 raw words (4 effective words) took 0.0s, 7134 effective words/s
2019-07-17 11:13:50,947 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:13:50,947 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:13:50,947 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:13:50,947 EPOCH - 5 : training on 33 raw words (7 effective words) took 0.0s, 11388 effective words/s
2019-07-17 11:13:50,947 training on a 165 raw words (30 effective words) took 0.0s, 4083 effective words/s
2019-07-17 11:13:50,947 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-17 11:14:29,561 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 11:14:29,561 collecting all words and their counts
2019-07-17 11:14:29,561 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 11:14:29,561 collected 17 word types and 2 unique tags from a corpus of 2 examples and 33 words
2019-07-17 11:14:29,561 Loading a fresh vocabulary
2019-07-17 11:14:29,561 min_count=0 retains 17 unique words (100% of original 17, drops 0)
2019-07-17 11:14:29,561 min_count=0 leaves 33 word corpus (100% of original 33, drops 0)
2019-07-17 11:14:29,561 deleting the raw counts dictionary of 17 items
2019-07-17 11:14:29,562 sample=0.001 downsamples 17 most-common words
2019-07-17 11:14:29,562 downsampling leaves estimated 4 word corpus (14.7% of prior 33)
2019-07-17 11:14:29,562 estimated required memory for 17 words and 5 dimensions: 9620 bytes
2019-07-17 11:14:29,562 resetting layer weights
2019-07-17 11:14:29,562 training model with 3 workers on 17 vocabulary and 5 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 11:14:29,563 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:14:29,563 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:14:29,563 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:14:29,563 EPOCH - 1 : training on 33 raw words (7 effective words) took 0.0s, 17196 effective words/s
2019-07-17 11:14:29,564 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:14:29,564 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:14:29,564 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:14:29,564 EPOCH - 2 : training on 33 raw words (5 effective words) took 0.0s, 8586 effective words/s
2019-07-17 11:14:29,565 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:14:29,565 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:14:29,565 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:14:29,565 EPOCH - 3 : training on 33 raw words (7 effective words) took 0.0s, 15674 effective words/s
2019-07-17 11:14:29,566 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:14:29,566 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:14:29,566 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:14:29,566 EPOCH - 4 : training on 33 raw words (4 effective words) took 0.0s, 9235 effective words/s
2019-07-17 11:14:29,567 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:14:29,567 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:14:29,567 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:14:29,567 EPOCH - 5 : training on 33 raw words (7 effective words) took 0.0s, 15461 effective words/s
2019-07-17 11:14:29,567 training on a 165 raw words (30 effective words) took 0.0s, 5465 effective words/s
2019-07-17 11:14:29,567 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-17 11:14:57,004 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 11:14:57,004 collecting all words and their counts
2019-07-17 11:14:57,004 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 11:14:57,004 collected 17 word types and 2 unique tags from a corpus of 2 examples and 33 words
2019-07-17 11:14:57,004 Loading a fresh vocabulary
2019-07-17 11:14:57,004 min_count=0 retains 17 unique words (100% of original 17, drops 0)
2019-07-17 11:14:57,004 min_count=0 leaves 33 word corpus (100% of original 33, drops 0)
2019-07-17 11:14:57,004 deleting the raw counts dictionary of 17 items
2019-07-17 11:14:57,005 sample=0.001 downsamples 17 most-common words
2019-07-17 11:14:57,005 downsampling leaves estimated 4 word corpus (14.7% of prior 33)
2019-07-17 11:14:57,005 estimated required memory for 17 words and 5 dimensions: 9620 bytes
2019-07-17 11:14:57,005 resetting layer weights
2019-07-17 11:14:57,005 training model with 3 workers on 17 vocabulary and 5 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 11:14:57,007 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:14:57,007 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:14:57,007 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:14:57,007 EPOCH - 1 : training on 33 raw words (7 effective words) took 0.0s, 9211 effective words/s
2019-07-17 11:14:57,008 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:14:57,008 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:14:57,008 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:14:57,008 EPOCH - 2 : training on 33 raw words (5 effective words) took 0.0s, 7932 effective words/s
2019-07-17 11:14:57,009 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:14:57,009 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:14:57,010 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:14:57,010 EPOCH - 3 : training on 33 raw words (7 effective words) took 0.0s, 14162 effective words/s
2019-07-17 11:14:57,010 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:14:57,010 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:14:57,010 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:14:57,010 EPOCH - 4 : training on 33 raw words (4 effective words) took 0.0s, 8721 effective words/s
2019-07-17 11:14:57,011 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:14:57,011 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:14:57,011 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:14:57,011 EPOCH - 5 : training on 33 raw words (7 effective words) took 0.0s, 18240 effective words/s
2019-07-17 11:14:57,011 training on a 165 raw words (30 effective words) took 0.0s, 4873 effective words/s
2019-07-17 11:14:57,011 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-17 11:15:45,416 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 11:15:45,416 collecting all words and their counts
2019-07-17 11:15:45,419 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 11:15:45,419 collected 17 word types and 2 unique tags from a corpus of 2 examples and 33 words
2019-07-17 11:15:45,419 Loading a fresh vocabulary
2019-07-17 11:15:45,419 min_count=0 retains 17 unique words (100% of original 17, drops 0)
2019-07-17 11:15:45,419 min_count=0 leaves 33 word corpus (100% of original 33, drops 0)
2019-07-17 11:15:45,419 deleting the raw counts dictionary of 17 items
2019-07-17 11:15:45,419 sample=0.001 downsamples 17 most-common words
2019-07-17 11:15:45,419 downsampling leaves estimated 4 word corpus (14.7% of prior 33)
2019-07-17 11:15:45,419 estimated required memory for 17 words and 5 dimensions: 9620 bytes
2019-07-17 11:15:45,419 resetting layer weights
2019-07-17 11:15:45,420 training model with 3 workers on 17 vocabulary and 5 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 11:15:45,421 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:15:45,421 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:15:45,421 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:15:45,421 EPOCH - 1 : training on 33 raw words (7 effective words) took 0.0s, 12834 effective words/s
2019-07-17 11:15:45,422 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:15:45,422 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:15:45,422 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:15:45,422 EPOCH - 2 : training on 33 raw words (5 effective words) took 0.0s, 11343 effective words/s
2019-07-17 11:15:45,423 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:15:45,423 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:15:45,423 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:15:45,423 EPOCH - 3 : training on 33 raw words (7 effective words) took 0.0s, 14574 effective words/s
2019-07-17 11:15:45,424 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:15:45,424 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:15:45,424 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:15:45,424 EPOCH - 4 : training on 33 raw words (4 effective words) took 0.0s, 9212 effective words/s
2019-07-17 11:15:45,425 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:15:45,425 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:15:45,425 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:15:45,426 EPOCH - 5 : training on 33 raw words (7 effective words) took 0.0s, 11903 effective words/s
2019-07-17 11:15:45,426 training on a 165 raw words (30 effective words) took 0.0s, 5153 effective words/s
2019-07-17 11:15:45,426 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-17 11:16:23,289 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 11:16:23,289 collecting all words and their counts
2019-07-17 11:16:23,292 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 11:16:23,293 collected 17 word types and 2 unique tags from a corpus of 2 examples and 33 words
2019-07-17 11:16:23,293 Loading a fresh vocabulary
2019-07-17 11:16:23,293 min_count=0 retains 17 unique words (100% of original 17, drops 0)
2019-07-17 11:16:23,293 min_count=0 leaves 33 word corpus (100% of original 33, drops 0)
2019-07-17 11:16:23,293 deleting the raw counts dictionary of 17 items
2019-07-17 11:16:23,293 sample=0.001 downsamples 17 most-common words
2019-07-17 11:16:23,293 downsampling leaves estimated 4 word corpus (14.7% of prior 33)
2019-07-17 11:16:23,293 estimated required memory for 17 words and 5 dimensions: 9620 bytes
2019-07-17 11:16:23,293 resetting layer weights
2019-07-17 11:16:23,294 training model with 3 workers on 17 vocabulary and 5 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 11:16:23,295 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:16:23,295 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:16:23,295 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:16:23,295 EPOCH - 1 : training on 33 raw words (7 effective words) took 0.0s, 15112 effective words/s
2019-07-17 11:16:23,296 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:16:23,296 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:16:23,296 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:16:23,296 EPOCH - 2 : training on 33 raw words (5 effective words) took 0.0s, 12505 effective words/s
2019-07-17 11:16:23,297 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:16:23,297 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:16:23,297 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:16:23,297 EPOCH - 3 : training on 33 raw words (7 effective words) took 0.0s, 15693 effective words/s
2019-07-17 11:16:23,298 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:16:23,298 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:16:23,298 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:16:23,298 EPOCH - 4 : training on 33 raw words (4 effective words) took 0.0s, 9374 effective words/s
2019-07-17 11:16:23,299 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:16:23,299 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:16:23,299 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:16:23,299 EPOCH - 5 : training on 33 raw words (7 effective words) took 0.0s, 16253 effective words/s
2019-07-17 11:16:23,299 training on a 165 raw words (30 effective words) took 0.0s, 5437 effective words/s
2019-07-17 11:16:23,299 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-17 11:16:44,781 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 11:16:44,782 collecting all words and their counts
2019-07-17 11:16:44,782 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 11:16:44,782 collected 17 word types and 2 unique tags from a corpus of 2 examples and 33 words
2019-07-17 11:16:44,782 Loading a fresh vocabulary
2019-07-17 11:16:44,782 min_count=0 retains 17 unique words (100% of original 17, drops 0)
2019-07-17 11:16:44,782 min_count=0 leaves 33 word corpus (100% of original 33, drops 0)
2019-07-17 11:16:44,782 deleting the raw counts dictionary of 17 items
2019-07-17 11:16:44,782 sample=0.001 downsamples 17 most-common words
2019-07-17 11:16:44,782 downsampling leaves estimated 4 word corpus (14.7% of prior 33)
2019-07-17 11:16:44,782 estimated required memory for 17 words and 5 dimensions: 9620 bytes
2019-07-17 11:16:44,782 resetting layer weights
2019-07-17 11:16:44,782 training model with 3 workers on 17 vocabulary and 5 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 11:16:44,784 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:16:44,784 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:16:44,784 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:16:44,784 EPOCH - 1 : training on 33 raw words (7 effective words) took 0.0s, 12307 effective words/s
2019-07-17 11:16:44,785 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:16:44,785 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:16:44,785 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:16:44,785 EPOCH - 2 : training on 33 raw words (5 effective words) took 0.0s, 8760 effective words/s
2019-07-17 11:16:44,786 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:16:44,786 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:16:44,786 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:16:44,786 EPOCH - 3 : training on 33 raw words (7 effective words) took 0.0s, 16603 effective words/s
2019-07-17 11:16:44,787 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:16:44,787 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:16:44,787 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:16:44,787 EPOCH - 4 : training on 33 raw words (4 effective words) took 0.0s, 8953 effective words/s
2019-07-17 11:16:44,788 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:16:44,788 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:16:44,788 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:16:44,788 EPOCH - 5 : training on 33 raw words (7 effective words) took 0.0s, 19744 effective words/s
2019-07-17 11:16:44,788 training on a 165 raw words (30 effective words) took 0.0s, 5432 effective words/s
2019-07-17 11:16:44,788 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-17 11:16:51,960 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 11:16:51,960 collecting all words and their counts
2019-07-17 11:16:51,963 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 11:16:51,963 collected 17 word types and 2 unique tags from a corpus of 2 examples and 33 words
2019-07-17 11:16:51,963 Loading a fresh vocabulary
2019-07-17 11:16:51,963 min_count=0 retains 17 unique words (100% of original 17, drops 0)
2019-07-17 11:16:51,963 min_count=0 leaves 33 word corpus (100% of original 33, drops 0)
2019-07-17 11:16:51,963 deleting the raw counts dictionary of 17 items
2019-07-17 11:16:51,963 sample=0.001 downsamples 17 most-common words
2019-07-17 11:16:51,963 downsampling leaves estimated 4 word corpus (14.7% of prior 33)
2019-07-17 11:16:51,963 estimated required memory for 17 words and 3 dimensions: 9332 bytes
2019-07-17 11:16:51,963 resetting layer weights
2019-07-17 11:16:51,964 training model with 3 workers on 17 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 11:16:51,965 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:16:51,965 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:16:51,965 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:16:51,965 EPOCH - 1 : training on 33 raw words (7 effective words) took 0.0s, 11943 effective words/s
2019-07-17 11:16:51,967 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:16:51,967 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:16:51,967 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:16:51,967 EPOCH - 2 : training on 33 raw words (5 effective words) took 0.0s, 8882 effective words/s
2019-07-17 11:16:51,968 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:16:51,969 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:16:51,969 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:16:51,969 EPOCH - 3 : training on 33 raw words (7 effective words) took 0.0s, 11930 effective words/s
2019-07-17 11:16:51,970 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:16:51,970 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:16:51,970 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:16:51,970 EPOCH - 4 : training on 33 raw words (4 effective words) took 0.0s, 7133 effective words/s
2019-07-17 11:16:51,972 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:16:51,972 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:16:51,972 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:16:51,972 EPOCH - 5 : training on 33 raw words (7 effective words) took 0.0s, 17434 effective words/s
2019-07-17 11:16:51,972 training on a 165 raw words (30 effective words) took 0.0s, 3608 effective words/s
2019-07-17 11:16:51,972 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-17 11:17:45,448 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 11:17:45,449 collecting all words and their counts
2019-07-17 11:17:45,449 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 11:17:45,449 collected 17 word types and 2 unique tags from a corpus of 2 examples and 33 words
2019-07-17 11:17:45,449 Loading a fresh vocabulary
2019-07-17 11:17:45,449 min_count=0 retains 17 unique words (100% of original 17, drops 0)
2019-07-17 11:17:45,449 min_count=0 leaves 33 word corpus (100% of original 33, drops 0)
2019-07-17 11:17:45,449 deleting the raw counts dictionary of 17 items
2019-07-17 11:17:45,449 sample=0.001 downsamples 17 most-common words
2019-07-17 11:17:45,449 downsampling leaves estimated 4 word corpus (14.7% of prior 33)
2019-07-17 11:17:45,450 estimated required memory for 17 words and 3 dimensions: 9332 bytes
2019-07-17 11:17:45,450 resetting layer weights
2019-07-17 11:17:45,450 training model with 3 workers on 17 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 11:17:45,451 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:17:45,451 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:17:45,451 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:17:45,451 EPOCH - 1 : training on 33 raw words (7 effective words) took 0.0s, 11995 effective words/s
2019-07-17 11:17:45,452 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:17:45,452 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:17:45,452 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:17:45,452 EPOCH - 2 : training on 33 raw words (5 effective words) took 0.0s, 9434 effective words/s
2019-07-17 11:17:45,453 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:17:45,453 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:17:45,454 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:17:45,454 EPOCH - 3 : training on 33 raw words (7 effective words) took 0.0s, 12084 effective words/s
2019-07-17 11:17:45,455 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:17:45,455 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:17:45,455 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:17:45,455 EPOCH - 4 : training on 33 raw words (4 effective words) took 0.0s, 6649 effective words/s
2019-07-17 11:17:45,456 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:17:45,456 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:17:45,456 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:17:45,456 EPOCH - 5 : training on 33 raw words (7 effective words) took 0.0s, 15137 effective words/s
2019-07-17 11:17:45,456 training on a 165 raw words (30 effective words) took 0.0s, 4947 effective words/s
2019-07-17 11:17:45,456 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-17 11:17:46,633 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 11:17:46,633 collecting all words and their counts
2019-07-17 11:17:46,633 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 11:17:46,671 PROGRESS: at example #10000, processed 127599 words (3411290/s), 14559 word types, 10000 tags
2019-07-17 11:17:46,688 collected 17571 word types and 14524 unique tags from a corpus of 14524 examples and 185031 words
2019-07-17 11:17:46,688 Loading a fresh vocabulary
2019-07-17 11:17:46,741 min_count=0 retains 17571 unique words (100% of original 17571, drops 0)
2019-07-17 11:17:46,742 min_count=0 leaves 185031 word corpus (100% of original 185031, drops 0)
2019-07-17 11:17:46,774 deleting the raw counts dictionary of 17571 items
2019-07-17 11:17:46,774 sample=0.001 downsamples 29 most-common words
2019-07-17 11:17:46,774 downsampling leaves estimated 146451 word corpus (79.1% of prior 185031)
2019-07-17 11:17:46,802 estimated required memory for 17571 words and 3 dimensions: 12286292 bytes
2019-07-17 11:17:46,802 resetting layer weights
2019-07-17 11:17:47,017 training model with 3 workers on 17571 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 11:17:47,378 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:17:47,387 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:17:47,388 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:17:47,388 EPOCH - 1 : training on 185031 raw words (161121 effective words) took 0.4s, 440513 effective words/s
2019-07-17 11:17:47,748 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:17:47,751 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:17:47,757 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:17:47,757 EPOCH - 2 : training on 185031 raw words (161034 effective words) took 0.4s, 441523 effective words/s
2019-07-17 11:17:48,113 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:17:48,120 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:17:48,126 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:17:48,126 EPOCH - 3 : training on 185031 raw words (161006 effective words) took 0.4s, 442640 effective words/s
2019-07-17 11:17:48,501 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:17:48,504 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:17:48,505 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:17:48,505 EPOCH - 4 : training on 185031 raw words (160797 effective words) took 0.4s, 429443 effective words/s
2019-07-17 11:17:48,868 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:17:48,872 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:17:48,872 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:17:48,872 EPOCH - 5 : training on 185031 raw words (160945 effective words) took 0.4s, 444505 effective words/s
2019-07-17 11:17:48,872 training on a 925155 raw words (804903 effective words) took 1.9s, 433845 effective words/s
2019-07-17 11:20:59,622 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 11:20:59,622 collecting all words and their counts
2019-07-17 11:20:59,625 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 11:20:59,625 collected 17 word types and 2 unique tags from a corpus of 2 examples and 33 words
2019-07-17 11:20:59,625 Loading a fresh vocabulary
2019-07-17 11:20:59,625 min_count=0 retains 17 unique words (100% of original 17, drops 0)
2019-07-17 11:20:59,625 min_count=0 leaves 33 word corpus (100% of original 33, drops 0)
2019-07-17 11:20:59,625 deleting the raw counts dictionary of 17 items
2019-07-17 11:20:59,625 sample=0.001 downsamples 17 most-common words
2019-07-17 11:20:59,625 downsampling leaves estimated 4 word corpus (14.7% of prior 33)
2019-07-17 11:20:59,625 estimated required memory for 17 words and 3 dimensions: 9332 bytes
2019-07-17 11:20:59,625 resetting layer weights
2019-07-17 11:20:59,626 training model with 3 workers on 17 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 11:20:59,627 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:20:59,627 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:20:59,627 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:20:59,627 EPOCH - 1 : training on 33 raw words (7 effective words) took 0.0s, 13292 effective words/s
2019-07-17 11:20:59,628 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:20:59,628 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:20:59,628 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:20:59,628 EPOCH - 2 : training on 33 raw words (5 effective words) took 0.0s, 9303 effective words/s
2019-07-17 11:20:59,629 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:20:59,629 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:20:59,629 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:20:59,629 EPOCH - 3 : training on 33 raw words (7 effective words) took 0.0s, 11735 effective words/s
2019-07-17 11:20:59,630 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:20:59,630 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:20:59,630 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:20:59,630 EPOCH - 4 : training on 33 raw words (4 effective words) took 0.0s, 9303 effective words/s
2019-07-17 11:20:59,631 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:20:59,631 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:20:59,631 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:20:59,631 EPOCH - 5 : training on 33 raw words (7 effective words) took 0.0s, 16946 effective words/s
2019-07-17 11:20:59,631 training on a 165 raw words (30 effective words) took 0.0s, 5410 effective words/s
2019-07-17 11:20:59,631 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-17 11:21:00,802 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 11:21:00,802 collecting all words and their counts
2019-07-17 11:21:00,803 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 11:21:00,876 PROGRESS: at example #10000, processed 127599 words (1733836/s), 14559 word types, 10000 tags
2019-07-17 11:21:00,895 collected 17571 word types and 14524 unique tags from a corpus of 14524 examples and 185031 words
2019-07-17 11:21:00,895 Loading a fresh vocabulary
2019-07-17 11:21:00,916 min_count=0 retains 17571 unique words (100% of original 17571, drops 0)
2019-07-17 11:21:00,916 min_count=0 leaves 185031 word corpus (100% of original 185031, drops 0)
2019-07-17 11:21:00,951 deleting the raw counts dictionary of 17571 items
2019-07-17 11:21:00,952 sample=0.001 downsamples 29 most-common words
2019-07-17 11:21:00,952 downsampling leaves estimated 146451 word corpus (79.1% of prior 185031)
2019-07-17 11:21:00,980 estimated required memory for 17571 words and 3 dimensions: 12286292 bytes
2019-07-17 11:21:00,980 resetting layer weights
2019-07-17 11:21:01,216 training model with 3 workers on 17571 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 11:21:01,593 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:21:01,609 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:21:01,610 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:21:01,610 EPOCH - 1 : training on 185031 raw words (160885 effective words) took 0.4s, 413248 effective words/s
2019-07-17 11:21:01,994 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:21:01,998 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:21:02,003 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:21:02,003 EPOCH - 2 : training on 185031 raw words (160978 effective words) took 0.4s, 415093 effective words/s
2019-07-17 11:21:02,393 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:21:02,397 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:21:02,399 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:21:02,399 EPOCH - 3 : training on 185031 raw words (160828 effective words) took 0.4s, 411208 effective words/s
2019-07-17 11:21:02,780 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:21:02,786 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:21:02,789 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:21:02,789 EPOCH - 4 : training on 185031 raw words (161025 effective words) took 0.4s, 418631 effective words/s
2019-07-17 11:21:03,178 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:21:03,183 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:21:03,187 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:21:03,187 EPOCH - 5 : training on 185031 raw words (160985 effective words) took 0.4s, 409458 effective words/s
2019-07-17 11:21:03,187 training on a 925155 raw words (804701 effective words) took 2.0s, 408125 effective words/s
2019-07-17 11:21:11,748 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 11:21:11,748 collecting all words and their counts
2019-07-17 11:21:11,748 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 11:21:11,748 collected 17 word types and 2 unique tags from a corpus of 2 examples and 33 words
2019-07-17 11:21:11,748 Loading a fresh vocabulary
2019-07-17 11:21:11,748 min_count=0 retains 17 unique words (100% of original 17, drops 0)
2019-07-17 11:21:11,748 min_count=0 leaves 33 word corpus (100% of original 33, drops 0)
2019-07-17 11:21:11,748 deleting the raw counts dictionary of 17 items
2019-07-17 11:21:11,749 sample=0.001 downsamples 17 most-common words
2019-07-17 11:21:11,749 downsampling leaves estimated 4 word corpus (14.7% of prior 33)
2019-07-17 11:21:11,749 estimated required memory for 17 words and 3 dimensions: 9332 bytes
2019-07-17 11:21:11,749 resetting layer weights
2019-07-17 11:21:11,749 training model with 3 workers on 17 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 11:21:11,751 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:21:11,751 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:21:11,751 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:21:11,751 EPOCH - 1 : training on 33 raw words (7 effective words) took 0.0s, 12374 effective words/s
2019-07-17 11:21:11,752 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:21:11,752 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:21:11,752 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:21:11,752 EPOCH - 2 : training on 33 raw words (5 effective words) took 0.0s, 10422 effective words/s
2019-07-17 11:21:11,753 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:21:11,753 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:21:11,753 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:21:11,753 EPOCH - 3 : training on 33 raw words (7 effective words) took 0.0s, 14447 effective words/s
2019-07-17 11:21:11,754 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:21:11,754 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:21:11,754 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:21:11,755 EPOCH - 4 : training on 33 raw words (4 effective words) took 0.0s, 8402 effective words/s
2019-07-17 11:21:11,755 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:21:11,755 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:21:11,755 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:21:11,755 EPOCH - 5 : training on 33 raw words (7 effective words) took 0.0s, 20516 effective words/s
2019-07-17 11:21:11,755 training on a 165 raw words (30 effective words) took 0.0s, 4733 effective words/s
2019-07-17 11:21:11,755 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-17 11:21:12,945 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 11:21:12,946 collecting all words and their counts
2019-07-17 11:21:12,946 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 11:21:12,985 PROGRESS: at example #10000, processed 127599 words (3246202/s), 14559 word types, 10000 tags
2019-07-17 11:21:13,003 collected 17571 word types and 14524 unique tags from a corpus of 14524 examples and 185031 words
2019-07-17 11:21:13,003 Loading a fresh vocabulary
2019-07-17 11:21:13,058 min_count=0 retains 17571 unique words (100% of original 17571, drops 0)
2019-07-17 11:21:13,058 min_count=0 leaves 185031 word corpus (100% of original 185031, drops 0)
2019-07-17 11:21:13,091 deleting the raw counts dictionary of 17571 items
2019-07-17 11:21:13,091 sample=0.001 downsamples 29 most-common words
2019-07-17 11:21:13,091 downsampling leaves estimated 146451 word corpus (79.1% of prior 185031)
2019-07-17 11:21:13,121 estimated required memory for 17571 words and 3 dimensions: 12286292 bytes
2019-07-17 11:21:13,121 resetting layer weights
2019-07-17 11:21:13,343 training model with 3 workers on 17571 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 11:21:13,729 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:21:13,732 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:21:13,738 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:21:13,738 EPOCH - 1 : training on 185031 raw words (160968 effective words) took 0.4s, 412279 effective words/s
2019-07-17 11:21:14,100 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:21:14,113 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:21:14,114 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:21:14,114 EPOCH - 2 : training on 185031 raw words (160982 effective words) took 0.4s, 434611 effective words/s
2019-07-17 11:21:14,496 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:21:14,497 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:21:14,505 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:21:14,505 EPOCH - 3 : training on 185031 raw words (160984 effective words) took 0.4s, 415953 effective words/s
2019-07-17 11:21:14,876 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:21:14,888 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:21:14,893 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:21:14,893 EPOCH - 4 : training on 185031 raw words (161083 effective words) took 0.4s, 421202 effective words/s
2019-07-17 11:21:15,266 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:21:15,274 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:21:15,277 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:21:15,277 EPOCH - 5 : training on 185031 raw words (160919 effective words) took 0.4s, 423787 effective words/s
2019-07-17 11:21:15,277 training on a 925155 raw words (804936 effective words) took 1.9s, 416128 effective words/s
2019-07-17 11:25:23,936 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 11:25:23,936 collecting all words and their counts
2019-07-17 11:25:23,936 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 11:25:23,936 collected 17 word types and 2 unique tags from a corpus of 2 examples and 33 words
2019-07-17 11:25:23,937 Loading a fresh vocabulary
2019-07-17 11:25:23,937 min_count=0 retains 17 unique words (100% of original 17, drops 0)
2019-07-17 11:25:23,937 min_count=0 leaves 33 word corpus (100% of original 33, drops 0)
2019-07-17 11:25:23,937 deleting the raw counts dictionary of 17 items
2019-07-17 11:25:23,937 sample=0.001 downsamples 17 most-common words
2019-07-17 11:25:23,937 downsampling leaves estimated 4 word corpus (14.7% of prior 33)
2019-07-17 11:25:23,937 estimated required memory for 17 words and 3 dimensions: 9332 bytes
2019-07-17 11:25:23,937 resetting layer weights
2019-07-17 11:25:23,937 training model with 3 workers on 17 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 11:25:23,938 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:25:23,938 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:25:23,939 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:25:23,939 EPOCH - 1 : training on 33 raw words (7 effective words) took 0.0s, 11706 effective words/s
2019-07-17 11:25:23,939 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:25:23,939 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:25:23,940 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:25:23,940 EPOCH - 2 : training on 33 raw words (5 effective words) took 0.0s, 10625 effective words/s
2019-07-17 11:25:23,940 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:25:23,941 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:25:23,941 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:25:23,941 EPOCH - 3 : training on 33 raw words (7 effective words) took 0.0s, 12826 effective words/s
2019-07-17 11:25:23,941 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:25:23,942 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:25:23,942 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:25:23,942 EPOCH - 4 : training on 33 raw words (4 effective words) took 0.0s, 9415 effective words/s
2019-07-17 11:25:23,942 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:25:23,943 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:25:23,943 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:25:23,943 EPOCH - 5 : training on 33 raw words (7 effective words) took 0.0s, 12362 effective words/s
2019-07-17 11:25:23,943 training on a 165 raw words (30 effective words) took 0.0s, 5415 effective words/s
2019-07-17 11:25:23,943 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-17 11:27:52,022 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 11:27:52,022 collecting all words and their counts
2019-07-17 11:27:52,022 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 11:27:52,022 collected 17 word types and 2 unique tags from a corpus of 2 examples and 33 words
2019-07-17 11:27:52,022 Loading a fresh vocabulary
2019-07-17 11:27:52,022 min_count=0 retains 17 unique words (100% of original 17, drops 0)
2019-07-17 11:27:52,022 min_count=0 leaves 33 word corpus (100% of original 33, drops 0)
2019-07-17 11:27:52,022 deleting the raw counts dictionary of 17 items
2019-07-17 11:27:52,022 sample=0.001 downsamples 17 most-common words
2019-07-17 11:27:52,022 downsampling leaves estimated 4 word corpus (14.7% of prior 33)
2019-07-17 11:27:52,022 estimated required memory for 17 words and 3 dimensions: 9332 bytes
2019-07-17 11:27:52,022 resetting layer weights
2019-07-17 11:27:52,023 training model with 3 workers on 17 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 11:27:52,024 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:27:52,024 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:27:52,024 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:27:52,024 EPOCH - 1 : training on 33 raw words (7 effective words) took 0.0s, 7549 effective words/s
2019-07-17 11:27:52,025 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:27:52,025 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:27:52,025 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:27:52,025 EPOCH - 2 : training on 33 raw words (5 effective words) took 0.0s, 12664 effective words/s
2019-07-17 11:27:52,026 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:27:52,026 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:27:52,026 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:27:52,026 EPOCH - 3 : training on 33 raw words (7 effective words) took 0.0s, 16183 effective words/s
2019-07-17 11:27:52,027 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:27:52,027 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:27:52,027 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:27:52,027 EPOCH - 4 : training on 33 raw words (4 effective words) took 0.0s, 6034 effective words/s
2019-07-17 11:27:52,028 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:27:52,028 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:27:52,028 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:27:52,028 EPOCH - 5 : training on 33 raw words (7 effective words) took 0.0s, 23895 effective words/s
2019-07-17 11:27:52,028 training on a 165 raw words (30 effective words) took 0.0s, 5199 effective words/s
2019-07-17 11:27:52,028 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-17 11:28:13,543 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 11:28:13,543 collecting all words and their counts
2019-07-17 11:28:13,544 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 11:28:13,544 collected 17 word types and 2 unique tags from a corpus of 2 examples and 33 words
2019-07-17 11:28:13,544 Loading a fresh vocabulary
2019-07-17 11:28:13,544 min_count=0 retains 17 unique words (100% of original 17, drops 0)
2019-07-17 11:28:13,544 min_count=0 leaves 33 word corpus (100% of original 33, drops 0)
2019-07-17 11:28:13,544 deleting the raw counts dictionary of 17 items
2019-07-17 11:28:13,544 sample=0.001 downsamples 17 most-common words
2019-07-17 11:28:13,544 downsampling leaves estimated 4 word corpus (14.7% of prior 33)
2019-07-17 11:28:13,544 estimated required memory for 17 words and 3 dimensions: 9332 bytes
2019-07-17 11:28:13,544 resetting layer weights
2019-07-17 11:28:13,544 training model with 3 workers on 17 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 11:28:13,545 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:28:13,546 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:28:13,546 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:28:13,546 EPOCH - 1 : training on 33 raw words (7 effective words) took 0.0s, 17659 effective words/s
2019-07-17 11:28:13,547 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:28:13,547 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:28:13,547 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:28:13,547 EPOCH - 2 : training on 33 raw words (5 effective words) took 0.0s, 8632 effective words/s
2019-07-17 11:28:13,548 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:28:13,548 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:28:13,548 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:28:13,548 EPOCH - 3 : training on 33 raw words (7 effective words) took 0.0s, 14399 effective words/s
2019-07-17 11:28:13,549 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:28:13,549 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:28:13,549 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:28:13,549 EPOCH - 4 : training on 33 raw words (4 effective words) took 0.0s, 8896 effective words/s
2019-07-17 11:28:13,550 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:28:13,550 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:28:13,550 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:28:13,550 EPOCH - 5 : training on 33 raw words (7 effective words) took 0.0s, 12751 effective words/s
2019-07-17 11:28:13,550 training on a 165 raw words (30 effective words) took 0.0s, 5102 effective words/s
2019-07-17 11:28:13,550 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-17 11:28:14,763 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 11:28:14,763 collecting all words and their counts
2019-07-17 11:28:14,763 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 11:28:14,802 PROGRESS: at example #10000, processed 127599 words (3301235/s), 14559 word types, 10000 tags
2019-07-17 11:28:14,820 collected 17571 word types and 14524 unique tags from a corpus of 14524 examples and 185031 words
2019-07-17 11:28:14,820 Loading a fresh vocabulary
2019-07-17 11:28:14,841 min_count=0 retains 17571 unique words (100% of original 17571, drops 0)
2019-07-17 11:28:14,841 min_count=0 leaves 185031 word corpus (100% of original 185031, drops 0)
2019-07-17 11:28:14,873 deleting the raw counts dictionary of 17571 items
2019-07-17 11:28:14,873 sample=0.001 downsamples 29 most-common words
2019-07-17 11:28:14,873 downsampling leaves estimated 146451 word corpus (79.1% of prior 185031)
2019-07-17 11:28:14,900 estimated required memory for 17571 words and 3 dimensions: 12286292 bytes
2019-07-17 11:28:14,901 resetting layer weights
2019-07-17 11:28:15,122 training model with 3 workers on 17571 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 11:28:15,517 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:28:15,519 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:28:15,519 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:28:15,519 EPOCH - 1 : training on 185031 raw words (160982 effective words) took 0.4s, 410608 effective words/s
2019-07-17 11:28:15,897 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:28:15,904 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:28:15,910 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:28:15,910 EPOCH - 2 : training on 185031 raw words (161013 effective words) took 0.4s, 418689 effective words/s
2019-07-17 11:28:16,288 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:28:16,296 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:28:16,297 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:28:16,297 EPOCH - 3 : training on 185031 raw words (160974 effective words) took 0.4s, 420506 effective words/s
2019-07-17 11:28:16,684 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:28:16,689 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:28:16,696 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:28:16,697 EPOCH - 4 : training on 185031 raw words (160906 effective words) took 0.4s, 407783 effective words/s
2019-07-17 11:28:17,080 worker thread finished; awaiting finish of 2 more threads
2019-07-17 11:28:17,082 worker thread finished; awaiting finish of 1 more threads
2019-07-17 11:28:17,091 worker thread finished; awaiting finish of 0 more threads
2019-07-17 11:28:17,091 EPOCH - 5 : training on 185031 raw words (160899 effective words) took 0.4s, 413473 effective words/s
2019-07-17 11:28:17,091 training on a 925155 raw words (804774 effective words) took 2.0s, 408797 effective words/s
2019-07-17 15:10:47,833 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 15:10:47,833 collecting all words and their counts
2019-07-17 15:10:47,834 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 15:10:47,834 collected 17 word types and 2 unique tags from a corpus of 2 examples and 33 words
2019-07-17 15:10:47,834 Loading a fresh vocabulary
2019-07-17 15:10:47,834 min_count=0 retains 17 unique words (100% of original 17, drops 0)
2019-07-17 15:10:47,834 min_count=0 leaves 33 word corpus (100% of original 33, drops 0)
2019-07-17 15:10:47,834 deleting the raw counts dictionary of 17 items
2019-07-17 15:10:47,834 sample=0.001 downsamples 17 most-common words
2019-07-17 15:10:47,834 downsampling leaves estimated 4 word corpus (14.7% of prior 33)
2019-07-17 15:10:47,834 estimated required memory for 17 words and 3 dimensions: 9332 bytes
2019-07-17 15:10:47,834 resetting layer weights
2019-07-17 15:10:47,834 training model with 3 workers on 17 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 15:10:47,835 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:10:47,836 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:10:47,836 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:10:47,836 EPOCH - 1 : training on 33 raw words (7 effective words) took 0.0s, 11694 effective words/s
2019-07-17 15:10:47,837 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:10:47,837 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:10:47,837 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:10:47,837 EPOCH - 2 : training on 33 raw words (5 effective words) took 0.0s, 5715 effective words/s
2019-07-17 15:10:47,838 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:10:47,838 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:10:47,838 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:10:47,838 EPOCH - 3 : training on 33 raw words (7 effective words) took 0.0s, 15201 effective words/s
2019-07-17 15:10:47,839 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:10:47,839 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:10:47,839 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:10:47,839 EPOCH - 4 : training on 33 raw words (4 effective words) took 0.0s, 7845 effective words/s
2019-07-17 15:10:47,840 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:10:47,840 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:10:47,840 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:10:47,840 EPOCH - 5 : training on 33 raw words (7 effective words) took 0.0s, 16691 effective words/s
2019-07-17 15:10:47,840 training on a 165 raw words (30 effective words) took 0.0s, 5073 effective words/s
2019-07-17 15:10:47,840 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-17 15:10:49,047 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 15:10:49,048 collecting all words and their counts
2019-07-17 15:10:49,048 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 15:10:49,087 PROGRESS: at example #10000, processed 127599 words (3254837/s), 14559 word types, 10000 tags
2019-07-17 15:10:49,105 collected 17571 word types and 14524 unique tags from a corpus of 14524 examples and 185031 words
2019-07-17 15:10:49,105 Loading a fresh vocabulary
2019-07-17 15:10:49,160 min_count=0 retains 17571 unique words (100% of original 17571, drops 0)
2019-07-17 15:10:49,160 min_count=0 leaves 185031 word corpus (100% of original 185031, drops 0)
2019-07-17 15:10:49,193 deleting the raw counts dictionary of 17571 items
2019-07-17 15:10:49,193 sample=0.001 downsamples 29 most-common words
2019-07-17 15:10:49,193 downsampling leaves estimated 146451 word corpus (79.1% of prior 185031)
2019-07-17 15:10:49,224 estimated required memory for 17571 words and 3 dimensions: 12286292 bytes
2019-07-17 15:10:49,224 resetting layer weights
2019-07-17 15:10:49,457 training model with 3 workers on 17571 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 15:10:49,828 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:10:49,834 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:10:49,839 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:10:49,840 EPOCH - 1 : training on 185031 raw words (160943 effective words) took 0.4s, 426986 effective words/s
2019-07-17 15:10:50,224 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:10:50,231 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:10:50,237 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:10:50,237 EPOCH - 2 : training on 185031 raw words (161104 effective words) took 0.4s, 410779 effective words/s
2019-07-17 15:10:50,626 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:10:50,634 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:10:50,635 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:10:50,635 EPOCH - 3 : training on 185031 raw words (160802 effective words) took 0.4s, 409695 effective words/s
2019-07-17 15:10:51,007 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:10:51,018 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:10:51,022 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:10:51,023 EPOCH - 4 : training on 185031 raw words (160972 effective words) took 0.4s, 420600 effective words/s
2019-07-17 15:10:51,404 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:10:51,414 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:10:51,418 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:10:51,418 EPOCH - 5 : training on 185031 raw words (161139 effective words) took 0.4s, 412514 effective words/s
2019-07-17 15:10:51,418 training on a 925155 raw words (804960 effective words) took 2.0s, 410564 effective words/s
2019-07-17 15:11:46,973 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 15:11:46,973 collecting all words and their counts
2019-07-17 15:11:46,973 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 15:11:46,973 collected 17 word types and 2 unique tags from a corpus of 2 examples and 33 words
2019-07-17 15:11:46,973 Loading a fresh vocabulary
2019-07-17 15:11:46,974 min_count=0 retains 17 unique words (100% of original 17, drops 0)
2019-07-17 15:11:46,974 min_count=0 leaves 33 word corpus (100% of original 33, drops 0)
2019-07-17 15:11:46,974 deleting the raw counts dictionary of 17 items
2019-07-17 15:11:46,974 sample=0.001 downsamples 17 most-common words
2019-07-17 15:11:46,974 downsampling leaves estimated 4 word corpus (14.7% of prior 33)
2019-07-17 15:11:46,974 estimated required memory for 17 words and 3 dimensions: 9332 bytes
2019-07-17 15:11:46,974 resetting layer weights
2019-07-17 15:11:46,974 training model with 3 workers on 17 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 15:11:46,975 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:11:46,975 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:11:46,975 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:11:46,975 EPOCH - 1 : training on 33 raw words (7 effective words) took 0.0s, 15188 effective words/s
2019-07-17 15:11:46,977 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:11:46,977 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:11:46,977 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:11:46,977 EPOCH - 2 : training on 33 raw words (5 effective words) took 0.0s, 7281 effective words/s
2019-07-17 15:11:46,978 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:11:46,978 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:11:46,978 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:11:46,978 EPOCH - 3 : training on 33 raw words (7 effective words) took 0.0s, 14916 effective words/s
2019-07-17 15:11:46,979 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:11:46,979 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:11:46,979 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:11:46,979 EPOCH - 4 : training on 33 raw words (4 effective words) took 0.0s, 10261 effective words/s
2019-07-17 15:11:46,980 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:11:46,980 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:11:46,980 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:11:46,980 EPOCH - 5 : training on 33 raw words (7 effective words) took 0.0s, 16547 effective words/s
2019-07-17 15:11:46,980 training on a 165 raw words (30 effective words) took 0.0s, 5291 effective words/s
2019-07-17 15:11:46,980 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-17 15:11:48,192 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 15:11:48,193 collecting all words and their counts
2019-07-17 15:11:48,193 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 15:11:48,229 PROGRESS: at example #10000, processed 127599 words (3511091/s), 14559 word types, 10000 tags
2019-07-17 15:11:48,246 collected 17571 word types and 14524 unique tags from a corpus of 14524 examples and 185031 words
2019-07-17 15:11:48,246 Loading a fresh vocabulary
2019-07-17 15:11:48,266 min_count=0 retains 17571 unique words (100% of original 17571, drops 0)
2019-07-17 15:11:48,266 min_count=0 leaves 185031 word corpus (100% of original 185031, drops 0)
2019-07-17 15:11:48,297 deleting the raw counts dictionary of 17571 items
2019-07-17 15:11:48,298 sample=0.001 downsamples 29 most-common words
2019-07-17 15:11:48,298 downsampling leaves estimated 146451 word corpus (79.1% of prior 185031)
2019-07-17 15:11:48,325 estimated required memory for 17571 words and 3 dimensions: 12286292 bytes
2019-07-17 15:11:48,325 resetting layer weights
2019-07-17 15:11:48,542 training model with 3 workers on 17571 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 15:11:48,908 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:11:48,917 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:11:48,922 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:11:48,922 EPOCH - 1 : training on 185031 raw words (160875 effective words) took 0.4s, 428450 effective words/s
2019-07-17 15:11:49,300 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:11:49,308 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:11:49,310 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:11:49,310 EPOCH - 2 : training on 185031 raw words (161021 effective words) took 0.4s, 420417 effective words/s
2019-07-17 15:11:49,698 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:11:49,701 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:11:49,709 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:11:49,709 EPOCH - 3 : training on 185031 raw words (160925 effective words) took 0.4s, 409158 effective words/s
2019-07-17 15:11:50,069 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:11:50,081 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:11:50,082 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:11:50,082 EPOCH - 4 : training on 185031 raw words (161052 effective words) took 0.4s, 437518 effective words/s
2019-07-17 15:11:50,450 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:11:50,461 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:11:50,466 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:11:50,466 EPOCH - 5 : training on 185031 raw words (160853 effective words) took 0.4s, 423314 effective words/s
2019-07-17 15:11:50,466 training on a 925155 raw words (804726 effective words) took 1.9s, 418139 effective words/s
2019-07-17 15:14:21,494 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 15:14:21,494 collecting all words and their counts
2019-07-17 15:14:21,494 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 15:14:21,494 collected 17 word types and 2 unique tags from a corpus of 2 examples and 33 words
2019-07-17 15:14:21,494 Loading a fresh vocabulary
2019-07-17 15:14:21,494 min_count=0 retains 17 unique words (100% of original 17, drops 0)
2019-07-17 15:14:21,494 min_count=0 leaves 33 word corpus (100% of original 33, drops 0)
2019-07-17 15:14:21,495 deleting the raw counts dictionary of 17 items
2019-07-17 15:14:21,495 sample=0.001 downsamples 17 most-common words
2019-07-17 15:14:21,495 downsampling leaves estimated 4 word corpus (14.7% of prior 33)
2019-07-17 15:14:21,495 estimated required memory for 17 words and 3 dimensions: 9332 bytes
2019-07-17 15:14:21,495 resetting layer weights
2019-07-17 15:14:21,495 training model with 3 workers on 17 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 15:14:21,496 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:14:21,496 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:14:21,496 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:14:21,496 EPOCH - 1 : training on 33 raw words (7 effective words) took 0.0s, 12669 effective words/s
2019-07-17 15:14:21,497 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:14:21,497 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:14:21,497 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:14:21,497 EPOCH - 2 : training on 33 raw words (5 effective words) took 0.0s, 12699 effective words/s
2019-07-17 15:14:21,498 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:14:21,498 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:14:21,498 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:14:21,498 EPOCH - 3 : training on 33 raw words (7 effective words) took 0.0s, 16289 effective words/s
2019-07-17 15:14:21,499 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:14:21,499 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:14:21,499 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:14:21,499 EPOCH - 4 : training on 33 raw words (4 effective words) took 0.0s, 8943 effective words/s
2019-07-17 15:14:21,503 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:14:21,503 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:14:21,503 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:14:21,503 EPOCH - 5 : training on 33 raw words (7 effective words) took 0.0s, 10884 effective words/s
2019-07-17 15:14:21,503 training on a 165 raw words (30 effective words) took 0.0s, 3610 effective words/s
2019-07-17 15:14:21,503 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-17 15:15:07,732 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 15:15:07,732 collecting all words and their counts
2019-07-17 15:15:07,732 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 15:15:07,732 collected 17 word types and 2 unique tags from a corpus of 2 examples and 33 words
2019-07-17 15:15:07,732 Loading a fresh vocabulary
2019-07-17 15:15:07,732 min_count=0 retains 17 unique words (100% of original 17, drops 0)
2019-07-17 15:15:07,732 min_count=0 leaves 33 word corpus (100% of original 33, drops 0)
2019-07-17 15:15:07,732 deleting the raw counts dictionary of 17 items
2019-07-17 15:15:07,732 sample=0.001 downsamples 17 most-common words
2019-07-17 15:15:07,732 downsampling leaves estimated 4 word corpus (14.7% of prior 33)
2019-07-17 15:15:07,732 estimated required memory for 17 words and 3 dimensions: 9332 bytes
2019-07-17 15:15:07,732 resetting layer weights
2019-07-17 15:15:07,733 training model with 3 workers on 17 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 15:15:07,734 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:15:07,734 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:15:07,734 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:15:07,734 EPOCH - 1 : training on 33 raw words (7 effective words) took 0.0s, 11948 effective words/s
2019-07-17 15:15:07,735 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:15:07,735 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:15:07,735 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:15:07,735 EPOCH - 2 : training on 33 raw words (5 effective words) took 0.0s, 12777 effective words/s
2019-07-17 15:15:07,736 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:15:07,736 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:15:07,736 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:15:07,736 EPOCH - 3 : training on 33 raw words (7 effective words) took 0.0s, 17434 effective words/s
2019-07-17 15:15:07,736 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:15:07,737 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:15:07,737 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:15:07,737 EPOCH - 4 : training on 33 raw words (4 effective words) took 0.0s, 11687 effective words/s
2019-07-17 15:15:07,737 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:15:07,738 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:15:07,738 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:15:07,738 EPOCH - 5 : training on 33 raw words (7 effective words) took 0.0s, 11457 effective words/s
2019-07-17 15:15:07,738 training on a 165 raw words (30 effective words) took 0.0s, 5878 effective words/s
2019-07-17 15:15:07,738 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-17 15:15:08,919 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 15:15:08,919 collecting all words and their counts
2019-07-17 15:15:08,919 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 15:15:08,960 PROGRESS: at example #10000, processed 127599 words (3169788/s), 14559 word types, 10000 tags
2019-07-17 15:15:08,978 collected 17571 word types and 14524 unique tags from a corpus of 14524 examples and 185031 words
2019-07-17 15:15:08,978 Loading a fresh vocabulary
2019-07-17 15:15:09,034 min_count=0 retains 17571 unique words (100% of original 17571, drops 0)
2019-07-17 15:15:09,034 min_count=0 leaves 185031 word corpus (100% of original 185031, drops 0)
2019-07-17 15:15:09,066 deleting the raw counts dictionary of 17571 items
2019-07-17 15:15:09,067 sample=0.001 downsamples 29 most-common words
2019-07-17 15:15:09,067 downsampling leaves estimated 146451 word corpus (79.1% of prior 185031)
2019-07-17 15:15:09,096 estimated required memory for 17571 words and 3 dimensions: 12286292 bytes
2019-07-17 15:15:09,096 resetting layer weights
2019-07-17 15:15:09,316 training model with 3 workers on 17571 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 15:15:09,696 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:15:09,696 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:15:09,700 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:15:09,700 EPOCH - 1 : training on 185031 raw words (161011 effective words) took 0.4s, 425162 effective words/s
2019-07-17 15:15:10,057 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:15:10,072 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:15:10,073 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:15:10,073 EPOCH - 2 : training on 185031 raw words (161159 effective words) took 0.4s, 438368 effective words/s
2019-07-17 15:15:10,443 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:15:10,445 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:15:10,453 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:15:10,453 EPOCH - 3 : training on 185031 raw words (160987 effective words) took 0.4s, 428780 effective words/s
2019-07-17 15:15:10,814 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:15:10,817 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:15:10,824 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:15:10,824 EPOCH - 4 : training on 185031 raw words (161030 effective words) took 0.4s, 439420 effective words/s
2019-07-17 15:15:11,187 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:15:11,192 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:15:11,198 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:15:11,198 EPOCH - 5 : training on 185031 raw words (161196 effective words) took 0.4s, 436892 effective words/s
2019-07-17 15:15:11,198 training on a 925155 raw words (805383 effective words) took 1.9s, 428077 effective words/s
2019-07-17 15:15:54,903 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 15:15:54,904 collecting all words and their counts
2019-07-17 15:15:54,904 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 15:15:54,904 collected 17 word types and 2 unique tags from a corpus of 2 examples and 33 words
2019-07-17 15:15:54,904 Loading a fresh vocabulary
2019-07-17 15:15:54,904 min_count=0 retains 17 unique words (100% of original 17, drops 0)
2019-07-17 15:15:54,904 min_count=0 leaves 33 word corpus (100% of original 33, drops 0)
2019-07-17 15:15:54,904 deleting the raw counts dictionary of 17 items
2019-07-17 15:15:54,904 sample=0.001 downsamples 17 most-common words
2019-07-17 15:15:54,904 downsampling leaves estimated 4 word corpus (14.7% of prior 33)
2019-07-17 15:15:54,904 estimated required memory for 17 words and 3 dimensions: 9332 bytes
2019-07-17 15:15:54,904 resetting layer weights
2019-07-17 15:15:54,905 training model with 3 workers on 17 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 15:15:54,906 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:15:54,906 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:15:54,906 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:15:54,906 EPOCH - 1 : training on 33 raw words (7 effective words) took 0.0s, 11120 effective words/s
2019-07-17 15:15:54,907 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:15:54,907 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:15:54,907 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:15:54,907 EPOCH - 2 : training on 33 raw words (5 effective words) took 0.0s, 9416 effective words/s
2019-07-17 15:15:54,908 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:15:54,908 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:15:54,908 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:15:54,908 EPOCH - 3 : training on 33 raw words (7 effective words) took 0.0s, 12199 effective words/s
2019-07-17 15:15:54,909 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:15:54,909 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:15:54,909 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:15:54,909 EPOCH - 4 : training on 33 raw words (4 effective words) took 0.0s, 8296 effective words/s
2019-07-17 15:15:54,911 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:15:54,911 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:15:54,911 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:15:54,911 EPOCH - 5 : training on 33 raw words (7 effective words) took 0.0s, 12687 effective words/s
2019-07-17 15:15:54,912 training on a 165 raw words (30 effective words) took 0.0s, 4314 effective words/s
2019-07-17 15:15:54,912 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-17 15:15:56,094 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 15:15:56,095 collecting all words and their counts
2019-07-17 15:15:56,099 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 15:15:56,135 PROGRESS: at example #10000, processed 127599 words (3541083/s), 14559 word types, 10000 tags
2019-07-17 15:15:56,152 collected 17571 word types and 14524 unique tags from a corpus of 14524 examples and 185031 words
2019-07-17 15:15:56,152 Loading a fresh vocabulary
2019-07-17 15:15:56,172 min_count=0 retains 17571 unique words (100% of original 17571, drops 0)
2019-07-17 15:15:56,172 min_count=0 leaves 185031 word corpus (100% of original 185031, drops 0)
2019-07-17 15:15:56,202 deleting the raw counts dictionary of 17571 items
2019-07-17 15:15:56,202 sample=0.001 downsamples 29 most-common words
2019-07-17 15:15:56,202 downsampling leaves estimated 146451 word corpus (79.1% of prior 185031)
2019-07-17 15:15:56,229 estimated required memory for 17571 words and 3 dimensions: 12286292 bytes
2019-07-17 15:15:56,230 resetting layer weights
2019-07-17 15:15:56,449 training model with 3 workers on 17571 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 15:15:56,814 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:15:56,819 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:15:56,827 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:15:56,827 EPOCH - 1 : training on 185031 raw words (160935 effective words) took 0.4s, 431771 effective words/s
2019-07-17 15:15:57,187 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:15:57,193 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:15:57,199 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:15:57,199 EPOCH - 2 : training on 185031 raw words (160843 effective words) took 0.4s, 438086 effective words/s
2019-07-17 15:15:57,566 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:15:57,569 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:15:57,574 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:15:57,574 EPOCH - 3 : training on 185031 raw words (160982 effective words) took 0.4s, 435123 effective words/s
2019-07-17 15:15:57,937 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:15:57,946 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:15:57,949 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:15:57,949 EPOCH - 4 : training on 185031 raw words (161017 effective words) took 0.4s, 434701 effective words/s
2019-07-17 15:15:58,311 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:15:58,312 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:15:58,315 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:15:58,315 EPOCH - 5 : training on 185031 raw words (161116 effective words) took 0.4s, 445532 effective words/s
2019-07-17 15:15:58,315 training on a 925155 raw words (804893 effective words) took 1.9s, 431394 effective words/s
2019-07-17 15:16:09,059 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 15:16:09,059 collecting all words and their counts
2019-07-17 15:16:09,059 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 15:16:09,059 collected 17 word types and 2 unique tags from a corpus of 2 examples and 33 words
2019-07-17 15:16:09,059 Loading a fresh vocabulary
2019-07-17 15:16:09,059 min_count=0 retains 17 unique words (100% of original 17, drops 0)
2019-07-17 15:16:09,059 min_count=0 leaves 33 word corpus (100% of original 33, drops 0)
2019-07-17 15:16:09,059 deleting the raw counts dictionary of 17 items
2019-07-17 15:16:09,060 sample=0.001 downsamples 17 most-common words
2019-07-17 15:16:09,060 downsampling leaves estimated 4 word corpus (14.7% of prior 33)
2019-07-17 15:16:09,060 estimated required memory for 17 words and 3 dimensions: 9332 bytes
2019-07-17 15:16:09,060 resetting layer weights
2019-07-17 15:16:09,060 training model with 3 workers on 17 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 15:16:09,062 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:16:09,062 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:16:09,062 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:16:09,062 EPOCH - 1 : training on 33 raw words (7 effective words) took 0.0s, 11252 effective words/s
2019-07-17 15:16:09,063 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:16:09,063 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:16:09,063 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:16:09,064 EPOCH - 2 : training on 33 raw words (5 effective words) took 0.0s, 10363 effective words/s
2019-07-17 15:16:09,064 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:16:09,065 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:16:09,065 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:16:09,065 EPOCH - 3 : training on 33 raw words (7 effective words) took 0.0s, 11021 effective words/s
2019-07-17 15:16:09,066 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:16:09,066 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:16:09,066 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:16:09,066 EPOCH - 4 : training on 33 raw words (4 effective words) took 0.0s, 8519 effective words/s
2019-07-17 15:16:09,067 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:16:09,067 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:16:09,067 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:16:09,067 EPOCH - 5 : training on 33 raw words (7 effective words) took 0.0s, 15485 effective words/s
2019-07-17 15:16:09,067 training on a 165 raw words (30 effective words) took 0.0s, 4559 effective words/s
2019-07-17 15:16:09,067 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-17 15:16:10,244 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 15:16:10,244 collecting all words and their counts
2019-07-17 15:16:10,244 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 15:16:10,285 PROGRESS: at example #10000, processed 127599 words (3183060/s), 14559 word types, 10000 tags
2019-07-17 15:16:10,303 collected 17571 word types and 14524 unique tags from a corpus of 14524 examples and 185031 words
2019-07-17 15:16:10,303 Loading a fresh vocabulary
2019-07-17 15:16:10,366 min_count=0 retains 17571 unique words (100% of original 17571, drops 0)
2019-07-17 15:16:10,366 min_count=0 leaves 185031 word corpus (100% of original 185031, drops 0)
2019-07-17 15:16:10,402 deleting the raw counts dictionary of 17571 items
2019-07-17 15:16:10,402 sample=0.001 downsamples 29 most-common words
2019-07-17 15:16:10,402 downsampling leaves estimated 146451 word corpus (79.1% of prior 185031)
2019-07-17 15:16:10,432 estimated required memory for 17571 words and 3 dimensions: 12286292 bytes
2019-07-17 15:16:10,432 resetting layer weights
2019-07-17 15:16:10,652 training model with 3 workers on 17571 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 15:16:11,017 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:16:11,025 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:16:11,027 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:16:11,027 EPOCH - 1 : training on 185031 raw words (161031 effective words) took 0.4s, 434843 effective words/s
2019-07-17 15:16:11,403 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:16:11,404 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:16:11,413 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:16:11,413 EPOCH - 2 : training on 185031 raw words (161006 effective words) took 0.4s, 421877 effective words/s
2019-07-17 15:16:11,771 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:16:11,777 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:16:11,784 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:16:11,784 EPOCH - 3 : training on 185031 raw words (160928 effective words) took 0.4s, 439502 effective words/s
2019-07-17 15:16:12,159 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:16:12,162 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:16:12,172 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:16:12,172 EPOCH - 4 : training on 185031 raw words (161032 effective words) took 0.4s, 420840 effective words/s
2019-07-17 15:16:12,548 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:16:12,549 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:16:12,554 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:16:12,554 EPOCH - 5 : training on 185031 raw words (160868 effective words) took 0.4s, 426037 effective words/s
2019-07-17 15:16:12,554 training on a 925155 raw words (804865 effective words) took 1.9s, 423057 effective words/s
2019-07-17 15:16:21,119 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 15:16:21,119 collecting all words and their counts
2019-07-17 15:16:21,119 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 15:16:21,119 collected 17 word types and 2 unique tags from a corpus of 2 examples and 33 words
2019-07-17 15:16:21,119 Loading a fresh vocabulary
2019-07-17 15:16:21,119 min_count=0 retains 17 unique words (100% of original 17, drops 0)
2019-07-17 15:16:21,119 min_count=0 leaves 33 word corpus (100% of original 33, drops 0)
2019-07-17 15:16:21,119 deleting the raw counts dictionary of 17 items
2019-07-17 15:16:21,119 sample=0.001 downsamples 17 most-common words
2019-07-17 15:16:21,119 downsampling leaves estimated 4 word corpus (14.7% of prior 33)
2019-07-17 15:16:21,119 estimated required memory for 17 words and 3 dimensions: 9332 bytes
2019-07-17 15:16:21,119 resetting layer weights
2019-07-17 15:16:21,120 training model with 3 workers on 17 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 15:16:21,121 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:16:21,121 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:16:21,121 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:16:21,121 EPOCH - 1 : training on 33 raw words (7 effective words) took 0.0s, 11170 effective words/s
2019-07-17 15:16:21,122 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:16:21,122 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:16:21,122 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:16:21,122 EPOCH - 2 : training on 33 raw words (5 effective words) took 0.0s, 11236 effective words/s
2019-07-17 15:16:21,123 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:16:21,123 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:16:21,123 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:16:21,123 EPOCH - 3 : training on 33 raw words (7 effective words) took 0.0s, 15916 effective words/s
2019-07-17 15:16:21,124 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:16:21,124 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:16:21,124 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:16:21,124 EPOCH - 4 : training on 33 raw words (4 effective words) took 0.0s, 10464 effective words/s
2019-07-17 15:16:21,125 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:16:21,125 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:16:21,125 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:16:21,125 EPOCH - 5 : training on 33 raw words (7 effective words) took 0.0s, 8922 effective words/s
2019-07-17 15:16:21,126 training on a 165 raw words (30 effective words) took 0.0s, 5167 effective words/s
2019-07-17 15:16:21,126 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-17 15:16:22,334 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 15:16:22,334 collecting all words and their counts
2019-07-17 15:16:22,335 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 15:16:22,380 PROGRESS: at example #10000, processed 127599 words (2810134/s), 14559 word types, 10000 tags
2019-07-17 15:16:22,398 collected 17571 word types and 14524 unique tags from a corpus of 14524 examples and 185031 words
2019-07-17 15:16:22,398 Loading a fresh vocabulary
2019-07-17 15:16:22,420 min_count=0 retains 17571 unique words (100% of original 17571, drops 0)
2019-07-17 15:16:22,420 min_count=0 leaves 185031 word corpus (100% of original 185031, drops 0)
2019-07-17 15:16:22,452 deleting the raw counts dictionary of 17571 items
2019-07-17 15:16:22,453 sample=0.001 downsamples 29 most-common words
2019-07-17 15:16:22,453 downsampling leaves estimated 146451 word corpus (79.1% of prior 185031)
2019-07-17 15:16:22,481 estimated required memory for 17571 words and 3 dimensions: 12286292 bytes
2019-07-17 15:16:22,481 resetting layer weights
2019-07-17 15:16:22,700 training model with 3 workers on 17571 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 15:16:23,069 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:16:23,072 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:16:23,075 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:16:23,075 EPOCH - 1 : training on 185031 raw words (160781 effective words) took 0.4s, 433576 effective words/s
2019-07-17 15:16:23,456 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:16:23,461 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:16:23,466 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:16:23,467 EPOCH - 2 : training on 185031 raw words (161037 effective words) took 0.4s, 416629 effective words/s
2019-07-17 15:16:23,826 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:16:23,834 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:16:23,841 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:16:23,841 EPOCH - 3 : training on 185031 raw words (160792 effective words) took 0.4s, 435273 effective words/s
2019-07-17 15:16:24,199 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:16:24,204 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:16:24,208 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:16:24,208 EPOCH - 4 : training on 185031 raw words (160964 effective words) took 0.4s, 443779 effective words/s
2019-07-17 15:16:24,591 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:16:24,597 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:16:24,604 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:16:24,604 EPOCH - 5 : training on 185031 raw words (161017 effective words) took 0.4s, 411557 effective words/s
2019-07-17 15:16:24,604 training on a 925155 raw words (804591 effective words) took 1.9s, 422437 effective words/s
2019-07-17 15:16:49,030 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 15:16:49,031 collecting all words and their counts
2019-07-17 15:16:49,031 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 15:16:49,031 collected 17 word types and 2 unique tags from a corpus of 2 examples and 33 words
2019-07-17 15:16:49,031 Loading a fresh vocabulary
2019-07-17 15:16:49,031 min_count=0 retains 17 unique words (100% of original 17, drops 0)
2019-07-17 15:16:49,031 min_count=0 leaves 33 word corpus (100% of original 33, drops 0)
2019-07-17 15:16:49,031 deleting the raw counts dictionary of 17 items
2019-07-17 15:16:49,031 sample=0.001 downsamples 17 most-common words
2019-07-17 15:16:49,031 downsampling leaves estimated 4 word corpus (14.7% of prior 33)
2019-07-17 15:16:49,031 estimated required memory for 17 words and 3 dimensions: 9332 bytes
2019-07-17 15:16:49,031 resetting layer weights
2019-07-17 15:16:49,032 training model with 3 workers on 17 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 15:16:49,033 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:16:49,033 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:16:49,033 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:16:49,033 EPOCH - 1 : training on 33 raw words (7 effective words) took 0.0s, 12109 effective words/s
2019-07-17 15:16:49,034 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:16:49,034 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:16:49,034 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:16:49,034 EPOCH - 2 : training on 33 raw words (5 effective words) took 0.0s, 11032 effective words/s
2019-07-17 15:16:49,035 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:16:49,035 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:16:49,035 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:16:49,035 EPOCH - 3 : training on 33 raw words (7 effective words) took 0.0s, 12174 effective words/s
2019-07-17 15:16:49,037 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:16:49,037 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:16:49,037 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:16:49,037 EPOCH - 4 : training on 33 raw words (4 effective words) took 0.0s, 6149 effective words/s
2019-07-17 15:16:49,038 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:16:49,038 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:16:49,038 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:16:49,038 EPOCH - 5 : training on 33 raw words (7 effective words) took 0.0s, 14679 effective words/s
2019-07-17 15:16:49,038 training on a 165 raw words (30 effective words) took 0.0s, 4700 effective words/s
2019-07-17 15:16:49,038 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-17 15:16:50,255 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 15:16:50,255 collecting all words and their counts
2019-07-17 15:16:50,255 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 15:16:50,293 PROGRESS: at example #10000, processed 127599 words (3347609/s), 14559 word types, 10000 tags
2019-07-17 15:16:50,310 collected 17571 word types and 14524 unique tags from a corpus of 14524 examples and 185031 words
2019-07-17 15:16:50,310 Loading a fresh vocabulary
2019-07-17 15:16:50,367 min_count=0 retains 17571 unique words (100% of original 17571, drops 0)
2019-07-17 15:16:50,367 min_count=0 leaves 185031 word corpus (100% of original 185031, drops 0)
2019-07-17 15:16:50,400 deleting the raw counts dictionary of 17571 items
2019-07-17 15:16:50,400 sample=0.001 downsamples 29 most-common words
2019-07-17 15:16:50,400 downsampling leaves estimated 146451 word corpus (79.1% of prior 185031)
2019-07-17 15:16:50,427 estimated required memory for 17571 words and 3 dimensions: 12286292 bytes
2019-07-17 15:16:50,427 resetting layer weights
2019-07-17 15:16:50,642 training model with 3 workers on 17571 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 15:16:50,996 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:16:51,003 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:16:51,010 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:16:51,010 EPOCH - 1 : training on 185031 raw words (160924 effective words) took 0.4s, 444123 effective words/s
2019-07-17 15:16:51,382 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:16:51,387 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:16:51,395 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:16:51,395 EPOCH - 2 : training on 185031 raw words (161080 effective words) took 0.4s, 423156 effective words/s
2019-07-17 15:16:51,745 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:16:51,756 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:16:51,761 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:16:51,761 EPOCH - 3 : training on 185031 raw words (160956 effective words) took 0.4s, 446980 effective words/s
2019-07-17 15:16:52,117 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:16:52,133 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:16:52,133 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:16:52,133 EPOCH - 4 : training on 185031 raw words (160890 effective words) took 0.4s, 437874 effective words/s
2019-07-17 15:16:52,509 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:16:52,514 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:16:52,519 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:16:52,519 EPOCH - 5 : training on 185031 raw words (160889 effective words) took 0.4s, 422140 effective words/s
2019-07-17 15:16:52,519 training on a 925155 raw words (804739 effective words) took 1.9s, 428790 effective words/s
2019-07-17 15:17:33,694 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 15:17:33,694 collecting all words and their counts
2019-07-17 15:17:33,695 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 15:17:33,695 collected 17 word types and 2 unique tags from a corpus of 2 examples and 33 words
2019-07-17 15:17:33,695 Loading a fresh vocabulary
2019-07-17 15:17:33,695 min_count=0 retains 17 unique words (100% of original 17, drops 0)
2019-07-17 15:17:33,695 min_count=0 leaves 33 word corpus (100% of original 33, drops 0)
2019-07-17 15:17:33,695 deleting the raw counts dictionary of 17 items
2019-07-17 15:17:33,695 sample=0.001 downsamples 17 most-common words
2019-07-17 15:17:33,695 downsampling leaves estimated 4 word corpus (14.7% of prior 33)
2019-07-17 15:17:33,695 estimated required memory for 17 words and 3 dimensions: 9332 bytes
2019-07-17 15:17:33,695 resetting layer weights
2019-07-17 15:17:33,695 training model with 3 workers on 17 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 15:17:33,696 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:17:33,697 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:17:33,697 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:17:33,697 EPOCH - 1 : training on 33 raw words (7 effective words) took 0.0s, 14072 effective words/s
2019-07-17 15:17:33,697 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:17:33,698 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:17:33,698 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:17:33,698 EPOCH - 2 : training on 33 raw words (5 effective words) took 0.0s, 9574 effective words/s
2019-07-17 15:17:33,699 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:17:33,699 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:17:33,699 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:17:33,699 EPOCH - 3 : training on 33 raw words (7 effective words) took 0.0s, 11365 effective words/s
2019-07-17 15:17:33,700 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:17:33,700 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:17:33,700 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:17:33,700 EPOCH - 4 : training on 33 raw words (4 effective words) took 0.0s, 8299 effective words/s
2019-07-17 15:17:33,702 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:17:33,702 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:17:33,702 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:17:33,702 EPOCH - 5 : training on 33 raw words (7 effective words) took 0.0s, 13610 effective words/s
2019-07-17 15:17:33,702 training on a 165 raw words (30 effective words) took 0.0s, 4432 effective words/s
2019-07-17 15:17:33,702 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-17 15:17:34,916 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 15:17:34,916 collecting all words and their counts
2019-07-17 15:17:34,916 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 15:17:34,955 PROGRESS: at example #10000, processed 127599 words (3315258/s), 14559 word types, 10000 tags
2019-07-17 15:17:34,972 collected 17571 word types and 14524 unique tags from a corpus of 14524 examples and 185031 words
2019-07-17 15:17:34,972 Loading a fresh vocabulary
2019-07-17 15:17:34,993 min_count=0 retains 17571 unique words (100% of original 17571, drops 0)
2019-07-17 15:17:34,993 min_count=0 leaves 185031 word corpus (100% of original 185031, drops 0)
2019-07-17 15:17:35,024 deleting the raw counts dictionary of 17571 items
2019-07-17 15:17:35,025 sample=0.001 downsamples 29 most-common words
2019-07-17 15:17:35,025 downsampling leaves estimated 146451 word corpus (79.1% of prior 185031)
2019-07-17 15:17:35,053 estimated required memory for 17571 words and 3 dimensions: 12286292 bytes
2019-07-17 15:17:35,053 resetting layer weights
2019-07-17 15:17:35,277 training model with 3 workers on 17571 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 15:17:35,643 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:17:35,650 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:17:35,651 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:17:35,652 EPOCH - 1 : training on 185031 raw words (160971 effective words) took 0.4s, 436362 effective words/s
2019-07-17 15:17:36,006 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:17:36,011 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:17:36,014 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:17:36,014 EPOCH - 2 : training on 185031 raw words (161046 effective words) took 0.4s, 449924 effective words/s
2019-07-17 15:17:36,383 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:17:36,386 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:17:36,389 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:17:36,389 EPOCH - 3 : training on 185031 raw words (160984 effective words) took 0.4s, 435053 effective words/s
2019-07-17 15:17:36,752 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:17:36,754 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:17:36,763 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:17:36,763 EPOCH - 4 : training on 185031 raw words (161005 effective words) took 0.4s, 436536 effective words/s
2019-07-17 15:17:37,121 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:17:37,122 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:17:37,132 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:17:37,132 EPOCH - 5 : training on 185031 raw words (160923 effective words) took 0.4s, 441581 effective words/s
2019-07-17 15:17:37,132 training on a 925155 raw words (804929 effective words) took 1.9s, 433999 effective words/s
2019-07-17 15:18:29,473 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 15:18:29,473 collecting all words and their counts
2019-07-17 15:18:29,473 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 15:18:29,473 collected 17 word types and 2 unique tags from a corpus of 2 examples and 33 words
2019-07-17 15:18:29,473 Loading a fresh vocabulary
2019-07-17 15:18:29,473 min_count=0 retains 17 unique words (100% of original 17, drops 0)
2019-07-17 15:18:29,473 min_count=0 leaves 33 word corpus (100% of original 33, drops 0)
2019-07-17 15:18:29,473 deleting the raw counts dictionary of 17 items
2019-07-17 15:18:29,474 sample=0.001 downsamples 17 most-common words
2019-07-17 15:18:29,474 downsampling leaves estimated 4 word corpus (14.7% of prior 33)
2019-07-17 15:18:29,474 estimated required memory for 17 words and 3 dimensions: 9332 bytes
2019-07-17 15:18:29,474 resetting layer weights
2019-07-17 15:18:29,474 training model with 3 workers on 17 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 15:18:29,475 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:18:29,475 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:18:29,475 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:18:29,475 EPOCH - 1 : training on 33 raw words (7 effective words) took 0.0s, 13546 effective words/s
2019-07-17 15:18:29,476 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:18:29,476 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:18:29,476 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:18:29,476 EPOCH - 2 : training on 33 raw words (5 effective words) took 0.0s, 11745 effective words/s
2019-07-17 15:18:29,477 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:18:29,477 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:18:29,477 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:18:29,477 EPOCH - 3 : training on 33 raw words (7 effective words) took 0.0s, 14420 effective words/s
2019-07-17 15:18:29,480 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:18:29,480 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:18:29,480 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:18:29,480 EPOCH - 4 : training on 33 raw words (4 effective words) took 0.0s, 18347 effective words/s
2019-07-17 15:18:29,481 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:18:29,481 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:18:29,481 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:18:29,481 EPOCH - 5 : training on 33 raw words (7 effective words) took 0.0s, 16046 effective words/s
2019-07-17 15:18:29,481 training on a 165 raw words (30 effective words) took 0.0s, 4300 effective words/s
2019-07-17 15:18:29,481 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-17 15:18:30,682 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 15:18:30,682 collecting all words and their counts
2019-07-17 15:18:30,682 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 15:18:30,722 PROGRESS: at example #10000, processed 127599 words (3228385/s), 14559 word types, 10000 tags
2019-07-17 15:18:30,740 collected 17571 word types and 14524 unique tags from a corpus of 14524 examples and 185031 words
2019-07-17 15:18:30,740 Loading a fresh vocabulary
2019-07-17 15:18:30,797 min_count=0 retains 17571 unique words (100% of original 17571, drops 0)
2019-07-17 15:18:30,797 min_count=0 leaves 185031 word corpus (100% of original 185031, drops 0)
2019-07-17 15:18:30,830 deleting the raw counts dictionary of 17571 items
2019-07-17 15:18:30,830 sample=0.001 downsamples 29 most-common words
2019-07-17 15:18:30,830 downsampling leaves estimated 146451 word corpus (79.1% of prior 185031)
2019-07-17 15:18:30,860 estimated required memory for 17571 words and 3 dimensions: 12286292 bytes
2019-07-17 15:18:30,860 resetting layer weights
2019-07-17 15:18:31,085 training model with 3 workers on 17571 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 15:18:31,454 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:18:31,457 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:18:31,458 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:18:31,458 EPOCH - 1 : training on 185031 raw words (160854 effective words) took 0.4s, 437334 effective words/s
2019-07-17 15:18:31,823 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:18:31,829 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:18:31,832 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:18:31,832 EPOCH - 2 : training on 185031 raw words (161093 effective words) took 0.4s, 435867 effective words/s
2019-07-17 15:18:32,195 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:18:32,198 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:18:32,202 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:18:32,202 EPOCH - 3 : training on 185031 raw words (161064 effective words) took 0.4s, 441977 effective words/s
2019-07-17 15:18:32,561 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:18:32,567 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:18:32,573 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:18:32,573 EPOCH - 4 : training on 185031 raw words (160888 effective words) took 0.4s, 438199 effective words/s
2019-07-17 15:18:32,926 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:18:32,940 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:18:32,941 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:18:32,941 EPOCH - 5 : training on 185031 raw words (161048 effective words) took 0.4s, 443698 effective words/s
2019-07-17 15:18:32,941 training on a 925155 raw words (804947 effective words) took 1.9s, 433612 effective words/s
2019-07-17 15:18:58,057 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 15:18:58,057 collecting all words and their counts
2019-07-17 15:18:58,057 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 15:18:58,057 collected 17 word types and 2 unique tags from a corpus of 2 examples and 33 words
2019-07-17 15:18:58,057 Loading a fresh vocabulary
2019-07-17 15:18:58,057 min_count=0 retains 17 unique words (100% of original 17, drops 0)
2019-07-17 15:18:58,057 min_count=0 leaves 33 word corpus (100% of original 33, drops 0)
2019-07-17 15:18:58,057 deleting the raw counts dictionary of 17 items
2019-07-17 15:18:58,057 sample=0.001 downsamples 17 most-common words
2019-07-17 15:18:58,057 downsampling leaves estimated 4 word corpus (14.7% of prior 33)
2019-07-17 15:18:58,058 estimated required memory for 17 words and 3 dimensions: 9332 bytes
2019-07-17 15:18:58,058 resetting layer weights
2019-07-17 15:18:58,058 training model with 3 workers on 17 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 15:18:58,059 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:18:58,059 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:18:58,059 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:18:58,059 EPOCH - 1 : training on 33 raw words (7 effective words) took 0.0s, 17256 effective words/s
2019-07-17 15:18:58,060 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:18:58,060 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:18:58,060 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:18:58,060 EPOCH - 2 : training on 33 raw words (5 effective words) took 0.0s, 14029 effective words/s
2019-07-17 15:18:58,061 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:18:58,061 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:18:58,061 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:18:58,061 EPOCH - 3 : training on 33 raw words (7 effective words) took 0.0s, 19667 effective words/s
2019-07-17 15:18:58,062 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:18:58,062 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:18:58,062 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:18:58,062 EPOCH - 4 : training on 33 raw words (4 effective words) took 0.0s, 9644 effective words/s
2019-07-17 15:18:58,063 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:18:58,063 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:18:58,063 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:18:58,063 EPOCH - 5 : training on 33 raw words (7 effective words) took 0.0s, 16799 effective words/s
2019-07-17 15:18:58,063 training on a 165 raw words (30 effective words) took 0.0s, 5658 effective words/s
2019-07-17 15:18:58,063 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-17 15:18:59,291 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 15:18:59,292 collecting all words and their counts
2019-07-17 15:18:59,292 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 15:18:59,330 PROGRESS: at example #10000, processed 127599 words (3320300/s), 14559 word types, 10000 tags
2019-07-17 15:18:59,348 collected 17571 word types and 14524 unique tags from a corpus of 14524 examples and 185031 words
2019-07-17 15:18:59,348 Loading a fresh vocabulary
2019-07-17 15:18:59,375 min_count=0 retains 17571 unique words (100% of original 17571, drops 0)
2019-07-17 15:18:59,375 min_count=0 leaves 185031 word corpus (100% of original 185031, drops 0)
2019-07-17 15:18:59,407 deleting the raw counts dictionary of 17571 items
2019-07-17 15:18:59,408 sample=0.001 downsamples 29 most-common words
2019-07-17 15:18:59,408 downsampling leaves estimated 146451 word corpus (79.1% of prior 185031)
2019-07-17 15:18:59,437 estimated required memory for 17571 words and 3 dimensions: 12286292 bytes
2019-07-17 15:18:59,437 resetting layer weights
2019-07-17 15:18:59,666 training model with 3 workers on 17571 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 15:19:00,034 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:19:00,039 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:19:00,045 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:19:00,045 EPOCH - 1 : training on 185031 raw words (161055 effective words) took 0.4s, 430019 effective words/s
2019-07-17 15:19:00,428 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:19:00,431 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:19:00,433 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:19:00,433 EPOCH - 2 : training on 185031 raw words (161109 effective words) took 0.4s, 420333 effective words/s
2019-07-17 15:19:00,791 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:19:00,793 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:19:00,796 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:19:00,796 EPOCH - 3 : training on 185031 raw words (161052 effective words) took 0.4s, 450087 effective words/s
2019-07-17 15:19:01,150 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:19:01,159 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:19:01,162 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:19:01,162 EPOCH - 4 : training on 185031 raw words (161078 effective words) took 0.4s, 445987 effective words/s
2019-07-17 15:19:01,524 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:19:01,525 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:19:01,535 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:19:01,535 EPOCH - 5 : training on 185031 raw words (161027 effective words) took 0.4s, 438022 effective words/s
2019-07-17 15:19:01,535 training on a 925155 raw words (805321 effective words) took 1.9s, 430923 effective words/s
2019-07-17 15:19:23,023 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 15:19:23,024 collecting all words and their counts
2019-07-17 15:19:23,024 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 15:19:23,024 collected 17 word types and 2 unique tags from a corpus of 2 examples and 33 words
2019-07-17 15:19:23,024 Loading a fresh vocabulary
2019-07-17 15:19:23,024 min_count=0 retains 17 unique words (100% of original 17, drops 0)
2019-07-17 15:19:23,024 min_count=0 leaves 33 word corpus (100% of original 33, drops 0)
2019-07-17 15:19:23,024 deleting the raw counts dictionary of 17 items
2019-07-17 15:19:23,024 sample=0.001 downsamples 17 most-common words
2019-07-17 15:19:23,024 downsampling leaves estimated 4 word corpus (14.7% of prior 33)
2019-07-17 15:19:23,024 estimated required memory for 17 words and 3 dimensions: 9332 bytes
2019-07-17 15:19:23,024 resetting layer weights
2019-07-17 15:19:23,024 training model with 3 workers on 17 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 15:19:23,026 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:19:23,026 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:19:23,026 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:19:23,026 EPOCH - 1 : training on 33 raw words (7 effective words) took 0.0s, 9313 effective words/s
2019-07-17 15:19:23,027 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:19:23,027 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:19:23,027 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:19:23,027 EPOCH - 2 : training on 33 raw words (5 effective words) took 0.0s, 9881 effective words/s
2019-07-17 15:19:23,028 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:19:23,028 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:19:23,028 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:19:23,028 EPOCH - 3 : training on 33 raw words (7 effective words) took 0.0s, 12809 effective words/s
2019-07-17 15:19:23,029 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:19:23,029 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:19:23,030 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:19:23,030 EPOCH - 4 : training on 33 raw words (4 effective words) took 0.0s, 8688 effective words/s
2019-07-17 15:19:23,031 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:19:23,031 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:19:23,031 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:19:23,031 EPOCH - 5 : training on 33 raw words (7 effective words) took 0.0s, 9864 effective words/s
2019-07-17 15:19:23,031 training on a 165 raw words (30 effective words) took 0.0s, 4639 effective words/s
2019-07-17 15:19:23,031 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-17 15:19:24,232 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 15:19:24,233 collecting all words and their counts
2019-07-17 15:19:24,233 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 15:19:24,272 PROGRESS: at example #10000, processed 127599 words (3269644/s), 14559 word types, 10000 tags
2019-07-17 15:19:24,289 collected 17571 word types and 14524 unique tags from a corpus of 14524 examples and 185031 words
2019-07-17 15:19:24,289 Loading a fresh vocabulary
2019-07-17 15:19:24,344 min_count=0 retains 17571 unique words (100% of original 17571, drops 0)
2019-07-17 15:19:24,344 min_count=0 leaves 185031 word corpus (100% of original 185031, drops 0)
2019-07-17 15:19:24,382 deleting the raw counts dictionary of 17571 items
2019-07-17 15:19:24,382 sample=0.001 downsamples 29 most-common words
2019-07-17 15:19:24,382 downsampling leaves estimated 146451 word corpus (79.1% of prior 185031)
2019-07-17 15:19:24,410 estimated required memory for 17571 words and 3 dimensions: 12286292 bytes
2019-07-17 15:19:24,410 resetting layer weights
2019-07-17 15:19:24,646 training model with 3 workers on 17571 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 15:19:25,010 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:19:25,012 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:19:25,021 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:19:25,021 EPOCH - 1 : training on 185031 raw words (160921 effective words) took 0.4s, 435777 effective words/s
2019-07-17 15:19:25,395 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:19:25,400 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:19:25,406 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:19:25,406 EPOCH - 2 : training on 185031 raw words (161093 effective words) took 0.4s, 423018 effective words/s
2019-07-17 15:19:25,754 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:19:25,763 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:19:25,767 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:19:25,767 EPOCH - 3 : training on 185031 raw words (160992 effective words) took 0.4s, 451748 effective words/s
2019-07-17 15:19:26,134 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:19:26,136 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:19:26,138 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:19:26,138 EPOCH - 4 : training on 185031 raw words (160858 effective words) took 0.4s, 440184 effective words/s
2019-07-17 15:19:26,515 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:19:26,519 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:19:26,526 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:19:26,526 EPOCH - 5 : training on 185031 raw words (160973 effective words) took 0.4s, 419437 effective words/s
2019-07-17 15:19:26,526 training on a 925155 raw words (804837 effective words) took 1.9s, 428138 effective words/s
2019-07-17 15:20:31,622 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 15:20:31,622 collecting all words and their counts
2019-07-17 15:20:31,622 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 15:20:31,622 collected 17 word types and 2 unique tags from a corpus of 2 examples and 33 words
2019-07-17 15:20:31,622 Loading a fresh vocabulary
2019-07-17 15:20:31,622 min_count=0 retains 17 unique words (100% of original 17, drops 0)
2019-07-17 15:20:31,622 min_count=0 leaves 33 word corpus (100% of original 33, drops 0)
2019-07-17 15:20:31,622 deleting the raw counts dictionary of 17 items
2019-07-17 15:20:31,622 sample=0.001 downsamples 17 most-common words
2019-07-17 15:20:31,623 downsampling leaves estimated 4 word corpus (14.7% of prior 33)
2019-07-17 15:20:31,623 estimated required memory for 17 words and 3 dimensions: 9332 bytes
2019-07-17 15:20:31,623 resetting layer weights
2019-07-17 15:20:31,623 training model with 3 workers on 17 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 15:20:31,624 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:20:31,624 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:20:31,624 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:20:31,624 EPOCH - 1 : training on 33 raw words (7 effective words) took 0.0s, 13615 effective words/s
2019-07-17 15:20:31,625 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:20:31,626 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:20:31,626 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:20:31,626 EPOCH - 2 : training on 33 raw words (5 effective words) took 0.0s, 8621 effective words/s
2019-07-17 15:20:31,626 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:20:31,627 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:20:31,627 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:20:31,627 EPOCH - 3 : training on 33 raw words (7 effective words) took 0.0s, 14161 effective words/s
2019-07-17 15:20:31,627 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:20:31,628 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:20:31,628 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:20:31,628 EPOCH - 4 : training on 33 raw words (4 effective words) took 0.0s, 7519 effective words/s
2019-07-17 15:20:31,629 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:20:31,629 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:20:31,629 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:20:31,629 EPOCH - 5 : training on 33 raw words (7 effective words) took 0.0s, 13239 effective words/s
2019-07-17 15:20:31,629 training on a 165 raw words (30 effective words) took 0.0s, 5001 effective words/s
2019-07-17 15:20:31,629 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-17 15:20:32,834 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 15:20:32,834 collecting all words and their counts
2019-07-17 15:20:32,834 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 15:20:32,873 PROGRESS: at example #10000, processed 127599 words (3308306/s), 14559 word types, 10000 tags
2019-07-17 15:20:32,890 collected 17571 word types and 14524 unique tags from a corpus of 14524 examples and 185031 words
2019-07-17 15:20:32,890 Loading a fresh vocabulary
2019-07-17 15:20:32,911 min_count=0 retains 17571 unique words (100% of original 17571, drops 0)
2019-07-17 15:20:32,911 min_count=0 leaves 185031 word corpus (100% of original 185031, drops 0)
2019-07-17 15:20:32,943 deleting the raw counts dictionary of 17571 items
2019-07-17 15:20:32,943 sample=0.001 downsamples 29 most-common words
2019-07-17 15:20:32,943 downsampling leaves estimated 146451 word corpus (79.1% of prior 185031)
2019-07-17 15:20:32,972 estimated required memory for 17571 words and 3 dimensions: 12286292 bytes
2019-07-17 15:20:32,972 resetting layer weights
2019-07-17 15:20:33,201 training model with 3 workers on 17571 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 15:20:33,573 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:20:33,582 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:20:33,583 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:20:33,583 EPOCH - 1 : training on 185031 raw words (160982 effective words) took 0.4s, 426965 effective words/s
2019-07-17 15:20:33,943 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:20:33,953 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:20:33,958 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:20:33,958 EPOCH - 2 : training on 185031 raw words (160991 effective words) took 0.4s, 435118 effective words/s
2019-07-17 15:20:34,325 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:20:34,330 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:20:34,331 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:20:34,331 EPOCH - 3 : training on 185031 raw words (160872 effective words) took 0.4s, 436453 effective words/s
2019-07-17 15:20:34,697 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:20:34,705 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:20:34,708 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:20:34,708 EPOCH - 4 : training on 185031 raw words (160852 effective words) took 0.4s, 431898 effective words/s
2019-07-17 15:20:35,071 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:20:35,076 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:20:35,080 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:20:35,080 EPOCH - 5 : training on 185031 raw words (161001 effective words) took 0.4s, 439200 effective words/s
2019-07-17 15:20:35,080 training on a 925155 raw words (804698 effective words) took 1.9s, 428371 effective words/s
2019-07-17 15:22:18,704 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 15:22:18,704 collecting all words and their counts
2019-07-17 15:22:18,704 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 15:22:18,704 collected 17 word types and 2 unique tags from a corpus of 2 examples and 33 words
2019-07-17 15:22:18,704 Loading a fresh vocabulary
2019-07-17 15:22:18,704 min_count=0 retains 17 unique words (100% of original 17, drops 0)
2019-07-17 15:22:18,704 min_count=0 leaves 33 word corpus (100% of original 33, drops 0)
2019-07-17 15:22:18,705 deleting the raw counts dictionary of 17 items
2019-07-17 15:22:18,705 sample=0.001 downsamples 17 most-common words
2019-07-17 15:22:18,705 downsampling leaves estimated 4 word corpus (14.7% of prior 33)
2019-07-17 15:22:18,705 estimated required memory for 17 words and 3 dimensions: 9332 bytes
2019-07-17 15:22:18,705 resetting layer weights
2019-07-17 15:22:18,705 training model with 3 workers on 17 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 15:22:18,706 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:22:18,706 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:22:18,706 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:22:18,706 EPOCH - 1 : training on 33 raw words (7 effective words) took 0.0s, 13355 effective words/s
2019-07-17 15:22:18,707 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:22:18,707 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:22:18,708 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:22:18,708 EPOCH - 2 : training on 33 raw words (5 effective words) took 0.0s, 9629 effective words/s
2019-07-17 15:22:18,709 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:22:18,709 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:22:18,709 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:22:18,709 EPOCH - 3 : training on 33 raw words (7 effective words) took 0.0s, 15131 effective words/s
2019-07-17 15:22:18,710 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:22:18,710 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:22:18,710 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:22:18,710 EPOCH - 4 : training on 33 raw words (4 effective words) took 0.0s, 9327 effective words/s
2019-07-17 15:22:18,710 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:22:18,711 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:22:18,711 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:22:18,711 EPOCH - 5 : training on 33 raw words (7 effective words) took 0.0s, 16553 effective words/s
2019-07-17 15:22:18,711 training on a 165 raw words (30 effective words) took 0.0s, 5449 effective words/s
2019-07-17 15:22:18,711 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-17 15:22:19,874 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 15:22:19,874 collecting all words and their counts
2019-07-17 15:22:19,874 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 15:22:19,911 PROGRESS: at example #10000, processed 127599 words (3506291/s), 14559 word types, 10000 tags
2019-07-17 15:22:19,928 collected 17571 word types and 14524 unique tags from a corpus of 14524 examples and 185031 words
2019-07-17 15:22:19,928 Loading a fresh vocabulary
2019-07-17 15:22:19,983 min_count=0 retains 17571 unique words (100% of original 17571, drops 0)
2019-07-17 15:22:19,983 min_count=0 leaves 185031 word corpus (100% of original 185031, drops 0)
2019-07-17 15:22:20,014 deleting the raw counts dictionary of 17571 items
2019-07-17 15:22:20,014 sample=0.001 downsamples 29 most-common words
2019-07-17 15:22:20,014 downsampling leaves estimated 146451 word corpus (79.1% of prior 185031)
2019-07-17 15:22:20,042 estimated required memory for 17571 words and 3 dimensions: 12286292 bytes
2019-07-17 15:22:20,042 resetting layer weights
2019-07-17 15:22:20,261 training model with 3 workers on 17571 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 15:22:20,633 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:22:20,635 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:22:20,637 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:22:20,637 EPOCH - 1 : training on 185031 raw words (160873 effective words) took 0.4s, 433941 effective words/s
2019-07-17 15:22:20,993 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:22:20,996 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:22:20,998 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:22:20,998 EPOCH - 2 : training on 185031 raw words (161011 effective words) took 0.4s, 452635 effective words/s
2019-07-17 15:22:21,367 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:22:21,373 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:22:21,377 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:22:21,378 EPOCH - 3 : training on 185031 raw words (161062 effective words) took 0.4s, 429488 effective words/s
2019-07-17 15:22:21,732 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:22:21,742 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:22:21,747 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:22:21,747 EPOCH - 4 : training on 185031 raw words (160973 effective words) took 0.4s, 440912 effective words/s
2019-07-17 15:22:22,100 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:22:22,104 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:22:22,111 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:22:22,111 EPOCH - 5 : training on 185031 raw words (160972 effective words) took 0.4s, 448732 effective words/s
2019-07-17 15:22:22,111 training on a 925155 raw words (804891 effective words) took 1.8s, 435124 effective words/s
2019-07-17 15:22:34,008 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 15:22:34,008 collecting all words and their counts
2019-07-17 15:22:34,008 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 15:22:34,008 collected 17 word types and 2 unique tags from a corpus of 2 examples and 33 words
2019-07-17 15:22:34,008 Loading a fresh vocabulary
2019-07-17 15:22:34,008 min_count=0 retains 17 unique words (100% of original 17, drops 0)
2019-07-17 15:22:34,008 min_count=0 leaves 33 word corpus (100% of original 33, drops 0)
2019-07-17 15:22:34,008 deleting the raw counts dictionary of 17 items
2019-07-17 15:22:34,009 sample=0.001 downsamples 17 most-common words
2019-07-17 15:22:34,009 downsampling leaves estimated 4 word corpus (14.7% of prior 33)
2019-07-17 15:22:34,009 estimated required memory for 17 words and 3 dimensions: 9332 bytes
2019-07-17 15:22:34,009 resetting layer weights
2019-07-17 15:22:34,009 training model with 3 workers on 17 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 15:22:34,010 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:22:34,010 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:22:34,010 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:22:34,011 EPOCH - 1 : training on 33 raw words (7 effective words) took 0.0s, 11100 effective words/s
2019-07-17 15:22:34,012 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:22:34,012 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:22:34,012 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:22:34,012 EPOCH - 2 : training on 33 raw words (5 effective words) took 0.0s, 8053 effective words/s
2019-07-17 15:22:34,013 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:22:34,013 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:22:34,013 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:22:34,013 EPOCH - 3 : training on 33 raw words (7 effective words) took 0.0s, 12381 effective words/s
2019-07-17 15:22:34,014 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:22:34,014 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:22:34,014 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:22:34,014 EPOCH - 4 : training on 33 raw words (4 effective words) took 0.0s, 8775 effective words/s
2019-07-17 15:22:34,015 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:22:34,015 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:22:34,015 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:22:34,015 EPOCH - 5 : training on 33 raw words (7 effective words) took 0.0s, 18868 effective words/s
2019-07-17 15:22:34,015 training on a 165 raw words (30 effective words) took 0.0s, 4849 effective words/s
2019-07-17 15:22:34,015 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-17 15:22:35,154 consider setting layer size to a multiple of 4 for greater performance
2019-07-17 15:22:35,154 collecting all words and their counts
2019-07-17 15:22:35,154 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-17 15:22:35,228 PROGRESS: at example #10000, processed 127599 words (1728730/s), 14559 word types, 10000 tags
2019-07-17 15:22:35,245 collected 17571 word types and 14524 unique tags from a corpus of 14524 examples and 185031 words
2019-07-17 15:22:35,245 Loading a fresh vocabulary
2019-07-17 15:22:35,266 min_count=0 retains 17571 unique words (100% of original 17571, drops 0)
2019-07-17 15:22:35,266 min_count=0 leaves 185031 word corpus (100% of original 185031, drops 0)
2019-07-17 15:22:35,297 deleting the raw counts dictionary of 17571 items
2019-07-17 15:22:35,298 sample=0.001 downsamples 29 most-common words
2019-07-17 15:22:35,298 downsampling leaves estimated 146451 word corpus (79.1% of prior 185031)
2019-07-17 15:22:35,326 estimated required memory for 17571 words and 3 dimensions: 12286292 bytes
2019-07-17 15:22:35,326 resetting layer weights
2019-07-17 15:22:35,563 training model with 3 workers on 17571 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-17 15:22:35,943 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:22:35,946 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:22:35,955 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:22:35,955 EPOCH - 1 : training on 185031 raw words (161059 effective words) took 0.4s, 416225 effective words/s
2019-07-17 15:22:36,341 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:22:36,344 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:22:36,350 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:22:36,350 EPOCH - 2 : training on 185031 raw words (160954 effective words) took 0.4s, 412769 effective words/s
2019-07-17 15:22:36,721 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:22:36,725 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:22:36,733 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:22:36,733 EPOCH - 3 : training on 185031 raw words (161127 effective words) took 0.4s, 427188 effective words/s
2019-07-17 15:22:37,102 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:22:37,106 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:22:37,114 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:22:37,114 EPOCH - 4 : training on 185031 raw words (160960 effective words) took 0.4s, 427448 effective words/s
2019-07-17 15:22:37,489 worker thread finished; awaiting finish of 2 more threads
2019-07-17 15:22:37,495 worker thread finished; awaiting finish of 1 more threads
2019-07-17 15:22:37,501 worker thread finished; awaiting finish of 0 more threads
2019-07-17 15:22:37,501 EPOCH - 5 : training on 185031 raw words (160868 effective words) took 0.4s, 421782 effective words/s
2019-07-17 15:22:37,501 training on a 925155 raw words (804968 effective words) took 1.9s, 415401 effective words/s
2019-07-17 17:47:22,166 'pattern' package not found; tag filters are not available for English
2019-07-17 17:51:51,798 'pattern' package not found; tag filters are not available for English
2019-07-17 18:03:08,899 'pattern' package not found; tag filters are not available for English
2019-07-24 10:06:08,709 'pattern' package not found; tag filters are not available for English
2019-07-24 10:27:37,176 consider setting layer size to a multiple of 4 for greater performance
2019-07-24 10:27:37,176 collecting all words and their counts
2019-07-24 10:27:37,176 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-24 10:27:37,210 PROGRESS: at example #10000, processed 91176 words (2670588/s), 10202 word types, 10000 tags
2019-07-24 10:27:37,225 collected 12243 word types and 14524 unique tags from a corpus of 14524 examples and 132373 words
2019-07-24 10:27:37,225 Loading a fresh vocabulary
2019-07-24 10:27:37,239 min_count=0 retains 12243 unique words (100% of original 12243, drops 0)
2019-07-24 10:27:37,239 min_count=0 leaves 132373 word corpus (100% of original 132373, drops 0)
2019-07-24 10:27:37,265 deleting the raw counts dictionary of 12243 items
2019-07-24 10:27:37,265 sample=0.001 downsamples 39 most-common words
2019-07-24 10:27:37,265 downsampling leaves estimated 120083 word corpus (90.7% of prior 132373)
2019-07-24 10:27:37,295 estimated required memory for 12243 words and 5 dimensions: 9806500 bytes
2019-07-24 10:27:37,295 resetting layer weights
2019-07-24 10:27:37,501 training model with 3 workers on 12243 vocabulary and 5 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-24 10:27:37,860 worker thread finished; awaiting finish of 2 more threads
2019-07-24 10:27:37,869 worker thread finished; awaiting finish of 1 more threads
2019-07-24 10:27:37,880 worker thread finished; awaiting finish of 0 more threads
2019-07-24 10:27:37,880 EPOCH - 1 : training on 132373 raw words (134646 effective words) took 0.4s, 360950 effective words/s
2019-07-24 10:27:38,253 worker thread finished; awaiting finish of 2 more threads
2019-07-24 10:27:38,255 worker thread finished; awaiting finish of 1 more threads
2019-07-24 10:27:38,272 worker thread finished; awaiting finish of 0 more threads
2019-07-24 10:27:38,272 EPOCH - 2 : training on 132373 raw words (134548 effective words) took 0.4s, 348306 effective words/s
2019-07-24 10:27:38,606 worker thread finished; awaiting finish of 2 more threads
2019-07-24 10:27:38,620 worker thread finished; awaiting finish of 1 more threads
2019-07-24 10:27:38,633 worker thread finished; awaiting finish of 0 more threads
2019-07-24 10:27:38,633 EPOCH - 3 : training on 132373 raw words (134613 effective words) took 0.4s, 378713 effective words/s
2019-07-24 10:27:39,123 worker thread finished; awaiting finish of 2 more threads
2019-07-24 10:27:39,137 worker thread finished; awaiting finish of 1 more threads
2019-07-24 10:27:39,143 worker thread finished; awaiting finish of 0 more threads
2019-07-24 10:27:39,143 EPOCH - 4 : training on 132373 raw words (134597 effective words) took 0.5s, 274772 effective words/s
2019-07-24 10:27:39,483 worker thread finished; awaiting finish of 2 more threads
2019-07-24 10:27:39,507 worker thread finished; awaiting finish of 1 more threads
2019-07-24 10:27:39,513 worker thread finished; awaiting finish of 0 more threads
2019-07-24 10:27:39,513 EPOCH - 5 : training on 132373 raw words (134644 effective words) took 0.4s, 370094 effective words/s
2019-07-24 10:27:39,513 training on a 661865 raw words (673048 effective words) took 2.0s, 334538 effective words/s
2019-07-24 10:42:56,725 'pattern' package not found; tag filters are not available for English
2019-07-24 10:48:31,504 'pattern' package not found; tag filters are not available for English
2019-07-24 10:50:55,403 'pattern' package not found; tag filters are not available for English
2019-07-24 10:55:37,913 'pattern' package not found; tag filters are not available for English
2019-07-24 10:55:43,357 consider setting layer size to a multiple of 4 for greater performance
2019-07-24 10:55:43,357 collecting all words and their counts
2019-07-24 10:55:43,357 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-24 10:55:43,391 PROGRESS: at example #10000, processed 91176 words (2673042/s), 10202 word types, 10000 tags
2019-07-24 10:55:43,407 collected 12243 word types and 14524 unique tags from a corpus of 14524 examples and 132373 words
2019-07-24 10:55:43,407 Loading a fresh vocabulary
2019-07-24 10:55:43,424 min_count=0 retains 12243 unique words (100% of original 12243, drops 0)
2019-07-24 10:55:43,424 min_count=0 leaves 132373 word corpus (100% of original 132373, drops 0)
2019-07-24 10:55:43,447 deleting the raw counts dictionary of 12243 items
2019-07-24 10:55:43,447 sample=0.001 downsamples 39 most-common words
2019-07-24 10:55:43,448 downsampling leaves estimated 120083 word corpus (90.7% of prior 132373)
2019-07-24 10:55:43,471 estimated required memory for 12243 words and 5 dimensions: 9806500 bytes
2019-07-24 10:55:43,471 resetting layer weights
2019-07-24 10:55:43,681 training model with 3 workers on 12243 vocabulary and 5 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-24 10:55:44,057 worker thread finished; awaiting finish of 2 more threads
2019-07-24 10:55:44,064 worker thread finished; awaiting finish of 1 more threads
2019-07-24 10:55:44,081 worker thread finished; awaiting finish of 0 more threads
2019-07-24 10:55:44,081 EPOCH - 1 : training on 132373 raw words (134642 effective words) took 0.4s, 341979 effective words/s
2019-07-24 10:55:44,406 worker thread finished; awaiting finish of 2 more threads
2019-07-24 10:55:44,417 worker thread finished; awaiting finish of 1 more threads
2019-07-24 10:55:44,435 worker thread finished; awaiting finish of 0 more threads
2019-07-24 10:55:44,435 EPOCH - 2 : training on 132373 raw words (134534 effective words) took 0.3s, 385903 effective words/s
2019-07-24 10:55:44,776 worker thread finished; awaiting finish of 2 more threads
2019-07-24 10:55:44,781 worker thread finished; awaiting finish of 1 more threads
2019-07-24 10:55:44,799 worker thread finished; awaiting finish of 0 more threads
2019-07-24 10:55:44,799 EPOCH - 3 : training on 132373 raw words (134604 effective words) took 0.4s, 375943 effective words/s
2019-07-24 10:55:45,140 worker thread finished; awaiting finish of 2 more threads
2019-07-24 10:55:45,150 worker thread finished; awaiting finish of 1 more threads
2019-07-24 10:55:45,163 worker thread finished; awaiting finish of 0 more threads
2019-07-24 10:55:45,163 EPOCH - 4 : training on 132373 raw words (134619 effective words) took 0.4s, 376300 effective words/s
2019-07-24 10:55:45,502 worker thread finished; awaiting finish of 2 more threads
2019-07-24 10:55:45,515 worker thread finished; awaiting finish of 1 more threads
2019-07-24 10:55:45,533 worker thread finished; awaiting finish of 0 more threads
2019-07-24 10:55:45,533 EPOCH - 5 : training on 132373 raw words (134684 effective words) took 0.4s, 369308 effective words/s
2019-07-24 10:55:45,533 training on a 661865 raw words (673083 effective words) took 1.9s, 363339 effective words/s
2019-07-24 10:56:02,137 consider setting layer size to a multiple of 4 for greater performance
2019-07-24 10:56:02,137 collecting all words and their counts
2019-07-24 10:56:02,138 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-24 10:56:02,168 PROGRESS: at example #10000, processed 91176 words (3009233/s), 10202 word types, 10000 tags
2019-07-24 10:56:02,182 collected 12243 word types and 14524 unique tags from a corpus of 14524 examples and 132373 words
2019-07-24 10:56:02,182 Loading a fresh vocabulary
2019-07-24 10:56:02,196 min_count=0 retains 12243 unique words (100% of original 12243, drops 0)
2019-07-24 10:56:02,197 min_count=0 leaves 132373 word corpus (100% of original 132373, drops 0)
2019-07-24 10:56:02,217 deleting the raw counts dictionary of 12243 items
2019-07-24 10:56:02,217 sample=0.001 downsamples 39 most-common words
2019-07-24 10:56:02,217 downsampling leaves estimated 120083 word corpus (90.7% of prior 132373)
2019-07-24 10:56:02,235 estimated required memory for 12243 words and 5 dimensions: 9806500 bytes
2019-07-24 10:56:02,235 resetting layer weights
2019-07-24 10:56:02,411 training model with 3 workers on 12243 vocabulary and 5 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-24 10:56:02,727 worker thread finished; awaiting finish of 2 more threads
2019-07-24 10:56:02,729 worker thread finished; awaiting finish of 1 more threads
2019-07-24 10:56:02,748 worker thread finished; awaiting finish of 0 more threads
2019-07-24 10:56:02,748 EPOCH - 1 : training on 132373 raw words (134619 effective words) took 0.3s, 406848 effective words/s
2019-07-24 10:56:03,063 worker thread finished; awaiting finish of 2 more threads
2019-07-24 10:56:03,070 worker thread finished; awaiting finish of 1 more threads
2019-07-24 10:56:03,086 worker thread finished; awaiting finish of 0 more threads
2019-07-24 10:56:03,086 EPOCH - 2 : training on 132373 raw words (134568 effective words) took 0.3s, 404222 effective words/s
2019-07-24 10:56:03,404 worker thread finished; awaiting finish of 2 more threads
2019-07-24 10:56:03,415 worker thread finished; awaiting finish of 1 more threads
2019-07-24 10:56:03,421 worker thread finished; awaiting finish of 0 more threads
2019-07-24 10:56:03,422 EPOCH - 3 : training on 132373 raw words (134798 effective words) took 0.3s, 409259 effective words/s
2019-07-24 10:56:03,736 worker thread finished; awaiting finish of 2 more threads
2019-07-24 10:56:03,739 worker thread finished; awaiting finish of 1 more threads
2019-07-24 10:56:03,756 worker thread finished; awaiting finish of 0 more threads
2019-07-24 10:56:03,756 EPOCH - 4 : training on 132373 raw words (134629 effective words) took 0.3s, 409661 effective words/s
2019-07-24 10:56:04,083 worker thread finished; awaiting finish of 2 more threads
2019-07-24 10:56:04,090 worker thread finished; awaiting finish of 1 more threads
2019-07-24 10:56:04,104 worker thread finished; awaiting finish of 0 more threads
2019-07-24 10:56:04,104 EPOCH - 5 : training on 132373 raw words (134624 effective words) took 0.3s, 393433 effective words/s
2019-07-24 10:56:04,104 training on a 661865 raw words (673238 effective words) took 1.7s, 397681 effective words/s
2019-07-24 10:59:49,357 'pattern' package not found; tag filters are not available for English
2019-07-24 11:04:54,074 'pattern' package not found; tag filters are not available for English
2019-07-24 11:13:41,833 'pattern' package not found; tag filters are not available for English
2019-07-24 11:50:19,397 'pattern' package not found; tag filters are not available for English
2019-07-24 11:59:24,566 'pattern' package not found; tag filters are not available for English
2019-07-24 12:01:29,783 'pattern' package not found; tag filters are not available for English
2019-07-24 12:02:18,486 'pattern' package not found; tag filters are not available for English
2019-07-24 12:03:20,511 'pattern' package not found; tag filters are not available for English
2019-07-24 12:05:21,947 'pattern' package not found; tag filters are not available for English
2019-07-24 12:07:10,467 'pattern' package not found; tag filters are not available for English
2019-07-24 12:08:02,248 'pattern' package not found; tag filters are not available for English
2019-07-24 12:14:35,353 'pattern' package not found; tag filters are not available for English
2019-07-24 12:20:57,539 'pattern' package not found; tag filters are not available for English
2019-07-24 12:22:26,152 'pattern' package not found; tag filters are not available for English
2019-07-24 12:24:06,028 'pattern' package not found; tag filters are not available for English
2019-07-24 12:25:35,576 'pattern' package not found; tag filters are not available for English
2019-07-24 12:26:39,972 'pattern' package not found; tag filters are not available for English
2019-07-24 12:27:34,103 'pattern' package not found; tag filters are not available for English
2019-07-24 12:30:14,403 consider setting layer size to a multiple of 4 for greater performance
2019-07-24 12:30:14,403 collecting all words and their counts
2019-07-24 12:30:14,403 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-24 12:30:14,408 collected 3683 word types and 1453 unique tags from a corpus of 1453 examples and 13192 words
2019-07-24 12:30:14,408 Loading a fresh vocabulary
2019-07-24 12:30:14,413 min_count=0 retains 3683 unique words (100% of original 3683, drops 0)
2019-07-24 12:30:14,413 min_count=0 leaves 13192 word corpus (100% of original 13192, drops 0)
2019-07-24 12:30:14,420 deleting the raw counts dictionary of 3683 items
2019-07-24 12:30:14,420 sample=0.001 downsamples 40 most-common words
2019-07-24 12:30:14,420 downsampling leaves estimated 11987 word corpus (90.9% of prior 13192)
2019-07-24 12:30:14,425 estimated required memory for 3683 words and 5 dimensions: 2308480 bytes
2019-07-24 12:30:14,425 resetting layer weights
2019-07-24 12:30:14,462 training model with 3 workers on 3683 vocabulary and 5 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-24 12:30:14,465 worker thread finished; awaiting finish of 2 more threads
2019-07-24 12:30:14,480 worker thread finished; awaiting finish of 1 more threads
2019-07-24 12:30:14,497 worker thread finished; awaiting finish of 0 more threads
2019-07-24 12:30:14,497 EPOCH - 1 : training on 13192 raw words (13463 effective words) took 0.0s, 417051 effective words/s
2019-07-24 12:30:14,499 worker thread finished; awaiting finish of 2 more threads
2019-07-24 12:30:14,514 worker thread finished; awaiting finish of 1 more threads
2019-07-24 12:30:14,531 worker thread finished; awaiting finish of 0 more threads
2019-07-24 12:30:14,531 EPOCH - 2 : training on 13192 raw words (13498 effective words) took 0.0s, 414596 effective words/s
2019-07-24 12:30:14,534 worker thread finished; awaiting finish of 2 more threads
2019-07-24 12:30:14,549 worker thread finished; awaiting finish of 1 more threads
2019-07-24 12:30:14,565 worker thread finished; awaiting finish of 0 more threads
2019-07-24 12:30:14,565 EPOCH - 3 : training on 13192 raw words (13429 effective words) took 0.0s, 422710 effective words/s
2019-07-24 12:30:14,568 worker thread finished; awaiting finish of 2 more threads
2019-07-24 12:30:14,582 worker thread finished; awaiting finish of 1 more threads
2019-07-24 12:30:14,600 worker thread finished; awaiting finish of 0 more threads
2019-07-24 12:30:14,600 EPOCH - 4 : training on 13192 raw words (13485 effective words) took 0.0s, 406793 effective words/s
2019-07-24 12:30:14,604 worker thread finished; awaiting finish of 2 more threads
2019-07-24 12:30:14,619 worker thread finished; awaiting finish of 1 more threads
2019-07-24 12:30:14,635 worker thread finished; awaiting finish of 0 more threads
2019-07-24 12:30:14,635 EPOCH - 5 : training on 13192 raw words (13392 effective words) took 0.0s, 423855 effective words/s
2019-07-24 12:30:14,635 training on a 65960 raw words (67267 effective words) took 0.2s, 389602 effective words/s
2019-07-24 12:30:14,635 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-24 12:45:06,786 'pattern' package not found; tag filters are not available for English
2019-07-24 12:47:01,887 'pattern' package not found; tag filters are not available for English
2019-07-24 12:49:08,886 'pattern' package not found; tag filters are not available for English
2019-07-24 12:51:59,661 'pattern' package not found; tag filters are not available for English
2019-07-24 13:03:50,714 consider setting layer size to a multiple of 4 for greater performance
2019-07-24 13:03:50,714 collecting all words and their counts
2019-07-24 13:03:50,715 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-24 13:03:50,719 collected 3683 word types and 1453 unique tags from a corpus of 1453 examples and 13192 words
2019-07-24 13:03:50,719 Loading a fresh vocabulary
2019-07-24 13:03:50,724 min_count=0 retains 3683 unique words (100% of original 3683, drops 0)
2019-07-24 13:03:50,724 min_count=0 leaves 13192 word corpus (100% of original 13192, drops 0)
2019-07-24 13:03:50,732 deleting the raw counts dictionary of 3683 items
2019-07-24 13:03:50,732 sample=0.001 downsamples 40 most-common words
2019-07-24 13:03:50,732 downsampling leaves estimated 11987 word corpus (90.9% of prior 13192)
2019-07-24 13:03:50,736 estimated required memory for 3683 words and 5 dimensions: 2308480 bytes
2019-07-24 13:03:50,736 resetting layer weights
2019-07-24 13:03:50,776 training model with 3 workers on 3683 vocabulary and 5 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-24 13:03:50,779 worker thread finished; awaiting finish of 2 more threads
2019-07-24 13:03:50,794 worker thread finished; awaiting finish of 1 more threads
2019-07-24 13:03:50,809 worker thread finished; awaiting finish of 0 more threads
2019-07-24 13:03:50,809 EPOCH - 1 : training on 13192 raw words (13464 effective words) took 0.0s, 439035 effective words/s
2019-07-24 13:03:50,812 worker thread finished; awaiting finish of 2 more threads
2019-07-24 13:03:50,826 worker thread finished; awaiting finish of 1 more threads
2019-07-24 13:03:50,842 worker thread finished; awaiting finish of 0 more threads
2019-07-24 13:03:50,842 EPOCH - 2 : training on 13192 raw words (13466 effective words) took 0.0s, 422444 effective words/s
2019-07-24 13:03:50,845 worker thread finished; awaiting finish of 2 more threads
2019-07-24 13:03:50,859 worker thread finished; awaiting finish of 1 more threads
2019-07-24 13:03:50,876 worker thread finished; awaiting finish of 0 more threads
2019-07-24 13:03:50,876 EPOCH - 3 : training on 13192 raw words (13410 effective words) took 0.0s, 425731 effective words/s
2019-07-24 13:03:50,879 worker thread finished; awaiting finish of 2 more threads
2019-07-24 13:03:50,893 worker thread finished; awaiting finish of 1 more threads
2019-07-24 13:03:50,911 worker thread finished; awaiting finish of 0 more threads
2019-07-24 13:03:50,911 EPOCH - 4 : training on 13192 raw words (13446 effective words) took 0.0s, 401742 effective words/s
2019-07-24 13:03:50,914 worker thread finished; awaiting finish of 2 more threads
2019-07-24 13:03:50,928 worker thread finished; awaiting finish of 1 more threads
2019-07-24 13:03:50,945 worker thread finished; awaiting finish of 0 more threads
2019-07-24 13:03:50,945 EPOCH - 5 : training on 13192 raw words (13412 effective words) took 0.0s, 414957 effective words/s
2019-07-24 13:03:50,945 training on a 65960 raw words (67198 effective words) took 0.2s, 397196 effective words/s
2019-07-24 13:03:50,945 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-24 13:46:36,797 'pattern' package not found; tag filters are not available for English
2019-07-24 13:57:27,383 consider setting layer size to a multiple of 4 for greater performance
2019-07-24 13:57:27,383 collecting all words and their counts
2019-07-24 13:57:27,383 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-24 13:57:27,387 collected 3683 word types and 1453 unique tags from a corpus of 1453 examples and 13192 words
2019-07-24 13:57:27,387 Loading a fresh vocabulary
2019-07-24 13:57:27,392 min_count=0 retains 3683 unique words (100% of original 3683, drops 0)
2019-07-24 13:57:27,392 min_count=0 leaves 13192 word corpus (100% of original 13192, drops 0)
2019-07-24 13:57:27,398 deleting the raw counts dictionary of 3683 items
2019-07-24 13:57:27,399 sample=0.001 downsamples 40 most-common words
2019-07-24 13:57:27,399 downsampling leaves estimated 11987 word corpus (90.9% of prior 13192)
2019-07-24 13:57:27,403 estimated required memory for 3683 words and 5 dimensions: 2308480 bytes
2019-07-24 13:57:27,403 resetting layer weights
2019-07-24 13:57:27,438 training model with 3 workers on 3683 vocabulary and 5 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-24 13:57:27,442 worker thread finished; awaiting finish of 2 more threads
2019-07-24 13:57:27,455 worker thread finished; awaiting finish of 1 more threads
2019-07-24 13:57:27,471 worker thread finished; awaiting finish of 0 more threads
2019-07-24 13:57:27,471 EPOCH - 1 : training on 13192 raw words (13463 effective words) took 0.0s, 446768 effective words/s
2019-07-24 13:57:27,473 worker thread finished; awaiting finish of 2 more threads
2019-07-24 13:57:27,488 worker thread finished; awaiting finish of 1 more threads
2019-07-24 13:57:27,505 worker thread finished; awaiting finish of 0 more threads
2019-07-24 13:57:27,505 EPOCH - 2 : training on 13192 raw words (13483 effective words) took 0.0s, 414779 effective words/s
2019-07-24 13:57:27,507 worker thread finished; awaiting finish of 2 more threads
2019-07-24 13:57:27,523 worker thread finished; awaiting finish of 1 more threads
2019-07-24 13:57:27,540 worker thread finished; awaiting finish of 0 more threads
2019-07-24 13:57:27,540 EPOCH - 3 : training on 13192 raw words (13460 effective words) took 0.0s, 407274 effective words/s
2019-07-24 13:57:27,543 worker thread finished; awaiting finish of 2 more threads
2019-07-24 13:57:27,558 worker thread finished; awaiting finish of 1 more threads
2019-07-24 13:57:27,576 worker thread finished; awaiting finish of 0 more threads
2019-07-24 13:57:27,576 EPOCH - 4 : training on 13192 raw words (13483 effective words) took 0.0s, 392518 effective words/s
2019-07-24 13:57:27,579 worker thread finished; awaiting finish of 2 more threads
2019-07-24 13:57:27,596 worker thread finished; awaiting finish of 1 more threads
2019-07-24 13:57:27,613 worker thread finished; awaiting finish of 0 more threads
2019-07-24 13:57:27,613 EPOCH - 5 : training on 13192 raw words (13423 effective words) took 0.0s, 390905 effective words/s
2019-07-24 13:57:27,613 training on a 65960 raw words (67312 effective words) took 0.2s, 386216 effective words/s
2019-07-24 13:57:27,613 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-24 13:57:51,430 consider setting layer size to a multiple of 4 for greater performance
2019-07-24 13:57:51,431 collecting all words and their counts
2019-07-24 13:57:51,434 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-24 13:57:51,439 collected 3683 word types and 1453 unique tags from a corpus of 1453 examples and 13192 words
2019-07-24 13:57:51,439 Loading a fresh vocabulary
2019-07-24 13:57:51,444 min_count=0 retains 3683 unique words (100% of original 3683, drops 0)
2019-07-24 13:57:51,444 min_count=0 leaves 13192 word corpus (100% of original 13192, drops 0)
2019-07-24 13:57:51,451 deleting the raw counts dictionary of 3683 items
2019-07-24 13:57:51,451 sample=0.001 downsamples 40 most-common words
2019-07-24 13:57:51,451 downsampling leaves estimated 11987 word corpus (90.9% of prior 13192)
2019-07-24 13:57:51,456 estimated required memory for 3683 words and 5 dimensions: 2308480 bytes
2019-07-24 13:57:51,456 resetting layer weights
2019-07-24 13:57:51,494 training model with 3 workers on 3683 vocabulary and 5 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-24 13:57:51,497 worker thread finished; awaiting finish of 2 more threads
2019-07-24 13:57:51,512 worker thread finished; awaiting finish of 1 more threads
2019-07-24 13:57:51,529 worker thread finished; awaiting finish of 0 more threads
2019-07-24 13:57:51,529 EPOCH - 1 : training on 13192 raw words (13463 effective words) took 0.0s, 412078 effective words/s
2019-07-24 13:57:51,531 worker thread finished; awaiting finish of 2 more threads
2019-07-24 13:57:51,549 worker thread finished; awaiting finish of 1 more threads
2019-07-24 13:57:51,565 worker thread finished; awaiting finish of 0 more threads
2019-07-24 13:57:51,566 EPOCH - 2 : training on 13192 raw words (13503 effective words) took 0.0s, 390737 effective words/s
2019-07-24 13:57:51,568 worker thread finished; awaiting finish of 2 more threads
2019-07-24 13:57:51,582 worker thread finished; awaiting finish of 1 more threads
2019-07-24 13:57:51,599 worker thread finished; awaiting finish of 0 more threads
2019-07-24 13:57:51,599 EPOCH - 3 : training on 13192 raw words (13461 effective words) took 0.0s, 423573 effective words/s
2019-07-24 13:57:51,602 worker thread finished; awaiting finish of 2 more threads
2019-07-24 13:57:51,618 worker thread finished; awaiting finish of 1 more threads
2019-07-24 13:57:51,634 worker thread finished; awaiting finish of 0 more threads
2019-07-24 13:57:51,634 EPOCH - 4 : training on 13192 raw words (13494 effective words) took 0.0s, 414903 effective words/s
2019-07-24 13:57:51,637 worker thread finished; awaiting finish of 2 more threads
2019-07-24 13:57:51,654 worker thread finished; awaiting finish of 1 more threads
2019-07-24 13:57:51,669 worker thread finished; awaiting finish of 0 more threads
2019-07-24 13:57:51,669 EPOCH - 5 : training on 13192 raw words (13465 effective words) took 0.0s, 410542 effective words/s
2019-07-24 13:57:51,669 training on a 65960 raw words (67386 effective words) took 0.2s, 386331 effective words/s
2019-07-24 13:57:51,669 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-24 13:58:30,380 'pattern' package not found; tag filters are not available for English
2019-07-24 13:58:32,139 consider setting layer size to a multiple of 4 for greater performance
2019-07-24 13:58:32,139 collecting all words and their counts
2019-07-24 13:58:32,140 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-24 13:58:32,144 collected 3683 word types and 1453 unique tags from a corpus of 1453 examples and 13192 words
2019-07-24 13:58:32,144 Loading a fresh vocabulary
2019-07-24 13:58:32,148 min_count=0 retains 3683 unique words (100% of original 3683, drops 0)
2019-07-24 13:58:32,148 min_count=0 leaves 13192 word corpus (100% of original 13192, drops 0)
2019-07-24 13:58:32,154 deleting the raw counts dictionary of 3683 items
2019-07-24 13:58:32,154 sample=0.001 downsamples 40 most-common words
2019-07-24 13:58:32,154 downsampling leaves estimated 11987 word corpus (90.9% of prior 13192)
2019-07-24 13:58:32,158 estimated required memory for 3683 words and 50 dimensions: 3895900 bytes
2019-07-24 13:58:32,158 resetting layer weights
2019-07-24 13:58:32,195 training model with 3 workers on 3683 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-24 13:58:32,197 worker thread finished; awaiting finish of 2 more threads
2019-07-24 13:58:32,212 worker thread finished; awaiting finish of 1 more threads
2019-07-24 13:58:32,228 worker thread finished; awaiting finish of 0 more threads
2019-07-24 13:58:32,228 EPOCH - 1 : training on 13192 raw words (13460 effective words) took 0.0s, 435145 effective words/s
2019-07-24 13:58:32,230 worker thread finished; awaiting finish of 2 more threads
2019-07-24 13:58:32,244 worker thread finished; awaiting finish of 1 more threads
2019-07-24 13:58:32,260 worker thread finished; awaiting finish of 0 more threads
2019-07-24 13:58:32,260 EPOCH - 2 : training on 13192 raw words (13468 effective words) took 0.0s, 448596 effective words/s
2019-07-24 13:58:32,262 worker thread finished; awaiting finish of 2 more threads
2019-07-24 13:58:32,277 worker thread finished; awaiting finish of 1 more threads
2019-07-24 13:58:32,293 worker thread finished; awaiting finish of 0 more threads
2019-07-24 13:58:32,293 EPOCH - 3 : training on 13192 raw words (13423 effective words) took 0.0s, 427612 effective words/s
2019-07-24 13:58:32,295 worker thread finished; awaiting finish of 2 more threads
2019-07-24 13:58:32,309 worker thread finished; awaiting finish of 1 more threads
2019-07-24 13:58:32,326 worker thread finished; awaiting finish of 0 more threads
2019-07-24 13:58:32,326 EPOCH - 4 : training on 13192 raw words (13501 effective words) took 0.0s, 437460 effective words/s
2019-07-24 13:58:32,329 worker thread finished; awaiting finish of 2 more threads
2019-07-24 13:58:32,343 worker thread finished; awaiting finish of 1 more threads
2019-07-24 13:58:32,359 worker thread finished; awaiting finish of 0 more threads
2019-07-24 13:58:32,359 EPOCH - 5 : training on 13192 raw words (13393 effective words) took 0.0s, 442513 effective words/s
2019-07-24 13:58:32,359 training on a 65960 raw words (67245 effective words) took 0.2s, 410037 effective words/s
2019-07-24 13:58:32,359 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-24 14:02:12,572 consider setting layer size to a multiple of 4 for greater performance
2019-07-24 14:02:12,572 collecting all words and their counts
2019-07-24 14:02:12,573 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-24 14:02:12,573 collected 17 word types and 2 unique tags from a corpus of 2 examples and 33 words
2019-07-24 14:02:12,573 Loading a fresh vocabulary
2019-07-24 14:02:12,573 min_count=0 retains 17 unique words (100% of original 17, drops 0)
2019-07-24 14:02:12,573 min_count=0 leaves 33 word corpus (100% of original 33, drops 0)
2019-07-24 14:02:12,573 deleting the raw counts dictionary of 17 items
2019-07-24 14:02:12,573 sample=0.001 downsamples 17 most-common words
2019-07-24 14:02:12,573 downsampling leaves estimated 4 word corpus (14.7% of prior 33)
2019-07-24 14:02:12,573 estimated required memory for 17 words and 3 dimensions: 9332 bytes
2019-07-24 14:02:12,573 resetting layer weights
2019-07-24 14:02:12,573 training model with 3 workers on 17 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-24 14:02:12,575 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:02:12,575 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:02:12,575 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:02:12,575 EPOCH - 1 : training on 33 raw words (7 effective words) took 0.0s, 11281 effective words/s
2019-07-24 14:02:12,576 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:02:12,576 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:02:12,576 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:02:12,576 EPOCH - 2 : training on 33 raw words (5 effective words) took 0.0s, 8809 effective words/s
2019-07-24 14:02:12,577 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:02:12,577 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:02:12,577 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:02:12,577 EPOCH - 3 : training on 33 raw words (7 effective words) took 0.0s, 13489 effective words/s
2019-07-24 14:02:12,578 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:02:12,578 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:02:12,578 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:02:12,578 EPOCH - 4 : training on 33 raw words (4 effective words) took 0.0s, 10270 effective words/s
2019-07-24 14:02:12,579 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:02:12,579 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:02:12,579 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:02:12,579 EPOCH - 5 : training on 33 raw words (7 effective words) took 0.0s, 17649 effective words/s
2019-07-24 14:02:12,579 training on a 165 raw words (30 effective words) took 0.0s, 5242 effective words/s
2019-07-24 14:02:12,579 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-24 14:02:13,690 consider setting layer size to a multiple of 4 for greater performance
2019-07-24 14:02:13,690 collecting all words and their counts
2019-07-24 14:02:13,690 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-24 14:02:13,728 PROGRESS: at example #10000, processed 127599 words (3420387/s), 14559 word types, 10000 tags
2019-07-24 14:02:13,744 collected 17571 word types and 14524 unique tags from a corpus of 14524 examples and 185031 words
2019-07-24 14:02:13,745 Loading a fresh vocabulary
2019-07-24 14:02:13,765 min_count=0 retains 17571 unique words (100% of original 17571, drops 0)
2019-07-24 14:02:13,765 min_count=0 leaves 185031 word corpus (100% of original 185031, drops 0)
2019-07-24 14:02:13,799 deleting the raw counts dictionary of 17571 items
2019-07-24 14:02:13,799 sample=0.001 downsamples 29 most-common words
2019-07-24 14:02:13,799 downsampling leaves estimated 146451 word corpus (79.1% of prior 185031)
2019-07-24 14:02:13,827 estimated required memory for 17571 words and 3 dimensions: 12286292 bytes
2019-07-24 14:02:13,827 resetting layer weights
2019-07-24 14:02:14,077 training model with 3 workers on 17571 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-24 14:02:14,451 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:02:14,452 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:02:14,459 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:02:14,460 EPOCH - 1 : training on 185031 raw words (160935 effective words) took 0.4s, 425927 effective words/s
2019-07-24 14:02:14,810 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:02:14,816 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:02:14,821 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:02:14,822 EPOCH - 2 : training on 185031 raw words (161002 effective words) took 0.4s, 450772 effective words/s
2019-07-24 14:02:15,182 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:02:15,194 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:02:15,197 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:02:15,197 EPOCH - 3 : training on 185031 raw words (161010 effective words) took 0.4s, 434700 effective words/s
2019-07-24 14:02:15,553 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:02:15,564 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:02:15,564 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:02:15,564 EPOCH - 4 : training on 185031 raw words (161041 effective words) took 0.4s, 444085 effective words/s
2019-07-24 14:02:15,926 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:02:15,929 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:02:15,933 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:02:15,933 EPOCH - 5 : training on 185031 raw words (160934 effective words) took 0.4s, 441917 effective words/s
2019-07-24 14:02:15,933 training on a 925155 raw words (804922 effective words) took 1.9s, 433581 effective words/s
2019-07-24 14:02:22,715 consider setting layer size to a multiple of 4 for greater performance
2019-07-24 14:02:22,715 collecting all words and their counts
2019-07-24 14:02:22,715 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-24 14:02:22,715 collected 17 word types and 2 unique tags from a corpus of 2 examples and 33 words
2019-07-24 14:02:22,715 Loading a fresh vocabulary
2019-07-24 14:02:22,715 min_count=0 retains 17 unique words (100% of original 17, drops 0)
2019-07-24 14:02:22,715 min_count=0 leaves 33 word corpus (100% of original 33, drops 0)
2019-07-24 14:02:22,716 deleting the raw counts dictionary of 17 items
2019-07-24 14:02:22,716 sample=0.001 downsamples 17 most-common words
2019-07-24 14:02:22,716 downsampling leaves estimated 4 word corpus (14.7% of prior 33)
2019-07-24 14:02:22,716 estimated required memory for 17 words and 3 dimensions: 9332 bytes
2019-07-24 14:02:22,716 resetting layer weights
2019-07-24 14:02:22,716 training model with 3 workers on 17 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-24 14:02:22,718 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:02:22,718 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:02:22,718 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:02:22,718 EPOCH - 1 : training on 33 raw words (7 effective words) took 0.0s, 11301 effective words/s
2019-07-24 14:02:22,719 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:02:22,719 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:02:22,719 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:02:22,719 EPOCH - 2 : training on 33 raw words (5 effective words) took 0.0s, 9292 effective words/s
2019-07-24 14:02:22,720 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:02:22,720 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:02:22,720 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:02:22,720 EPOCH - 3 : training on 33 raw words (7 effective words) took 0.0s, 10554 effective words/s
2019-07-24 14:02:22,721 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:02:22,722 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:02:22,722 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:02:22,722 EPOCH - 4 : training on 33 raw words (4 effective words) took 0.0s, 6145 effective words/s
2019-07-24 14:02:22,723 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:02:22,723 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:02:22,723 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:02:22,723 EPOCH - 5 : training on 33 raw words (7 effective words) took 0.0s, 11739 effective words/s
2019-07-24 14:02:22,723 training on a 165 raw words (30 effective words) took 0.0s, 4474 effective words/s
2019-07-24 14:02:22,723 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-24 14:02:23,907 consider setting layer size to a multiple of 4 for greater performance
2019-07-24 14:02:23,907 collecting all words and their counts
2019-07-24 14:02:23,907 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-24 14:02:23,946 PROGRESS: at example #10000, processed 127599 words (3276152/s), 14559 word types, 10000 tags
2019-07-24 14:02:23,965 collected 17571 word types and 14524 unique tags from a corpus of 14524 examples and 185031 words
2019-07-24 14:02:23,965 Loading a fresh vocabulary
2019-07-24 14:02:24,020 min_count=0 retains 17571 unique words (100% of original 17571, drops 0)
2019-07-24 14:02:24,020 min_count=0 leaves 185031 word corpus (100% of original 185031, drops 0)
2019-07-24 14:02:24,059 deleting the raw counts dictionary of 17571 items
2019-07-24 14:02:24,060 sample=0.001 downsamples 29 most-common words
2019-07-24 14:02:24,060 downsampling leaves estimated 146451 word corpus (79.1% of prior 185031)
2019-07-24 14:02:24,088 estimated required memory for 17571 words and 3 dimensions: 12286292 bytes
2019-07-24 14:02:24,088 resetting layer weights
2019-07-24 14:02:24,311 training model with 3 workers on 17571 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-24 14:02:24,684 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:02:24,693 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:02:24,695 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:02:24,695 EPOCH - 1 : training on 185031 raw words (160915 effective words) took 0.4s, 424778 effective words/s
2019-07-24 14:02:25,078 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:02:25,081 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:02:25,086 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:02:25,086 EPOCH - 2 : training on 185031 raw words (160908 effective words) took 0.4s, 416179 effective words/s
2019-07-24 14:02:25,454 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:02:25,458 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:02:25,466 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:02:25,466 EPOCH - 3 : training on 185031 raw words (161163 effective words) took 0.4s, 429018 effective words/s
2019-07-24 14:02:25,841 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:02:25,849 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:02:25,853 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:02:25,853 EPOCH - 4 : training on 185031 raw words (161188 effective words) took 0.4s, 421226 effective words/s
2019-07-24 14:02:26,231 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:02:26,235 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:02:26,241 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:02:26,241 EPOCH - 5 : training on 185031 raw words (161158 effective words) took 0.4s, 420861 effective words/s
2019-07-24 14:02:26,241 training on a 925155 raw words (805332 effective words) took 1.9s, 417176 effective words/s
2019-07-24 14:04:24,057 consider setting layer size to a multiple of 4 for greater performance
2019-07-24 14:04:24,057 collecting all words and their counts
2019-07-24 14:04:24,057 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-24 14:04:24,062 collected 3683 word types and 1453 unique tags from a corpus of 1453 examples and 13192 words
2019-07-24 14:04:24,062 Loading a fresh vocabulary
2019-07-24 14:04:24,136 min_count=0 retains 3683 unique words (100% of original 3683, drops 0)
2019-07-24 14:04:24,136 min_count=0 leaves 13192 word corpus (100% of original 13192, drops 0)
2019-07-24 14:04:24,142 deleting the raw counts dictionary of 3683 items
2019-07-24 14:04:24,142 sample=0.001 downsamples 40 most-common words
2019-07-24 14:04:24,142 downsampling leaves estimated 11987 word corpus (90.9% of prior 13192)
2019-07-24 14:04:24,146 estimated required memory for 3683 words and 50 dimensions: 3895900 bytes
2019-07-24 14:04:24,147 resetting layer weights
2019-07-24 14:04:24,181 training model with 3 workers on 3683 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-24 14:04:24,184 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:04:24,199 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:04:24,215 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:04:24,215 EPOCH - 1 : training on 13192 raw words (13450 effective words) took 0.0s, 425128 effective words/s
2019-07-24 14:04:24,217 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:04:24,232 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:04:24,248 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:04:24,248 EPOCH - 2 : training on 13192 raw words (13464 effective words) took 0.0s, 434756 effective words/s
2019-07-24 14:04:24,251 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:04:24,265 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:04:24,281 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:04:24,281 EPOCH - 3 : training on 13192 raw words (13424 effective words) took 0.0s, 437783 effective words/s
2019-07-24 14:04:24,284 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:04:24,298 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:04:24,315 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:04:24,315 EPOCH - 4 : training on 13192 raw words (13464 effective words) took 0.0s, 417312 effective words/s
2019-07-24 14:04:24,317 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:04:24,331 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:04:24,348 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:04:24,348 EPOCH - 5 : training on 13192 raw words (13388 effective words) took 0.0s, 423238 effective words/s
2019-07-24 14:04:24,348 training on a 65960 raw words (67190 effective words) took 0.2s, 402522 effective words/s
2019-07-24 14:04:24,348 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-24 14:51:19,258 consider setting layer size to a multiple of 4 for greater performance
2019-07-24 14:51:19,258 collecting all words and their counts
2019-07-24 14:51:19,258 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-24 14:51:19,258 collected 17 word types and 2 unique tags from a corpus of 2 examples and 33 words
2019-07-24 14:51:19,258 Loading a fresh vocabulary
2019-07-24 14:51:19,258 min_count=0 retains 17 unique words (100% of original 17, drops 0)
2019-07-24 14:51:19,258 min_count=0 leaves 33 word corpus (100% of original 33, drops 0)
2019-07-24 14:51:19,258 deleting the raw counts dictionary of 17 items
2019-07-24 14:51:19,259 sample=0.001 downsamples 17 most-common words
2019-07-24 14:51:19,259 downsampling leaves estimated 4 word corpus (14.7% of prior 33)
2019-07-24 14:51:19,259 estimated required memory for 17 words and 3 dimensions: 9332 bytes
2019-07-24 14:51:19,259 resetting layer weights
2019-07-24 14:51:19,259 training model with 3 workers on 17 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-24 14:51:19,260 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:51:19,260 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:51:19,261 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:51:19,261 EPOCH - 1 : training on 33 raw words (7 effective words) took 0.0s, 10959 effective words/s
2019-07-24 14:51:19,261 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:51:19,262 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:51:19,262 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:51:19,262 EPOCH - 2 : training on 33 raw words (5 effective words) took 0.0s, 10095 effective words/s
2019-07-24 14:51:19,263 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:51:19,263 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:51:19,263 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:51:19,263 EPOCH - 3 : training on 33 raw words (7 effective words) took 0.0s, 13342 effective words/s
2019-07-24 14:51:19,264 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:51:19,264 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:51:19,264 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:51:19,264 EPOCH - 4 : training on 33 raw words (4 effective words) took 0.0s, 7509 effective words/s
2019-07-24 14:51:19,265 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:51:19,265 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:51:19,265 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:51:19,265 EPOCH - 5 : training on 33 raw words (7 effective words) took 0.0s, 13734 effective words/s
2019-07-24 14:51:19,265 training on a 165 raw words (30 effective words) took 0.0s, 5358 effective words/s
2019-07-24 14:51:19,265 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-24 14:51:20,416 consider setting layer size to a multiple of 4 for greater performance
2019-07-24 14:51:20,416 collecting all words and their counts
2019-07-24 14:51:20,416 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-24 14:51:20,453 PROGRESS: at example #10000, processed 127599 words (3431851/s), 14559 word types, 10000 tags
2019-07-24 14:51:20,471 collected 17571 word types and 14524 unique tags from a corpus of 14524 examples and 185031 words
2019-07-24 14:51:20,471 Loading a fresh vocabulary
2019-07-24 14:51:20,534 min_count=0 retains 17571 unique words (100% of original 17571, drops 0)
2019-07-24 14:51:20,534 min_count=0 leaves 185031 word corpus (100% of original 185031, drops 0)
2019-07-24 14:51:20,564 deleting the raw counts dictionary of 17571 items
2019-07-24 14:51:20,565 sample=0.001 downsamples 29 most-common words
2019-07-24 14:51:20,565 downsampling leaves estimated 146451 word corpus (79.1% of prior 185031)
2019-07-24 14:51:20,593 estimated required memory for 17571 words and 3 dimensions: 12286292 bytes
2019-07-24 14:51:20,593 resetting layer weights
2019-07-24 14:51:20,805 training model with 3 workers on 17571 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-24 14:51:21,181 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:51:21,191 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:51:21,196 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:51:21,197 EPOCH - 1 : training on 185031 raw words (161049 effective words) took 0.4s, 416496 effective words/s
2019-07-24 14:51:21,569 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:51:21,572 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:51:21,574 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:51:21,574 EPOCH - 2 : training on 185031 raw words (160863 effective words) took 0.4s, 432264 effective words/s
2019-07-24 14:51:21,939 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:51:21,941 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:51:21,945 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:51:21,945 EPOCH - 3 : training on 185031 raw words (160980 effective words) took 0.4s, 439381 effective words/s
2019-07-24 14:51:22,331 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:51:22,334 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:51:22,336 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:51:22,336 EPOCH - 4 : training on 185031 raw words (161064 effective words) took 0.4s, 416837 effective words/s
2019-07-24 14:51:22,715 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:51:22,719 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:51:22,722 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:51:22,722 EPOCH - 5 : training on 185031 raw words (160980 effective words) took 0.4s, 422474 effective words/s
2019-07-24 14:51:22,722 training on a 925155 raw words (804936 effective words) took 1.9s, 419832 effective words/s
2019-07-24 14:51:29,290 consider setting layer size to a multiple of 4 for greater performance
2019-07-24 14:51:29,290 collecting all words and their counts
2019-07-24 14:51:29,290 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-24 14:51:29,290 collected 17 word types and 2 unique tags from a corpus of 2 examples and 33 words
2019-07-24 14:51:29,290 Loading a fresh vocabulary
2019-07-24 14:51:29,290 min_count=0 retains 17 unique words (100% of original 17, drops 0)
2019-07-24 14:51:29,290 min_count=0 leaves 33 word corpus (100% of original 33, drops 0)
2019-07-24 14:51:29,290 deleting the raw counts dictionary of 17 items
2019-07-24 14:51:29,290 sample=0.001 downsamples 17 most-common words
2019-07-24 14:51:29,290 downsampling leaves estimated 4 word corpus (14.7% of prior 33)
2019-07-24 14:51:29,290 estimated required memory for 17 words and 3 dimensions: 9332 bytes
2019-07-24 14:51:29,290 resetting layer weights
2019-07-24 14:51:29,291 training model with 3 workers on 17 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-24 14:51:29,292 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:51:29,292 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:51:29,292 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:51:29,292 EPOCH - 1 : training on 33 raw words (7 effective words) took 0.0s, 10407 effective words/s
2019-07-24 14:51:29,293 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:51:29,293 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:51:29,293 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:51:29,293 EPOCH - 2 : training on 33 raw words (5 effective words) took 0.0s, 10454 effective words/s
2019-07-24 14:51:29,294 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:51:29,294 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:51:29,294 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:51:29,294 EPOCH - 3 : training on 33 raw words (7 effective words) took 0.0s, 13786 effective words/s
2019-07-24 14:51:29,295 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:51:29,295 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:51:29,295 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:51:29,295 EPOCH - 4 : training on 33 raw words (4 effective words) took 0.0s, 8841 effective words/s
2019-07-24 14:51:29,296 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:51:29,296 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:51:29,297 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:51:29,297 EPOCH - 5 : training on 33 raw words (7 effective words) took 0.0s, 12975 effective words/s
2019-07-24 14:51:29,297 training on a 165 raw words (30 effective words) took 0.0s, 5002 effective words/s
2019-07-24 14:51:29,297 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-24 14:51:30,450 consider setting layer size to a multiple of 4 for greater performance
2019-07-24 14:51:30,450 collecting all words and their counts
2019-07-24 14:51:30,450 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-24 14:51:30,486 PROGRESS: at example #10000, processed 127599 words (3532485/s), 14559 word types, 10000 tags
2019-07-24 14:51:30,503 collected 17571 word types and 14524 unique tags from a corpus of 14524 examples and 185031 words
2019-07-24 14:51:30,503 Loading a fresh vocabulary
2019-07-24 14:51:30,524 min_count=0 retains 17571 unique words (100% of original 17571, drops 0)
2019-07-24 14:51:30,524 min_count=0 leaves 185031 word corpus (100% of original 185031, drops 0)
2019-07-24 14:51:30,555 deleting the raw counts dictionary of 17571 items
2019-07-24 14:51:30,556 sample=0.001 downsamples 29 most-common words
2019-07-24 14:51:30,556 downsampling leaves estimated 146451 word corpus (79.1% of prior 185031)
2019-07-24 14:51:30,584 estimated required memory for 17571 words and 3 dimensions: 12286292 bytes
2019-07-24 14:51:30,584 resetting layer weights
2019-07-24 14:51:30,791 training model with 3 workers on 17571 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-24 14:51:31,161 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:51:31,169 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:51:31,175 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:51:31,175 EPOCH - 1 : training on 185031 raw words (160911 effective words) took 0.4s, 424386 effective words/s
2019-07-24 14:51:31,545 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:51:31,553 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:51:31,557 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:51:31,557 EPOCH - 2 : training on 185031 raw words (160958 effective words) took 0.4s, 426341 effective words/s
2019-07-24 14:51:31,927 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:51:31,932 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:51:31,939 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:51:31,939 EPOCH - 3 : training on 185031 raw words (161125 effective words) took 0.4s, 428061 effective words/s
2019-07-24 14:51:32,310 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:51:32,325 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:51:32,325 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:51:32,325 EPOCH - 4 : training on 185031 raw words (161004 effective words) took 0.4s, 421707 effective words/s
2019-07-24 14:51:32,690 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:51:32,693 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:51:32,700 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:51:32,700 EPOCH - 5 : training on 185031 raw words (161182 effective words) took 0.4s, 435987 effective words/s
2019-07-24 14:51:32,700 training on a 925155 raw words (805180 effective words) took 1.9s, 421780 effective words/s
2019-07-24 14:53:02,489 consider setting layer size to a multiple of 4 for greater performance
2019-07-24 14:53:02,489 collecting all words and their counts
2019-07-24 14:53:02,490 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-24 14:53:02,490 collected 17 word types and 2 unique tags from a corpus of 2 examples and 33 words
2019-07-24 14:53:02,490 Loading a fresh vocabulary
2019-07-24 14:53:02,490 min_count=0 retains 17 unique words (100% of original 17, drops 0)
2019-07-24 14:53:02,490 min_count=0 leaves 33 word corpus (100% of original 33, drops 0)
2019-07-24 14:53:02,490 deleting the raw counts dictionary of 17 items
2019-07-24 14:53:02,490 sample=0.001 downsamples 17 most-common words
2019-07-24 14:53:02,490 downsampling leaves estimated 4 word corpus (14.7% of prior 33)
2019-07-24 14:53:02,490 estimated required memory for 17 words and 3 dimensions: 9332 bytes
2019-07-24 14:53:02,490 resetting layer weights
2019-07-24 14:53:02,490 training model with 3 workers on 17 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-24 14:53:02,491 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:53:02,491 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:53:02,491 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:53:02,491 EPOCH - 1 : training on 33 raw words (7 effective words) took 0.0s, 13772 effective words/s
2019-07-24 14:53:02,494 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:53:02,494 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:53:02,494 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:53:02,495 EPOCH - 2 : training on 33 raw words (5 effective words) took 0.0s, 7795 effective words/s
2019-07-24 14:53:02,495 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:53:02,495 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:53:02,496 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:53:02,496 EPOCH - 3 : training on 33 raw words (7 effective words) took 0.0s, 15509 effective words/s
2019-07-24 14:53:02,500 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:53:02,500 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:53:02,500 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:53:02,500 EPOCH - 4 : training on 33 raw words (4 effective words) took 0.0s, 12854 effective words/s
2019-07-24 14:53:02,501 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:53:02,501 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:53:02,502 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:53:02,502 EPOCH - 5 : training on 33 raw words (7 effective words) took 0.0s, 16233 effective words/s
2019-07-24 14:53:02,502 training on a 165 raw words (30 effective words) took 0.0s, 2666 effective words/s
2019-07-24 14:53:02,502 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-24 14:53:03,669 consider setting layer size to a multiple of 4 for greater performance
2019-07-24 14:53:03,669 collecting all words and their counts
2019-07-24 14:53:03,669 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-24 14:53:16,631 consider setting layer size to a multiple of 4 for greater performance
2019-07-24 14:53:16,631 collecting all words and their counts
2019-07-24 14:53:16,631 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-24 14:53:16,631 collected 17 word types and 2 unique tags from a corpus of 2 examples and 33 words
2019-07-24 14:53:16,631 Loading a fresh vocabulary
2019-07-24 14:53:16,632 min_count=0 retains 17 unique words (100% of original 17, drops 0)
2019-07-24 14:53:16,632 min_count=0 leaves 33 word corpus (100% of original 33, drops 0)
2019-07-24 14:53:16,632 deleting the raw counts dictionary of 17 items
2019-07-24 14:53:16,632 sample=0.001 downsamples 17 most-common words
2019-07-24 14:53:16,632 downsampling leaves estimated 4 word corpus (14.7% of prior 33)
2019-07-24 14:53:16,632 estimated required memory for 17 words and 3 dimensions: 9332 bytes
2019-07-24 14:53:16,632 resetting layer weights
2019-07-24 14:53:16,632 training model with 3 workers on 17 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-24 14:53:16,633 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:53:16,633 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:53:16,633 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:53:16,633 EPOCH - 1 : training on 33 raw words (7 effective words) took 0.0s, 15086 effective words/s
2019-07-24 14:53:16,634 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:53:16,635 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:53:16,635 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:53:16,635 EPOCH - 2 : training on 33 raw words (5 effective words) took 0.0s, 9669 effective words/s
2019-07-24 14:53:16,635 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:53:16,636 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:53:16,636 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:53:16,636 EPOCH - 3 : training on 33 raw words (7 effective words) took 0.0s, 16312 effective words/s
2019-07-24 14:53:16,636 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:53:16,636 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:53:16,636 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:53:16,637 EPOCH - 4 : training on 33 raw words (4 effective words) took 0.0s, 11096 effective words/s
2019-07-24 14:53:16,637 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:53:16,637 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:53:16,638 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:53:16,638 EPOCH - 5 : training on 33 raw words (7 effective words) took 0.0s, 13097 effective words/s
2019-07-24 14:53:16,638 training on a 165 raw words (30 effective words) took 0.0s, 5426 effective words/s
2019-07-24 14:53:16,638 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-07-24 14:53:17,896 consider setting layer size to a multiple of 4 for greater performance
2019-07-24 14:53:17,896 collecting all words and their counts
2019-07-24 14:53:17,896 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-07-24 14:53:17,938 PROGRESS: at example #10000, processed 127599 words (3030652/s), 14559 word types, 10000 tags
2019-07-24 14:53:17,956 collected 17571 word types and 14524 unique tags from a corpus of 14524 examples and 185031 words
2019-07-24 14:53:17,956 Loading a fresh vocabulary
2019-07-24 14:53:17,977 min_count=0 retains 17571 unique words (100% of original 17571, drops 0)
2019-07-24 14:53:17,978 min_count=0 leaves 185031 word corpus (100% of original 185031, drops 0)
2019-07-24 14:53:18,011 deleting the raw counts dictionary of 17571 items
2019-07-24 14:53:18,012 sample=0.001 downsamples 29 most-common words
2019-07-24 14:53:18,012 downsampling leaves estimated 146451 word corpus (79.1% of prior 185031)
2019-07-24 14:53:18,051 estimated required memory for 17571 words and 3 dimensions: 12286292 bytes
2019-07-24 14:53:18,051 resetting layer weights
2019-07-24 14:53:18,275 training model with 3 workers on 17571 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-07-24 14:53:18,655 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:53:18,667 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:53:18,670 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:53:18,670 EPOCH - 1 : training on 185031 raw words (160968 effective words) took 0.4s, 413484 effective words/s
2019-07-24 14:53:19,069 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:53:19,074 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:53:19,080 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:53:19,080 EPOCH - 2 : training on 185031 raw words (161093 effective words) took 0.4s, 398811 effective words/s
2019-07-24 14:53:19,468 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:53:19,471 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:53:19,481 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:53:19,481 EPOCH - 3 : training on 185031 raw words (161123 effective words) took 0.4s, 405831 effective words/s
2019-07-24 14:53:19,855 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:53:19,878 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:53:19,878 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:53:19,878 EPOCH - 4 : training on 185031 raw words (160920 effective words) took 0.4s, 410507 effective words/s
2019-07-24 14:53:20,277 worker thread finished; awaiting finish of 2 more threads
2019-07-24 14:53:20,277 worker thread finished; awaiting finish of 1 more threads
2019-07-24 14:53:20,282 worker thread finished; awaiting finish of 0 more threads
2019-07-24 14:53:20,282 EPOCH - 5 : training on 185031 raw words (161004 effective words) took 0.4s, 403611 effective words/s
2019-07-24 14:53:20,282 training on a 925155 raw words (805108 effective words) took 2.0s, 401186 effective words/s
2019-08-02 10:59:08,788 consider setting layer size to a multiple of 4 for greater performance
2019-08-02 10:59:08,788 collecting all words and their counts
2019-08-02 10:59:08,788 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-08-02 10:59:08,797 collected 3683 word types and 1453 unique tags from a corpus of 1453 examples and 13192 words
2019-08-02 10:59:08,797 Loading a fresh vocabulary
2019-08-02 10:59:08,805 min_count=0 retains 3683 unique words (100% of original 3683, drops 0)
2019-08-02 10:59:08,805 min_count=0 leaves 13192 word corpus (100% of original 13192, drops 0)
2019-08-02 10:59:08,814 deleting the raw counts dictionary of 3683 items
2019-08-02 10:59:08,815 sample=0.001 downsamples 40 most-common words
2019-08-02 10:59:08,815 downsampling leaves estimated 11987 word corpus (90.9% of prior 13192)
2019-08-02 10:59:08,823 estimated required memory for 3683 words and 50 dimensions: 3895900 bytes
2019-08-02 10:59:08,823 resetting layer weights
2019-08-02 10:59:08,864 training model with 3 workers on 3683 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-08-02 10:59:08,867 worker thread finished; awaiting finish of 2 more threads
2019-08-02 10:59:08,883 worker thread finished; awaiting finish of 1 more threads
2019-08-02 10:59:08,899 worker thread finished; awaiting finish of 0 more threads
2019-08-02 10:59:08,900 EPOCH - 1 : training on 13192 raw words (13452 effective words) took 0.0s, 404791 effective words/s
2019-08-02 10:59:08,902 worker thread finished; awaiting finish of 2 more threads
2019-08-02 10:59:08,916 worker thread finished; awaiting finish of 1 more threads
2019-08-02 10:59:08,933 worker thread finished; awaiting finish of 0 more threads
2019-08-02 10:59:08,933 EPOCH - 2 : training on 13192 raw words (13486 effective words) took 0.0s, 426917 effective words/s
2019-08-02 10:59:08,936 worker thread finished; awaiting finish of 2 more threads
2019-08-02 10:59:08,949 worker thread finished; awaiting finish of 1 more threads
2019-08-02 10:59:08,965 worker thread finished; awaiting finish of 0 more threads
2019-08-02 10:59:08,965 EPOCH - 3 : training on 13192 raw words (13417 effective words) took 0.0s, 440784 effective words/s
2019-08-02 10:59:08,968 worker thread finished; awaiting finish of 2 more threads
2019-08-02 10:59:08,981 worker thread finished; awaiting finish of 1 more threads
2019-08-02 10:59:08,997 worker thread finished; awaiting finish of 0 more threads
2019-08-02 10:59:08,997 EPOCH - 4 : training on 13192 raw words (13461 effective words) took 0.0s, 441943 effective words/s
2019-08-02 10:59:09,000 worker thread finished; awaiting finish of 2 more threads
2019-08-02 10:59:09,014 worker thread finished; awaiting finish of 1 more threads
2019-08-02 10:59:09,031 worker thread finished; awaiting finish of 0 more threads
2019-08-02 10:59:09,031 EPOCH - 5 : training on 13192 raw words (13399 effective words) took 0.0s, 416461 effective words/s
2019-08-02 10:59:09,032 training on a 65960 raw words (67215 effective words) took 0.2s, 401820 effective words/s
2019-08-02 10:59:09,032 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-08-07 08:57:39,577 consider setting layer size to a multiple of 4 for greater performance
2019-08-07 08:57:39,577 collecting all words and their counts
2019-08-07 08:57:39,577 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-08-07 08:57:39,582 collected 3683 word types and 1453 unique tags from a corpus of 1453 examples and 13192 words
2019-08-07 08:57:39,582 Loading a fresh vocabulary
2019-08-07 08:57:39,587 min_count=0 retains 3683 unique words (100% of original 3683, drops 0)
2019-08-07 08:57:39,587 min_count=0 leaves 13192 word corpus (100% of original 13192, drops 0)
2019-08-07 08:57:39,593 deleting the raw counts dictionary of 3683 items
2019-08-07 08:57:39,594 sample=0.001 downsamples 40 most-common words
2019-08-07 08:57:39,594 downsampling leaves estimated 11987 word corpus (90.9% of prior 13192)
2019-08-07 08:57:39,598 estimated required memory for 3683 words and 50 dimensions: 3895900 bytes
2019-08-07 08:57:39,598 resetting layer weights
2019-08-07 08:57:39,637 training model with 3 workers on 3683 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-08-07 08:57:39,640 worker thread finished; awaiting finish of 2 more threads
2019-08-07 08:57:39,654 worker thread finished; awaiting finish of 1 more threads
2019-08-07 08:57:39,673 worker thread finished; awaiting finish of 0 more threads
2019-08-07 08:57:39,673 EPOCH - 1 : training on 13192 raw words (13409 effective words) took 0.0s, 401210 effective words/s
2019-08-07 08:57:39,675 worker thread finished; awaiting finish of 2 more threads
2019-08-07 08:57:39,694 worker thread finished; awaiting finish of 1 more threads
2019-08-07 08:57:39,712 worker thread finished; awaiting finish of 0 more threads
2019-08-07 08:57:39,712 EPOCH - 2 : training on 13192 raw words (13414 effective words) took 0.0s, 362266 effective words/s
2019-08-07 08:57:39,715 worker thread finished; awaiting finish of 2 more threads
2019-08-07 08:57:39,730 worker thread finished; awaiting finish of 1 more threads
2019-08-07 08:57:39,748 worker thread finished; awaiting finish of 0 more threads
2019-08-07 08:57:39,748 EPOCH - 3 : training on 13192 raw words (13424 effective words) took 0.0s, 391131 effective words/s
2019-08-07 08:57:39,751 worker thread finished; awaiting finish of 2 more threads
2019-08-07 08:57:39,771 worker thread finished; awaiting finish of 1 more threads
2019-08-07 08:57:39,788 worker thread finished; awaiting finish of 0 more threads
2019-08-07 08:57:39,788 EPOCH - 4 : training on 13192 raw words (13487 effective words) took 0.0s, 351086 effective words/s
2019-08-07 08:57:39,792 worker thread finished; awaiting finish of 2 more threads
2019-08-07 08:57:39,809 worker thread finished; awaiting finish of 1 more threads
2019-08-07 08:57:39,826 worker thread finished; awaiting finish of 0 more threads
2019-08-07 08:57:39,827 EPOCH - 5 : training on 13192 raw words (13384 effective words) took 0.0s, 379517 effective words/s
2019-08-07 08:57:39,827 training on a 65960 raw words (67118 effective words) took 0.2s, 354575 effective words/s
2019-08-07 08:57:39,827 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-08-12 07:30:55,045 'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.
2019-08-12 07:41:15,944 'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.
2019-08-12 07:52:49,514 'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.
2019-08-12 08:22:04,540 'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.
2019-08-13 06:46:41,200 'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.
2019-08-13 06:46:41,205 'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.
2019-08-13 06:46:41,209 'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.
2019-08-13 06:46:41,212 'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.
2019-08-13 06:46:41,217 'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.
2019-08-13 06:46:41,220 'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.
2019-08-13 06:46:41,223 'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.
2019-08-13 06:46:41,225 'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.
2019-08-13 06:46:41,229 'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.
2019-08-13 06:46:41,232 'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.
2019-08-13 06:47:29,660 consider setting layer size to a multiple of 4 for greater performance
2019-08-13 06:47:29,660 collecting all words and their counts
2019-08-13 06:47:29,660 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-08-13 06:47:29,665 collected 3683 word types and 1453 unique tags from a corpus of 1453 examples and 13192 words
2019-08-13 06:47:29,665 Loading a fresh vocabulary
2019-08-13 06:47:29,669 min_count=0 retains 3683 unique words (100% of original 3683, drops 0)
2019-08-13 06:47:29,669 min_count=0 leaves 13192 word corpus (100% of original 13192, drops 0)
2019-08-13 06:47:29,676 deleting the raw counts dictionary of 3683 items
2019-08-13 06:47:29,676 sample=0.001 downsamples 40 most-common words
2019-08-13 06:47:29,676 downsampling leaves estimated 11987 word corpus (90.9% of prior 13192)
2019-08-13 06:47:29,681 estimated required memory for 3683 words and 50 dimensions: 3895900 bytes
2019-08-13 06:47:29,681 resetting layer weights
2019-08-13 06:47:29,721 training model with 3 workers on 3683 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-08-13 06:47:29,723 worker thread finished; awaiting finish of 2 more threads
2019-08-13 06:47:29,741 worker thread finished; awaiting finish of 1 more threads
2019-08-13 06:47:29,759 worker thread finished; awaiting finish of 0 more threads
2019-08-13 06:47:29,759 EPOCH - 1 : training on 13192 raw words (13474 effective words) took 0.0s, 376888 effective words/s
2019-08-13 06:47:29,761 worker thread finished; awaiting finish of 2 more threads
2019-08-13 06:47:29,778 worker thread finished; awaiting finish of 1 more threads
2019-08-13 06:47:29,796 worker thread finished; awaiting finish of 0 more threads
2019-08-13 06:47:29,796 EPOCH - 2 : training on 13192 raw words (13485 effective words) took 0.0s, 378516 effective words/s
2019-08-13 06:47:29,799 worker thread finished; awaiting finish of 2 more threads
2019-08-13 06:47:29,813 worker thread finished; awaiting finish of 1 more threads
2019-08-13 06:47:29,831 worker thread finished; awaiting finish of 0 more threads
2019-08-13 06:47:29,831 EPOCH - 3 : training on 13192 raw words (13485 effective words) took 0.0s, 410863 effective words/s
2019-08-13 06:47:29,833 worker thread finished; awaiting finish of 2 more threads
2019-08-13 06:47:29,847 worker thread finished; awaiting finish of 1 more threads
2019-08-13 06:47:29,865 worker thread finished; awaiting finish of 0 more threads
2019-08-13 06:47:29,865 EPOCH - 4 : training on 13192 raw words (13446 effective words) took 0.0s, 409235 effective words/s
2019-08-13 06:47:29,868 worker thread finished; awaiting finish of 2 more threads
2019-08-13 06:47:29,887 worker thread finished; awaiting finish of 1 more threads
2019-08-13 06:47:29,904 worker thread finished; awaiting finish of 0 more threads
2019-08-13 06:47:29,904 EPOCH - 5 : training on 13192 raw words (13416 effective words) took 0.0s, 369458 effective words/s
2019-08-13 06:47:29,904 training on a 65960 raw words (67306 effective words) took 0.2s, 367460 effective words/s
2019-08-13 06:47:29,904 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-08-13 06:49:08,654 'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.
2019-08-13 06:49:08,658 'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.
2019-08-13 06:49:08,662 'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.
2019-08-13 06:49:08,666 'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.
2019-08-13 06:49:08,671 'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.
2019-08-13 06:49:08,675 'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.
2019-08-13 06:49:08,680 'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.
2019-08-13 06:49:08,684 'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.
2019-08-13 06:49:08,693 'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.
2019-08-13 06:49:08,701 'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.
2019-08-14 07:32:11,809 consider setting layer size to a multiple of 4 for greater performance
2019-08-14 07:32:11,809 collecting all words and their counts
2019-08-14 07:32:11,810 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-08-14 07:32:11,819 collected 3683 word types and 1453 unique tags from a corpus of 1453 examples and 13192 words
2019-08-14 07:32:11,819 Loading a fresh vocabulary
2019-08-14 07:32:11,828 min_count=0 retains 3683 unique words (100% of original 3683, drops 0)
2019-08-14 07:32:11,828 min_count=0 leaves 13192 word corpus (100% of original 13192, drops 0)
2019-08-14 07:32:11,842 deleting the raw counts dictionary of 3683 items
2019-08-14 07:32:11,842 sample=0.001 downsamples 40 most-common words
2019-08-14 07:32:11,842 downsampling leaves estimated 11987 word corpus (90.9% of prior 13192)
2019-08-14 07:32:11,852 estimated required memory for 3683 words and 50 dimensions: 3895900 bytes
2019-08-14 07:32:11,853 resetting layer weights
2019-08-14 07:32:11,906 training model with 3 workers on 3683 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-08-14 07:32:11,909 worker thread finished; awaiting finish of 2 more threads
2019-08-14 07:32:11,941 worker thread finished; awaiting finish of 1 more threads
2019-08-14 07:32:11,957 worker thread finished; awaiting finish of 0 more threads
2019-08-14 07:32:11,957 EPOCH - 1 : training on 13192 raw words (13448 effective words) took 0.0s, 274925 effective words/s
2019-08-14 07:32:11,960 worker thread finished; awaiting finish of 2 more threads
2019-08-14 07:32:11,983 worker thread finished; awaiting finish of 1 more threads
2019-08-14 07:32:12,006 worker thread finished; awaiting finish of 0 more threads
2019-08-14 07:32:12,006 EPOCH - 2 : training on 13192 raw words (13484 effective words) took 0.0s, 286733 effective words/s
2019-08-14 07:32:12,010 worker thread finished; awaiting finish of 2 more threads
2019-08-14 07:32:12,028 worker thread finished; awaiting finish of 1 more threads
2019-08-14 07:32:12,066 worker thread finished; awaiting finish of 0 more threads
2019-08-14 07:32:12,066 EPOCH - 3 : training on 13192 raw words (13424 effective words) took 0.1s, 236384 effective words/s
2019-08-14 07:32:12,068 worker thread finished; awaiting finish of 2 more threads
2019-08-14 07:32:12,091 worker thread finished; awaiting finish of 1 more threads
2019-08-14 07:32:12,123 worker thread finished; awaiting finish of 0 more threads
2019-08-14 07:32:12,123 EPOCH - 4 : training on 13192 raw words (13458 effective words) took 0.1s, 241414 effective words/s
2019-08-14 07:32:12,127 worker thread finished; awaiting finish of 2 more threads
2019-08-14 07:32:12,151 worker thread finished; awaiting finish of 1 more threads
2019-08-14 07:32:12,178 worker thread finished; awaiting finish of 0 more threads
2019-08-14 07:32:12,178 EPOCH - 5 : training on 13192 raw words (13392 effective words) took 0.1s, 257374 effective words/s
2019-08-14 07:32:12,179 training on a 65960 raw words (67206 effective words) took 0.3s, 246694 effective words/s
2019-08-14 07:32:12,179 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-08-14 07:34:02,107 'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.
2019-08-14 07:34:02,112 'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.
2019-08-14 07:34:02,116 'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.
2019-08-14 07:34:02,119 'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.
2019-08-14 07:34:02,123 'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.
2019-08-14 07:34:02,127 'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.
2019-08-14 07:34:02,131 'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.
2019-08-14 07:34:02,135 'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.
2019-08-14 07:34:02,139 'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.
2019-08-14 07:34:02,142 'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.
2019-08-14 11:46:19,468 'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.
2019-08-14 18:17:17,096 consider setting layer size to a multiple of 4 for greater performance
2019-08-14 18:17:17,097 collecting all words and their counts
2019-08-14 18:17:17,097 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-08-14 18:17:17,097 collected 17 word types and 2 unique tags from a corpus of 2 examples and 33 words
2019-08-14 18:17:17,097 Loading a fresh vocabulary
2019-08-14 18:17:17,097 min_count=0 retains 17 unique words (100% of original 17, drops 0)
2019-08-14 18:17:17,097 min_count=0 leaves 33 word corpus (100% of original 33, drops 0)
2019-08-14 18:17:17,097 deleting the raw counts dictionary of 17 items
2019-08-14 18:17:17,098 sample=0.001 downsamples 17 most-common words
2019-08-14 18:17:17,098 downsampling leaves estimated 4 word corpus (14.7% of prior 33)
2019-08-14 18:17:17,098 estimated required memory for 17 words and 3 dimensions: 9332 bytes
2019-08-14 18:17:17,098 resetting layer weights
2019-08-14 18:17:17,098 training model with 3 workers on 17 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-08-14 18:17:17,100 worker thread finished; awaiting finish of 2 more threads
2019-08-14 18:17:17,100 worker thread finished; awaiting finish of 1 more threads
2019-08-14 18:17:17,103 worker thread finished; awaiting finish of 0 more threads
2019-08-14 18:17:17,103 EPOCH - 1 : training on 33 raw words (7 effective words) took 0.0s, 1847 effective words/s
2019-08-14 18:17:17,105 worker thread finished; awaiting finish of 2 more threads
2019-08-14 18:17:17,105 worker thread finished; awaiting finish of 1 more threads
2019-08-14 18:17:17,105 worker thread finished; awaiting finish of 0 more threads
2019-08-14 18:17:17,105 EPOCH - 2 : training on 33 raw words (5 effective words) took 0.0s, 6917 effective words/s
2019-08-14 18:17:17,106 worker thread finished; awaiting finish of 2 more threads
2019-08-14 18:17:17,106 worker thread finished; awaiting finish of 1 more threads
2019-08-14 18:17:17,106 worker thread finished; awaiting finish of 0 more threads
2019-08-14 18:17:17,106 EPOCH - 3 : training on 33 raw words (7 effective words) took 0.0s, 10763 effective words/s
2019-08-14 18:17:17,107 worker thread finished; awaiting finish of 2 more threads
2019-08-14 18:17:17,107 worker thread finished; awaiting finish of 1 more threads
2019-08-14 18:17:17,108 worker thread finished; awaiting finish of 0 more threads
2019-08-14 18:17:17,108 EPOCH - 4 : training on 33 raw words (4 effective words) took 0.0s, 7612 effective words/s
2019-08-14 18:17:17,108 worker thread finished; awaiting finish of 2 more threads
2019-08-14 18:17:17,109 worker thread finished; awaiting finish of 1 more threads
2019-08-14 18:17:17,109 worker thread finished; awaiting finish of 0 more threads
2019-08-14 18:17:17,109 EPOCH - 5 : training on 33 raw words (7 effective words) took 0.0s, 17160 effective words/s
2019-08-14 18:17:17,109 training on a 165 raw words (30 effective words) took 0.0s, 2912 effective words/s
2019-08-14 18:17:17,109 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-08-14 18:17:18,233 consider setting layer size to a multiple of 4 for greater performance
2019-08-14 18:17:18,233 collecting all words and their counts
2019-08-14 18:17:18,233 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-08-14 18:17:18,268 PROGRESS: at example #10000, processed 127599 words (3695781/s), 14559 word types, 10000 tags
2019-08-14 18:17:18,283 collected 17571 word types and 14524 unique tags from a corpus of 14524 examples and 185031 words
2019-08-14 18:17:18,283 Loading a fresh vocabulary
2019-08-14 18:17:18,302 min_count=0 retains 17571 unique words (100% of original 17571, drops 0)
2019-08-14 18:17:18,302 min_count=0 leaves 185031 word corpus (100% of original 185031, drops 0)
2019-08-14 18:17:18,333 deleting the raw counts dictionary of 17571 items
2019-08-14 18:17:18,333 sample=0.001 downsamples 29 most-common words
2019-08-14 18:17:18,333 downsampling leaves estimated 146451 word corpus (79.1% of prior 185031)
2019-08-14 18:17:18,359 estimated required memory for 17571 words and 3 dimensions: 12286292 bytes
2019-08-14 18:17:18,359 resetting layer weights
2019-08-14 18:17:18,597 training model with 3 workers on 17571 vocabulary and 3 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-08-14 18:17:18,959 worker thread finished; awaiting finish of 2 more threads
2019-08-14 18:17:18,964 worker thread finished; awaiting finish of 1 more threads
2019-08-14 18:17:18,969 worker thread finished; awaiting finish of 0 more threads
2019-08-14 18:17:18,969 EPOCH - 1 : training on 185031 raw words (161037 effective words) took 0.4s, 438680 effective words/s
2019-08-14 18:17:19,367 worker thread finished; awaiting finish of 2 more threads
2019-08-14 18:17:19,378 worker thread finished; awaiting finish of 1 more threads
2019-08-14 18:17:19,381 worker thread finished; awaiting finish of 0 more threads
2019-08-14 18:17:19,381 EPOCH - 2 : training on 185031 raw words (160983 effective words) took 0.4s, 394940 effective words/s
2019-08-14 18:17:19,739 worker thread finished; awaiting finish of 2 more threads
2019-08-14 18:17:19,744 worker thread finished; awaiting finish of 1 more threads
2019-08-14 18:17:19,746 worker thread finished; awaiting finish of 0 more threads
2019-08-14 18:17:19,746 EPOCH - 3 : training on 185031 raw words (160979 effective words) took 0.4s, 448372 effective words/s
2019-08-14 18:17:20,109 worker thread finished; awaiting finish of 2 more threads
2019-08-14 18:17:20,125 worker thread finished; awaiting finish of 1 more threads
2019-08-14 18:17:20,131 worker thread finished; awaiting finish of 0 more threads
2019-08-14 18:17:20,132 EPOCH - 4 : training on 185031 raw words (160981 effective words) took 0.4s, 422184 effective words/s
2019-08-14 18:17:20,505 worker thread finished; awaiting finish of 2 more threads
2019-08-14 18:17:20,522 worker thread finished; awaiting finish of 1 more threads
2019-08-14 18:17:20,526 worker thread finished; awaiting finish of 0 more threads
2019-08-14 18:17:20,526 EPOCH - 5 : training on 185031 raw words (161008 effective words) took 0.4s, 413818 effective words/s
2019-08-14 18:17:20,527 training on a 925155 raw words (804988 effective words) took 1.9s, 417205 effective words/s
2019-08-14 18:32:53,765 consider setting layer size to a multiple of 4 for greater performance
2019-08-14 18:32:53,765 collecting all words and their counts
2019-08-14 18:32:53,765 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-08-14 18:32:53,770 collected 3683 word types and 1453 unique tags from a corpus of 1453 examples and 13192 words
2019-08-14 18:32:53,770 Loading a fresh vocabulary
2019-08-14 18:32:53,774 min_count=0 retains 3683 unique words (100% of original 3683, drops 0)
2019-08-14 18:32:53,774 min_count=0 leaves 13192 word corpus (100% of original 13192, drops 0)
2019-08-14 18:32:53,780 deleting the raw counts dictionary of 3683 items
2019-08-14 18:32:53,780 sample=0.001 downsamples 40 most-common words
2019-08-14 18:32:53,780 downsampling leaves estimated 11987 word corpus (90.9% of prior 13192)
2019-08-14 18:32:53,785 estimated required memory for 3683 words and 50 dimensions: 3895900 bytes
2019-08-14 18:32:53,785 resetting layer weights
2019-08-14 18:32:53,821 training model with 3 workers on 3683 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-08-14 18:32:53,824 worker thread finished; awaiting finish of 2 more threads
2019-08-14 18:32:53,839 worker thread finished; awaiting finish of 1 more threads
2019-08-14 18:32:53,856 worker thread finished; awaiting finish of 0 more threads
2019-08-14 18:32:53,857 EPOCH - 1 : training on 13192 raw words (13466 effective words) took 0.0s, 405798 effective words/s
2019-08-14 18:32:53,859 worker thread finished; awaiting finish of 2 more threads
2019-08-14 18:32:53,873 worker thread finished; awaiting finish of 1 more threads
2019-08-14 18:32:53,890 worker thread finished; awaiting finish of 0 more threads
2019-08-14 18:32:53,890 EPOCH - 2 : training on 13192 raw words (13487 effective words) took 0.0s, 419951 effective words/s
2019-08-14 18:32:53,893 worker thread finished; awaiting finish of 2 more threads
2019-08-14 18:32:53,908 worker thread finished; awaiting finish of 1 more threads
2019-08-14 18:32:53,925 worker thread finished; awaiting finish of 0 more threads
2019-08-14 18:32:53,925 EPOCH - 3 : training on 13192 raw words (13455 effective words) took 0.0s, 405415 effective words/s
2019-08-14 18:32:53,928 worker thread finished; awaiting finish of 2 more threads
2019-08-14 18:32:53,943 worker thread finished; awaiting finish of 1 more threads
2019-08-14 18:32:53,960 worker thread finished; awaiting finish of 0 more threads
2019-08-14 18:32:53,960 EPOCH - 4 : training on 13192 raw words (13467 effective words) took 0.0s, 414727 effective words/s
2019-08-14 18:32:53,963 worker thread finished; awaiting finish of 2 more threads
2019-08-14 18:32:53,977 worker thread finished; awaiting finish of 1 more threads
2019-08-14 18:32:53,995 worker thread finished; awaiting finish of 0 more threads
2019-08-14 18:32:53,995 EPOCH - 5 : training on 13192 raw words (13469 effective words) took 0.0s, 408134 effective words/s
2019-08-14 18:32:53,995 training on a 65960 raw words (67344 effective words) took 0.2s, 388448 effective words/s
2019-08-14 18:32:53,995 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-08-14 18:54:32,057 consider setting layer size to a multiple of 4 for greater performance
2019-08-14 18:54:32,058 collecting all words and their counts
2019-08-14 18:54:32,058 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
2019-08-14 18:54:32,063 collected 3683 word types and 1453 unique tags from a corpus of 1453 examples and 13192 words
2019-08-14 18:54:32,063 Loading a fresh vocabulary
2019-08-14 18:54:32,068 min_count=0 retains 3683 unique words (100% of original 3683, drops 0)
2019-08-14 18:54:32,068 min_count=0 leaves 13192 word corpus (100% of original 13192, drops 0)
2019-08-14 18:54:32,075 deleting the raw counts dictionary of 3683 items
2019-08-14 18:54:32,075 sample=0.001 downsamples 40 most-common words
2019-08-14 18:54:32,075 downsampling leaves estimated 11987 word corpus (90.9% of prior 13192)
2019-08-14 18:54:32,080 estimated required memory for 3683 words and 50 dimensions: 3895900 bytes
2019-08-14 18:54:32,080 resetting layer weights
2019-08-14 18:54:32,119 training model with 3 workers on 3683 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
2019-08-14 18:54:32,122 worker thread finished; awaiting finish of 2 more threads
2019-08-14 18:54:32,139 worker thread finished; awaiting finish of 1 more threads
2019-08-14 18:54:32,165 worker thread finished; awaiting finish of 0 more threads
2019-08-14 18:54:32,165 EPOCH - 1 : training on 13192 raw words (13439 effective words) took 0.0s, 305746 effective words/s
2019-08-14 18:54:32,168 worker thread finished; awaiting finish of 2 more threads
2019-08-14 18:54:32,182 worker thread finished; awaiting finish of 1 more threads
2019-08-14 18:54:32,199 worker thread finished; awaiting finish of 0 more threads
2019-08-14 18:54:32,199 EPOCH - 2 : training on 13192 raw words (13490 effective words) took 0.0s, 419655 effective words/s
2019-08-14 18:54:32,202 worker thread finished; awaiting finish of 2 more threads
2019-08-14 18:54:32,217 worker thread finished; awaiting finish of 1 more threads
2019-08-14 18:54:32,234 worker thread finished; awaiting finish of 0 more threads
2019-08-14 18:54:32,235 EPOCH - 3 : training on 13192 raw words (13427 effective words) took 0.0s, 395854 effective words/s
2019-08-14 18:54:32,237 worker thread finished; awaiting finish of 2 more threads
2019-08-14 18:54:32,252 worker thread finished; awaiting finish of 1 more threads
2019-08-14 18:54:32,269 worker thread finished; awaiting finish of 0 more threads
2019-08-14 18:54:32,269 EPOCH - 4 : training on 13192 raw words (13480 effective words) took 0.0s, 408445 effective words/s
2019-08-14 18:54:32,272 worker thread finished; awaiting finish of 2 more threads
2019-08-14 18:54:32,287 worker thread finished; awaiting finish of 1 more threads
2019-08-14 18:54:32,304 worker thread finished; awaiting finish of 0 more threads
2019-08-14 18:54:32,304 EPOCH - 5 : training on 13192 raw words (13410 effective words) took 0.0s, 404987 effective words/s
2019-08-14 18:54:32,304 training on a 65960 raw words (67246 effective words) took 0.2s, 362994 effective words/s
2019-08-14 18:54:32,304 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay
2019-08-15 06:51:36,312 'pattern' package not found; tag filters are not available for English
2019-08-15 06:59:53,483 'pattern' package not found; tag filters are not available for English
2019-08-15 07:01:44,295 'pattern' package not found; tag filters are not available for English
2019-08-21 11:14:29,118 'pattern' package not found; tag filters are not available for English
2019-08-21 11:15:15,945 'pattern' package not found; tag filters are not available for English
